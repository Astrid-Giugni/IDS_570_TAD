---
title: "Week 07: Word2Vec and LDA introduction"
editor: visual
format: html
execute:
  echo: true
  warning: false
  message: false
---

## Step 0: Transition to Python

Before working with text in today's tutorial, we are going to have an introduction to getting started in Python. This is for those of you who haven't worked in Python before. **Note**: this doesn't mean that you want to forget everything that you learned in R up to now! The analyses that we conducted in R allowed us to develop a granular understanding of how to represent and measure textual features. The tasks we will do in Python will be less "transparent", but you don't want to give up the intuition you developed so far.

If you have never worked with Python, you can find a beginner friendly, step by step guide on how to download Python and set up VS Code (the environment you will use for Python–think of it as the Python version of R studio): [**here**](https://code.visualstudio.com/docs/python/python-tutorial).

If you have worked with Python before, you can use whichever environment you want.

**Quick note**: you want Python 3.11.x. The bottleneck is scikit-learn, which doesn't play well with the most recent version of Python.

**A second note about learning Python:** If you are new to Python, the first thing that you will see as you look at tutorials is that Python is an [**object-oriented language**](https://realpython.com/python3-object-oriented-programming/). I have added a link that takes you to [realpython.com](https://realpython.com/), which is a great resource for learning Python. But don't worry too much about understanding all the finer points of this from the get-go! The TA's and I will help you use the code that I give you as an example and then you can start learning more about objects and classes and all that jazz.

-   As we go along this tutorial: I am re-introducing the training wheels. I am including links to some of the major Python concepts as they come up in the code!

```{r}
################## Ignore this (it allows me to run Python in quarto for website)
library(reticulate)

use_python("C:/Users/astri/miniconda3/envs/tad/python.exe", required = TRUE)
py_config()
```

### Getting started: reading, tokenizing, and stop words

The first few steps that we are going to take are very similar to what we did in R: the logic is the same, but the syntax is different. The first thing that we need to do is to read the text and then check that things look correct!

In R, what we do at the beginning is:

-   load a text file into memory,

-   check how long it is,

-   preview the beginning,

-   and try to spot obvious encoding or OCR problems early (that is, does the text look weird?).

We are now going to do the same step in Python. The main difference is that we are going to use a [**path-handling tool**](https://realpython.com/python-pathlib/) in Python, `Path`. We are then going to actually read the file with this line: `text = path.read_text(encoding="utf-8", errors="replace")`. Here, I am asking Python to decode the text using `utf-8` and to replace any unreadable characters instead of crashing. The replacement is the Unicode replacement character: �.

```{python}
from pathlib import Path

path = Path("texts/wealth.txt")
text = path.read_text(encoding="utf-8", errors="replace")
# The lines above read the wealth.txt into a long string

print("Characters:", len(text)) #character count 
print("Lines:", text.count("\n") + 1) #line count

# taking a look that the text looks ok. Similar to head() and slice() in R
print("\n--- START ---\n")
print(text[:800])

mid = len(text)//2
print("\n--- MIDDLE SLICE ---\n")
print(text[mid:mid+800])
```

The `print()` function is something that will come up over and over again. If you get lost by how I am using it, do click on this [**explanation**](https://realpython.com/python-print/).

The next step is to tokenize the text. This is the Python version of `unnest_tokens()` in tidytext. But in Python, we have to do this using a regex: so, hopefully, you feel comfortable with the regex unit from Week 2. We are also going to lowercase.

Since we also want to run some checks on our process (as we did in R), we are going to inspect the most frequent tokens using [`Counter`](https://realpython.com/python-counter/), the Python parallel to tidytext's `count(word, sort =TRUE)`.

**Making things look nicer:** You can skip this explanation, if you are new to Python. This is equivalent to: `print(w, c)`. I am just trying to make the display more readable for the class!

In the next block below, I am going to use an `f-string` inside `print()`. An `f-string` is a way to format text, so `f"{w:>12} {c}"` means (working inside out):

-   take the value of `w` and convert it to text, right-align it inside a space 12 character wide;

-   then when you print separate the columns (the two spaces between `{w:>12}` and `{c}`)

```{python}
import re
from collections import Counter

tokens = re.findall(r"[A-Za-z]+(?:'[A-Za-z]+)?", text.lower())
print("Tokens:", len(tokens))
print("Unique tokens:", len(set(tokens)))

counts = Counter(tokens)
print("\nTop 25 tokens:")
for w, c in counts.most_common(25):
    print(f"{w:>12}  {c}")

```

The library [**Scikit-learn**](https://scikit-learn.org/stable/) is going to allow us to import a standard list of stopwords. More importantly, `Scikit-learn` is a machine learning library for Python and we will use it more than once!

We are going to covert the list of pre-defined stopwords into a [**set**](https://realpython.com/python-sets/) because it will make filtering tokens more efficient. We can then use the **in-place OR operator** `|=` to add all the elements of our custom stopwords to the standard stopwords (the link at "sets" will also explain `|=`).

**Aside:** if you go to the tutorial, you might see that there is another **OR operator** `|` . This operator creates a new set. I am just modifying the preexisting one by adding the custom stopwords.

```{python}

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

stopwords = set(ENGLISH_STOP_WORDS) #turn it into a set

# This is option and right now, I am only using this as an example of syntax
custom_stopwords = {"upon", "shall", "hath"}

stopwords |= custom_stopwords

# let's check how many stopwords we have and what they look like
print("Stopwords loaded:", len(stopwords))
print("Sample:", sorted(list(stopwords))[:25])
```

Now that we have our stopwords as we want them, we can apply it to our tokens. We are also going to remove really short tokens (fewer than 3 tokens). Something that sometimes throws people off: I am stating the length requirement as a strict inequality. Now look at the code below: do you understand why have `len(t) >= 3` ?

```{python}

from collections import Counter

clean_tokens = [t for t in tokens if t not in stopwords and len(t) >= 3]

print("Tokens (raw):", len(tokens))
print("Tokens (clean):", len(clean_tokens))
print("Unique tokens (clean):", len(set(clean_tokens)))

clean_counts = Counter(clean_tokens)

print("\nTop 25 tokens after cleaning:")
for w, c in clean_counts.most_common(25):
    print(f"{w:>12}  {c}")
```

### Segmenting the text and preparing for Word2Vec and LDA

So far, the steps should be very familiar (with a different coding language). The next step is driven by the model itself. Since LDA does not operate on individual words or on a continuous token stream, we are going to be constrained by the structure that it needs. Word2Vec *can* work on the entire *Wealth of Nations* at once, but for the sake of continuity, we will stick to the same structure for both models.

LDA needs to work with documents or, as in our case, pre-defined segments of a longer text. These are the assumptions behind LDA:

-   each document is a mixture of topics,

-   each topic is a distribution over words.

This forces us to decide what counts as a "document" in our corpus. In some cases, this will be obvious to you: if you have a collection of political speeches, then each speech is a document. In other cases, we have to make a decision. *The Wealth of Nations* is one long text and we can segment it in a number of ways. There are two common options (each with its one trade-offs): one is to use internal subdivisions, such as chapters; the other is to use fixed-sized segments. We are going to go for the second one and create 800-token chunks.

**Note:** both options are legitimate choices. I am picking fixed-sized segments *because* I know that the chapters in *The Wealth of Nations* are of varied length and that a single topic (such as labor or political economy) is split over several chapters. The approach I take here also ensures that no single segment dominates the model simply because of its length. LDA *tends* to work better when documents are not extremely long. But this choice comes with a trade-off: I am losing track of Adam Smith's structure. For different purposes (that is, for an actual project rather than a class tutorial), I might choose to "chunk" by chapter instead.

```{python}
# Segmenting

SEGMENT_SIZE = 800  # fixed-size "document." We can always adjust length
# slice the entire list of tokens from token #0 to the last token, breaking it into chunks of SEGMENT_SIZE.
segments = [
    clean_tokens[i:i+SEGMENT_SIZE] 
    for i in range(0, len(clean_tokens), SEGMENT_SIZE)
]

#inspect the segmentation
print("Number of segments:", len(segments))
print("First segment length:", len(segments[0]))
print("Last segment length:", len(segments[-1]))
```

Each segment is now a document for LDA: we have 186 segments, which is a good document count for LDA, and segment sizes are consistent (800; last shorter, which makes sense). Let's see what word counts in a segment look like:

```{python}

from collections import Counter

segment_counts = [Counter(seg) for seg in segments]

# check: top words in segment 0
print("Top words in segment 0:")
for w, c in segment_counts[0].most_common(15):
    print(f"{w:>12}  {c}")
```

This also looks reasonable for the opening of the book (chapter, book, division, labor, work, nations, society) and it means that we can proceed to the next step.

At this point, we now have two complementary representations of the same text:

-   **`segments`**: each segment is an ordered sequence of tokens

-   **`segment_counts`**: each segment is represented by word counts (bag-of-words)

We are going to need both representations, but for two different models. [Word2Vec]{.underline} learns meaning from *local context and word order*, so it can work directly with `segments`. On the other hand, [LDA]{.underline} ignores word order and models documents as mixtures of topics, so it requires a bag-of-words representation, which we will build explicitly in the next section.

We will begin with Word2Vec, because it extends ideas you have already encountered: a word's meaning is shaped by the contexts in which it appears. For a good resource on understanding Word2Vec, see [[**here**]{.underline}](https://www.tensorflow.org/text/tutorials/word2vec). But, in short, instead of storing explicit co-occurrence counts, Word2Vec learns dense vectors for each word. These vectors are learned by optimizing a predictive task: *given a word, predict its surrounding context (or vice versa)*. The result is a compressed, smoothed representation of the same information we previously counted directly.

OK, what does this mean in practice for us? Each segment is a sequence of tokens and Word2Vec slides a fixed-size context window across these sequences and learns vector representations that are good at predicting nearby words.

```{python}
# Key step: Word2Vec training

from gensim.models import Word2Vec  

w2v = Word2Vec(
    sentences=segments,     # each segment is a list of tokens
    vector_size=100,        # dimensionality
    window=5,               # context window (parallel to your co-occurrence window)
    min_count=20,           # ignore very rare words
    workers=4,              # number of CPU cores for parallel processing
    sg=1                    # 1=skip-gram, 0=CBOW
)

print("Vocabulary size:", len(w2v.wv.key_to_index))
```

We have to make some choices when we train Word2Vec:

-   `vector_size=100`: each word is represented as a 100-dimensional vector (if we choose higher dimension, we capture more information about each word, but it's slower and requires more text data). Common values in practice: 100-300. 100 is often the default.

-   `window=5` matches what we did in our co-occurrence calculation.

-   `min_count=20` keeps the vocab manageable and stable for teaching.

-   **Important:** if you have an older machine and it's freezing; `change workers = 4` to 2 or even 1.

-   `sg=1` (skip-gram) is usually better for capturing rarer, more specific relations.

Now that the model is trained, we can inspect its learned semantic space by asking for a word's **nearest neighbors**. The goal is to use **cosine similarity** to find which word vectors are closest to a given/target word in the vector space. In this case, let's look at "trade," "labor," "price," "capital," and "market" to see what kind of results we get.

-   quick aside if you haven't worked in Python before: this is the first time we are defining a function using the [**def** keyword](https://realpython.com/defining-your-own-python-function/#getting-to-know-functions-in-python).

```{python}
# Nearest neighbors (cosine similarity)

def show_neighbors(word, topn=10):
    if word not in w2v.wv:
        print(f"'{word}' not in vocabulary.")
        return
    print(f"\nNearest neighbors for '{word}':")
    for w, sim in w2v.wv.most_similar(word, topn=topn):
        print(f"{w:>12}  {sim:.3f}")

for target in ["trade", "labor", "price", "capital", "market"]:
    show_neighbors(target, topn=10)
```

For **trade**: the terms that we are getting indicate that Smith is using structural and institutional language in conjunction with this term. We have verbs like carrying, carried, and shipping (possibly used as a noun...), as well as structural qualifiers such as branches, returns, and direct. Some words are perhaps more surprising and would require more investigation (such as round).

For **labor:** there is a strong sign the model has learned trade as an activity embedded in the language of key economic activities, such as productive, distributed, enables, and, more obviously, but very reassuring, laborer and tools.

-   As an exercise for the students: what do you think of the neighbors for price, capital, and markets? Any surprises?

I think that at this point, it will be helpful to compare with the co-occurrence analysis we did last week. Let's compute simple co-occurrence neighbors using the same as in week 6 (`window = 5`) and compare.

```{python}
# Co-occurrence neighbors (window = 5)

from collections import Counter, defaultdict

WINDOW = 5
cooc = defaultdict(Counter)

for seg in segments:
    for i, token in enumerate(seg):
        start = max(0, i - WINDOW)
        end = min(len(seg), i + WINDOW + 1)
        for j in range(start, end):
            if i != j:
                cooc[token][seg[j]] += 1

def show_cooc_neighbors(word, topn=10):
    print(f"\nCo-occurrence neighbors for '{word}':")
    for w, c in cooc[word].most_common(topn):
        print(f"{w:>12}  {c}")
```

Now we can look at them and compare with Word2Vec neighbors defined by cosine similarity:

```{python}
for target in ["trade", "labor", "price"]:
    show_cooc_neighbors(target, topn=10)

```

-   What differences do you note in these lists? Both represent "neighbors" of our target words, but obtain through two different methods.

## LDA Topic Modeling:

Up to this point, we have used Word2Vec to explore how individual words relate to one another based on shared contexts. This approach is well suited to questions like:

-   Which words behave similarly across the text? How is a concept used, qualified, or framed?

This is really helpful if you are trying to think about how, for example, different texts portray the same terminology and concepts. But what if we want to focus on the *text* as a whole instead? This is the question that topic models—and LDA in particular—are designed to answer.

### Some background on LDA:

In class, we briefly discussed the Dirichlet distribution. I emphasized that you don't need to know the details in the background in order to actually work with LDA topic modeling. However, you do need to understand some basics about distributions so as to develop some intuition. First, if you need a clear and to the point reminder of binomial distributions, look at [[**Josh Starmer's youtube channel**]{.underline}](https://www.youtube.com/watch?v=J8jNoF-K8E8) \[he is a faculty member at UNC; not a random channel\]. If you want a more robust discussion, see [Peter Dalgaard, *Introductory Statistics with R*](https://find.library.duke.edu/catalog/DUKE99119538962108501) (2008), chapter 3 \[statistics applications is one of the reasons I started the semester with R\]. Once you are comfortable with that, use the discussion in chapter 6 of *Text as Data* for Dirichlet distribution (or take it on faith for now!).

With that behind us, the next step that we are going to do is the parallel to creating a DFM, but we have to organize our text into the data structure required by LDA. We already have `segments` and `segment_counts`, we need a [**dictionary**](https://docs.python.org/3/tutorial/datastructures.html) (this is a key data structure in Python, follow the link if you are unfamiliar with it) and a document-**term** matrix (D**T**M). LDA works with a "bag-of-words" representation, so we need to get word counts for each segment. This should be very familiar for our work in R.

In the code below, `Dictionary` builds a vocabulary from our corpus and creates a mapping between each unique word (like "trade") and a unique integer ID (assigned sequentially, in the order that new words are first encountered when gensim scans the corpus).

```{python}

from gensim.corpora import Dictionary

# Build a dictionary from the segments
dictionary = Dictionary(segments)

# do some cleaning
dictionary.filter_extremes(
    no_below=10,    # must appear in at least 10 segments
    no_above=0.5   # must appear in no more than 50% of segments
)

print("Vocabulary size after filtering:", len(dictionary))

# Convert segments to bag-of-words format
corpus = [dictionary.doc2bow(seg) for seg in segments]

#Take a look
print("First document (bow format):")
print(corpus[0][:10])
```

In the filtering step we are mirroring what we did in R when we removed very rare words and stopwords, but with a slightly different logic. Remember, we are trying to create topics and rare words in the corpus won't really define a what the text is "about," so we filter out words that appear in fewer than 10 segments by using `no_below=10` (out of 186 segments). Conversely, really frequent words tend to be too general to distinguish topics, so we filter for words that appear in more than half of the segments using `no_above=0.5`. [Note]{.underline}: both these values are heuristics that tend to work well for a first run of the model. But you might decide that, based on your question, you want to try different filters!

At this point, we have the same kind of data structure we relied on in R for bag-of-words models: **documents represented by word counts**. The next step will simply convert these counts into the specific input format required by the LDA implementation we are using.

We are finally done with preprocessing and we can actually fit the LDA model (yay!). [To reiterate]{.underline}: these "topics" are computational constructs; they are estimates of latent word distributions that, taken together, explain the observed word counts reasonable well.

```{python}
# Train LDA model

from gensim.models import LdaModel

lda = LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=8,
    random_state=42,
    passes=10,
    alpha="auto",
    eta="auto"
)

print("LDA model trained.")

```

#### Why these hyperparameter settings:

-   `num_topics=8`: small enough to read–it's a good starting place

-   `passes=10`: this is how many times the model goes through the corpus during training. More passes will produce more stable topic estimates, but they take longer to compute. The size of your corpus matters here: with a small corpus, each document will carry a lot of weight and topic estimates are more sensitive to noise, so I will tend to do 10-15 passes. With a larger corpus, you can get away with fewer passes (5-10-ish) because the model encounters each word in more contexts. I don't have a a clear cut rule because the answer also depends on: "how much time and GPU to you have?"

    -   [A nuanced point]{.underline}: the model also has a default number of [**iterations**]{.underline} that we are not changing. Iterations are different from passes. The iterations control how thoroughly the model updates its estimates during each visit: these are the internal optimization steps that occur during *each* pass.

When we train an LDA model, we are not only choosing *how many* topics to use. We are also choosing "how flexible" the model is in assigning topics to documents and words to topics

-   `alpha="auto"`: the `alpha` setting governs the document-topic distribution. That is, does the model assume that documents (segments in our case) tend to focus on one or fewer topics (low `alpha`) or does it assume that they contain a mix of many topics (higher `alpha`)? When we set it to `"auto"`, we allow the model to learn this behavior from the data. I chose `"auto"` because we created arbitrary segments in the text based on a pre-selected number of tokens. If I had a different corpus, I would have to think more about `alpha`.

    -   What does this mean? In `gensim`'s LDA implementation, the default setting for the `alpha` parameter is 'symmetric'. This means that alpha has the same value for all topics. When `alpha` is symmetric, gensim calculates it using this formula: $\alpha$ = 1.0 / number of topics. When I say that based on the structure of the corpus, I would have to think about it more, I mean this quite literally: I will base my decisions on the exact question that I am asking with topic modelling and the details of the corpus.

    -   If you really want to dig into this, here are the resources that I would review before making these decisions: "[Finding scientific topics](https://www.pnas.org/doi/10.1073/pnas.0307752101)" (2004); "[Optimising Semantic Coherence in Topic Models](https://aclanthology.org/D11-1024.pdf)" (2011); and for good measure, I am including the original paper on LDA, though it wouldn't be my first stop for practical decisions, "[Latent Dirichlet Allocation](www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)" (2003).

-   `eta="auto"`: you can think of this setting as, how "chatty" do we want our topics to be? A low eta encourages topics to focus on fewer words, while a high eta encourages more words per topic. I usually select auto as it allows the model to infer how sharp (fewer words) topics should be based on the actual distribution of words in the corpus. In technical literature, you will find this parameter denoted by $\beta$.

-   `random_state`: this will probably seem the most opaque setting. It fixes the sequence of random choices the model makes during initialization. This is a step for reproducibility and transparency. The goal is to be able to obtain the same output from run to run, **but** \[flashing warning light, sirens blaring, warning, warning\] only in the case that for each run you have the same corpus, the same preprocessing, the all same hyperparamaters settings, the same software versions (!), and the same hardware behavior. We do what we can to be as transparent as possible. Sometimes, life has other plans.

For more information on all the hyperparameters in [**gensim's LDA follow the link**](https://radimrehurek.com/gensim/models/ldamodel.html). If you really want to dig further into this topic, I strongly recommend [**this paper**](https://proceedings.neurips.cc/paper_files/paper/2009/file/0d0871f0806eae32d30983b62252da50-Paper.pdf).

At the end of the day: LDA results depend on modeling choices and random initialization. Different runs can produce different, but equally reasonable, topic structures.

OK, we can finally enjoy the fruits of our labor. Let's inspect the topics:

```{python}

for topic_id in range(lda.num_topics):
    print(f"\nTopic {topic_id}:")
    for word, weight in lda.show_topic(topic_id, topn=12):
        print(f"{word:>12}  {weight:.3f}")

```

And let's visualize them with a bar plot that gives us a representation of the topic prevalence across segments:

```{python}

import numpy as np
import matplotlib.pyplot as plt

K = lda.num_topics
topic_mass = np.zeros(K)

# Sum topic probabilities over all segments
for bow in corpus:
    doc_topics = lda.get_document_topics(bow, minimum_probability=0)
    for k, p in doc_topics:
        topic_mass[k] += p

# Normalize to proportions (so bars sum to 1)
topic_share = topic_mass / topic_mass.sum()

# Plot
plt.figure()
plt.bar(range(K), topic_share)
plt.xticks(range(K), [f"T{k}" for k in range(K)])
plt.ylabel("Share of topic mass (across segments)")
plt.xlabel("Topic")
plt.title("LDA topic prevalence in Wealth of Nations (by segment)")
plt.show()

# Print the numeric values too (useful for interpretation)
for k, s in enumerate(topic_share):
    print(f"Topic {k}: {s:.3f}")

```

Each bar shows the average proportion of that topic across all your 186 segments—i.e., which themes are most prevalent in the book under your segmentation choice.

One more visualization: topic prevalence by segment. This makes the structure of *The Wealth of Nations* visible.

```{python}

import numpy as np
import matplotlib.pyplot as plt

# Get per-segment topic distributions
doc_topic_matrix = np.array([
    [p for _, p in lda.get_document_topics(bow, minimum_probability=0)]
    for bow in corpus
])

plt.figure()

# Plot only the top 4 topics by overall prevalence (less clutter)
top_topics = np.argsort(topic_share)[-4:]

for k in top_topics:
    plt.plot(doc_topic_matrix[:, k], label=f"Topic {k}")

plt.xlabel("Segment index (approx. book progression)")
plt.ylabel("Topic proportion")
plt.title("Topic prevalence across Wealth of Nations")
plt.legend()
plt.show()

```
