---
title: "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python"
editor: visual
format: html
execute:
  echo: true
  warning: false
  message: false
---

```{r}
#| include: false

library(reticulate)

use_python("C:/Users/astri/miniconda3/envs/tad/python.exe", required = TRUE)
py_config()
```

# Weeks 8 and 9 goals:

So far, we have done unsupervised and semi-structured approaches to text analysis (frequency, TF–IDF, similarity, co-occurrence, and Word2Vec). During these two weeks we are going to make a key transition to supervised methods. I am grouping the two together because we need to take some time to think about how to *thoughtfully* set up the classification task: language is flexible and conceptual boundaries are porous!

You can move across the code and steps for these two weeks at your own pace as you get familiar with the process. [In class]{.underline}: I will move through the lectures *assuming* that everyone will finish working through this entire tutorial by March 6th (the end of week 9).

[**For Week 8**]{.underline}:

The goal is to **train a supervised classifier** that learns to recognize a category of texts. In particular, we are going to figure out how to distinguish which texts engage in merchant-related discourse versus those that do not.

In humanities (and social sciences), supervised learning is often the workhorse method for answering questions like:

-   Is this text pro-trade or protectionist?
-   Does this paragraph contain “merchant discourse”?
-   Which documents are about X, versus not-X?

But supervised learning raises a methodological question immediately: Where do labels come from?

In humanities contexts, we often do **not** begin with a fully human-labeled dataset (though sometimes there is no getting around to labelling things by hand as I demonstrated with NER code book a couple of weeks ago). So we will begin with **weak supervision**: we will generate *imperfect* labels using rules, then train a model, and validate it carefully. So, for week 8, [you will use Word2Vec]{.underline} to:

1.  Define a category you want to detect in texts (a binary label). For us, it will be about the concept "merchant" (this will be our "seed" concept).
2.  Segment texts into a usable unit of analysis (chunks).
3.  Create a weakly supervised training dataset using a seed term and related terms.

[**For Week 9**]{.underline}**:**

1.  Train a **baseline text classifier** in Python using:

    -   TF–IDF features
    -   Logistic regression

2.  Evaluate the classifier using:

    -   a train/test split
    -   a confusion matrix
    -   precision, recall, and F1 score

3.  Reflect on what “accuracy” means when labels come from weak supervision.

## Big picture workflow

Here is the full pipeline we will build:

1.  Load texts from a folder (`texts/`): [you will **need** to get the 500+ files that I uploaded on Canvas under "Train_Text_Documents."]{.underline}
2.  Segment each text into sentence-based chunks (our “documents” for classification)
3.  Use Word2Vec (trained on our corpus) to expand a seed concept (seed: `merchant`)
4.  Build a search-word list (Tier A/B/C: more details below)
5.  Use the search-word list to create weak labels
6.  Create a CORE vs NEG dataset (and optionally a MAYBE set)
7.  Split into train and test
8.  Train a classifier (TF–IDF + logistic regression)
9.  Evaluate and interpret

We will move slowly and validate outputs at each stage. Again, I am keeping the training wheels on, but

## What you need before starting

-   VSCode installed
-   A project folder like: `IDS_570_TAD/`
-   A subfolder: `texts/` containing your `.txt` files
-   A Python virtual environment (`.venv`) activated in VSCode
-   Packages we will use:
    -   `nltk` –\> we will use this in Step 2 for sentence tokenization (`nltk` has strong sentence boundary detection)
    -   `gensim` –\> we will use this in Step 5 to train Word2Vec
    -   `scikit-learn` –\> we will use this in Step 10-11 for TF-IDF and logistic regression
    -   `tqdm` -\> to make sure we don't lose [our minds](https://tqdm.github.io/) while we wait.

(We will install these together in the steps below.)

## Week 8 (Part I--Week Supervision)

## Step 0: Project setup & environment

Before we do any modeling, we need to make sure our project is set up correctly. This step is mainly for those of you **new to Python**, but it also will help everyone learn good project organization skills: many errors in text analysis come from working in the wrong folder or environment.

Where:

-   `texts/` contains the raw `.txt` files you want to analyze.
-   `data/` will store intermediate outputs (JSON files, inspection samples).
-   `models/` will store trained models (e.g., Word2Vec, classifiers).
-   Each `stepX_*.py` file does one conceptual task.

We intentionally separate steps into different scripts so that: - each step is easy to test and debug - you can rerun part of the pipeline without rerunning everything - you can clearly see how the pipeline is constructed.

### Opening the terminal in VSCode; Virtual environment set up

Make sure you open VSCode [in the project folder]{.underline} (`IDS_570_TAD`).

Then open the [**terminal**](https://code.visualstudio.com/docs/terminal/getting-started): there are a ton of videos online, if you have never done it before. If you are using something other than VSCode, you can also find guides online.

You should see something like:

\`\`\`bash PS D:\Users...\IDS\_570_TAD\>

If you see a different folder, you are in the wrong place. This is important because we are going to set up a Python virtual environment to keep project-specific packages isolated.

Next, create the environment and activate it:

`python -m venv .venv`

then, in Windows:

`.venv\Scripts\Activate.ps1`

Note, if this fails try:

`Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`

Then try activation again.

**OR** in Mac/Linux

`source .venv/bin/activate`

For full instructions, see [**here**](https://docs.python.org/3/library/venv.html). Your terminal prompt should now include `.venv`.

### Installing required packages

Install the libraries we will use:

`pip install nltk gensim scikit-learn tqdm`

We will download one additional resource (NLTK sentence tokenizer) later.

Before moving on, check:

-   Can you see your `.txt` files from Python?

-   Does `python step1_load_texts.py` run without errors?

-   Is your terminal in the correct folder?

If something is off here, it is worth fixing **before** you continue: go back through the steps above and see if something is missing. There are also a lot of videos online that help you go through this process.

## Step 1: Loading, Inspecting, and Segmenting Texts

Before we segment, vectorize, or classify anything, we need to confirm that Python can see and read our texts correctly.

### Reading text files from a folder

We will assume all `.txt` files are stored in a folder called `texts/`. Now, create a new file and name it: `step1_load_texts.py` and then run it:

``` python
from pathlib import Path

TEXT_DIR = Path("texts")

# Collect all .txt files
files = sorted(TEXT_DIR.glob("*.txt"))

print(f"Found {len(files)} .txt files.")

# Print the first few filenames
print("First 10 files:")
for f in files[:10]:
    print(" ", f)
```

This should tell you that you found [about]{.underline} 516 .txt files. **Note**: you have a [very]{.underline} slightly different training set; you are **responsible** for keeping track of how this affects things down the line (though the first few steps should match). It's my way of keeping you on your toes!

You should see a list of filenames printed to the terminal. Now we want to inspect one file to make sure that we have set things up correctly (**note**: in the future, I will [**assume**]{.underline} that you will do this on your own!). To the same file as above, add the following and run the script again:

``` python
# Read one example file
example_file = files[0]

with open(example_file, "r", encoding="utf-8", errors="ignore") as f:
    text = f.read()

print("\nReading file:")
print(example_file)
print("-" * 40)
print("Number of characters:", len(text))
print("\nFirst 1,000 characters:\n")
print(text[:1000])
```

We are just making sure that we can load the data and that things look correct (that is, we have funky looking Early Modern texts). In my case (and it should be in your too), the file read was: `texts\A00419.txt`; number of characters: 2888252l and then you should get the first few lines of the text.

## Step 2: Segmenting Texts into Chunks

Our `.txt` files are far too large to classify as single units. A supervised classifier expects many medium-sized examples and some of the texts I gave you are *very* long (the one above had 2,888,252 characters!). So we need to decide what counts as a “document” for classification.

-   For this week, we will treat chunks of sentences as our unit of analysis. We're using sentence chunks because we want to capture enough context for semantic meaning (a single sentence might be ambiguous), but not so much that we get incorrect labels (a 50-page chapter likely discusses many topics and might lead to mislabelling: such a large "chunk" may not be about "merchants" in any meaningful sense, but, for example, it could be a biblical passage mentioning a merchants *metaphorically*–we would need a more careful approach for these kinds of texts).

**Note:** There is no universally correct choice here; segmentation is a modeling decision and I am basing the "chunk" size on experience.

We will begin by splitting each text into sentences using NLTK. I have two pieces of documentation for NLTK that I recommend, if you want to delve into it: [**here**](https://www.nltk.org/) and [[**here**]{.underline}](https://guides.library.upenn.edu/penntdm/python/nltk).

Create a new file: `step2_segment_and_chunk.py` . We will chunk the texts and check some basic diagnostics to make sure that the code is actually running properly.

``` python
from pathlib import Path
import nltk

# Download required tokenizers (run once)
nltk.download("punkt", quiet=True)
nltk.download("punkt_tab", quiet=True)

TEXT_DIR = Path("texts")
txt_paths = sorted(TEXT_DIR.glob("*.txt"))

sample_path = txt_paths[0]

with open(sample_path, "r", encoding="utf-8", errors="ignore") as f:
    text = f.read()

# Split text into sentences
sentences = nltk.sent_tokenize(text)

print("File:", sample_path)
print("Number of sentences:", len(sentences))
print()

# Group sentences into chunks of ~120 words
TARGET_WORDS = 120

chunks = []
current = []
current_len = 0

for sent in sentences:
    words = sent.split()
    if not words:
        continue

    # If adding this sentence would exceed the target, finalize the chunk
    if current_len + len(words) > TARGET_WORDS and current:
        chunks.append(" ".join(current))
        current = []
        current_len = 0

    current.append(sent)
    current_len += len(words)

# Add any leftover sentences
if current:
    chunks.append(" ".join(current))

print("Number of chunks:", len(chunks))

# Diagnostics on chunk length (rough word counts)
lengths = [len(c.split()) for c in chunks]
lengths_sorted = sorted(lengths)

print()
print("Approx word counts per chunk:")
print("  min:", min(lengths))
print("  median:", lengths_sorted[len(lengths_sorted)//2])
print("  max:", max(lengths))

lo, hi = 5, 200
in_range = sum(lo <= n <= hi for n in lengths)
print(f"Chunks with {lo}–{hi} words:", in_range)
print("Share in range:", round(in_range / len(lengths), 3))

print()
print("--- Chunk 1 preview (first 400 chars) ---")
print(chunks[0][:400])
```

And run the script.

[You should see]{.underline}:

-   Thousands of chunks (`5172`)

-   A median chunk length around 100–120 words (`median: 101`)

-   Most chunks falling in the 5–200 word range (`5009`; with the share in range: `0.968`)

-   And a preview of the first chunk.

This tells us that our segmentation choice is reasonable for classification.

## Step 3: Tokenize Chunks for Modelling

This step will be conceptually familiar from when we used `unnest_tokens()` in tidytext.\
we transform a text column into word tokens.

We will do this using [`gensim.utils.simple_preprocess()`](https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html).

Create a new file: `step3_tokenize_chunks.py`. This new script is going to **build on** what we did in the "step2" script, s you will find the same logic repeated at the beginning.

``` python
from pathlib import Path
import nltk
from gensim.utils import simple_preprocess

nltk.download("punkt", quiet=True)
nltk.download("punkt_tab", quiet=True)

TEXT_DIR = Path("texts")
txt_paths = sorted(TEXT_DIR.glob("*.txt"))
sample_path = txt_paths[0]

with open(sample_path, "r", encoding="utf-8", errors="ignore") as f:
    text = f.read()

# Step 2 logic: sentences -> chunks (~120 words)
sentences = nltk.sent_tokenize(text)

TARGET_WORDS = 120
chunks = []
current = []
current_len = 0

for sent in sentences:
    words = sent.split()
    if not words:
        continue

    if current_len + len(words) > TARGET_WORDS and current:
        chunks.append(" ".join(current))
        current = []
        current_len = 0

    current.append(sent)
    current_len += len(words)

if current:
    chunks.append(" ".join(current))

# Step 3: now we tokenize each chunk
token_lists = [simple_preprocess(c, deacc=True) for c in chunks]

print("File:", sample_path)
print("Chunks (strings):", len(chunks))
print("Chunks (token lists):", len(token_lists))

print("\n--- Token preview (first 60 tokens of first chunk) ---")
print(token_lists[0][:60])

print("\n--- Token count of first chunk ---")
print(len(token_lists[0]))
```

Now run it. What we are doing:

-   `simple_preprocess()` returns a **list of tokens**.

-   Tokens are lowercased and cleaned.

-   This is now in the format required by Word2Vec:

    -   Word2Vec expects `sentences=[["token","token",...], ...]`

The output should look like:

`Chunks (strings): 5172`

`Chunks (token lists): 5172`

`--- Token preview (first 60 tokens of first chunk) --- ['the', 'first', 'booke', 'of', 'the', 'covntrie', 'farme', 'chap', 'what', 'manner', 'of', 'husbandrie', 'is', 'entreated', 'of', 'in', 'the', 'discourse', 'following', 'even', 'as', 'the', 'manner', 'of', 'building', 'vsed', 'at', 'this', 'day', 'the', 'varietie', 'of', 'countries', 'causeth', 'diuers', 'manner', 'of', 'labouring', 'of', 'the', 'earth']`

`--- Token count of first chunk --- 41`

### ⚠️ **Note of warning:**

gensim may look “stuck” the first time you import it. When you import `gensim` (especially on Windows), Python may appear to freeze for 10–60 seconds. This is normal and does **not** mean your code is broken.

-   `gensim` loads compiled components used for Word2Vec
-   on Windows, this initial load can be slow
-   Python does not print progress messages during this step If the terminal is not showing an error message, wait patiently before interrupting the process. Only stop the program if it has been unresponsive for several minutes.

## Step 4: Process all Files and Filter Chunks

So far, we have been working with a single example file to make sure that things are working. Now we are ready to apply the same segmentation and tokenization steps to the entire corpus. We are going to make the following sampling decisions:

-   discard chunks that have fewer than 5 tokens (not enough context)

-   discard chunks that have more than 200 tokens (too diffuse and hard to interpret)

Again, these thresholds are based on experience in working with Early Modern texts: they are reasonable defaults to start with and we could always change them if things don't work out. The **goal** is to define a reasonable and usable training example.

Create a new file: `step4_process_all_files.py`.

``` python
from pathlib import Path
import nltk
from gensim.utils import simple_preprocess

nltk.download("punkt", quiet=True)
nltk.download("punkt_tab", quiet=True)

TEXT_DIR = Path("texts")
txt_paths = sorted(TEXT_DIR.glob("*.txt"))

TARGET_WORDS = 120
MIN_WORDS = 5
MAX_WORDS = 200

all_chunks = []
all_token_lists = []

def chunk_text(text, target_words=120):
    sentences = nltk.sent_tokenize(text)

    chunks = []
    current = []
    current_len = 0

    for sent in sentences:
        words = sent.split()
        if not words:
            continue

        if current_len + len(words) > target_words and current:
            chunks.append(" ".join(current))
            current = []
            current_len = 0

        current.append(sent)
        current_len += len(words)

    if current:
        chunks.append(" ".join(current))

    return chunks

print(f"Found {len(txt_paths)} text files.")
print()

for path in txt_paths:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        text = f.read()

    chunks = chunk_text(text, TARGET_WORDS)

    for c in chunks:
        tokens = simple_preprocess(c, deacc=True)
        n_tokens = len(tokens)

        # Filter by length
        if MIN_WORDS <= n_tokens <= MAX_WORDS:
            all_chunks.append(c)
            all_token_lists.append(tokens)

print("Total chunks kept (after filtering):", len(all_chunks))
```

And run it. At this point, you can see:

-   total number of .txt files processed: `516`

-   total chunks kept (after filtering): `539057`

## Step 5: Train Word2 Vec on our Corpus

In Week 07, we used Word2Vec as a way to represent meaning geometrically: words that occur in similar contexts end up close together in vector space.

This week, we will use Word2Vec for a very practical purpose: expand a seed term into a list of conceptually related terms (so we can build a weakly supervised training set).

Our seed term will be:

-   `merchant`

We will then have Word2Vec look for terms that are in the same (geometric) neighborhood as [merchant]{.underline}.

OK, create a new file `step5_train_word2vec.py` and run it:

``` python
from pathlib import Path
import nltk
from gensim.utils import simple_preprocess
from gensim.models import Word2Vec

nltk.download("punkt", quiet=True)
nltk.download("punkt_tab", quiet=True)


# Load and preprocess all texts (same logic as Step 4)


TEXT_DIR = Path("texts")
txt_paths = sorted(TEXT_DIR.glob("*.txt"))

TARGET_WORDS = 120
MIN_WORDS = 5
MAX_WORDS = 200

def chunk_text(text, target_words=120):
    sentences = nltk.sent_tokenize(text)

    chunks = []
    current = []
    current_len = 0

    for sent in sentences:
        words = sent.split()
        if not words:
            continue

        if current_len + len(words) > target_words and current:
            chunks.append(" ".join(current))
            current = []
            current_len = 0

        current.append(sent)
        current_len += len(words)

    if current:
        chunks.append(" ".join(current))

    return chunks

token_lists = []

print(f"Found {len(txt_paths)} text files.")
print("Building token lists...")

for path in txt_paths:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        text = f.read()

    chunks = chunk_text(text, TARGET_WORDS)

    for c in chunks:
        tokens = simple_preprocess(c, deacc=True)
        if MIN_WORDS <= len(tokens) <= MAX_WORDS:
            token_lists.append(tokens)

print("\nTotal tokenized chunks kept:", len(token_lists))

################
# Train Word2Vec (same logic as what we did in week 07)
################

print("\nTraining Word2Vec...")

model = Word2Vec(
    sentences=token_lists,
    vector_size=200,   # dimensionality of word vectors
    window=5,          # context window size
    min_count=5,       # ignore very rare words
    workers=4,         # adjust depending on your machine (see Week 07)
    sg=1               # 1 = skip-gram; 0 = CBOW
)

# Save model

Path("models").mkdir(exist_ok=True)
model_path = Path("models") / "w2v_full.bin"
model.save(str(model_path))

print("\nModel saved to:", model_path)
```

This will take a little while (or more than a little while, depending on your machine). Once it's done, it will tell you: `Model saved to: models\w2v_full.bin`.

### Checking the semantic neighborhood

Now, we are going to load the model and ask for neighbors of our seed concept (that is, what words are near "merchant" in the vector space?).

So, create another file, `step5b_query_word2vec.py`, and run it:

``` python
from pathlib import Path
from gensim.models import Word2Vec

model_path = Path("models") / "w2v_full.bin"
model = Word2Vec.load(str(model_path))

seed = "merchant"

if seed not in model.wv:
    print(f"'{seed}' not found in the model vocabulary.")
    print("This usually means min_count is too high or the corpus is too small.")
else:
    print(f"Top 30 words similar to '{seed}':")
    for word, score in model.wv.similar_by_word(seed, topn=30):
        print(f"  {word:20s} {score:.3f}")
```

My output is:

`Top 30 words similar to 'merchant':`

`marchant 0.736`

`factor 0.702`

`merchants 0.689`

`jeweller 0.667`

`customer 0.665`

`purser 0.648`

`tailor 0.639`

`wholesale 0.637`

`clothier 0.635`

`worshipful 0.632`

`vintner 0.629`

`clothyer 0.620`

`adventurer 0.620`

`trade 0.620`

`brewer 0.619`

`venturer 0.617`

`chapman 0.616`

`factory 0.610`

`seller 0.610`

`staple 0.606`

`tradesman 0.606`

`adventurers 0.606`

`sailor 0.604`

`horner 0.603`

`staplers 0.599`

`easterling 0.599`

`banker 0.598`

`marchants 0.595`

`goldsmith 0.594`

`apprentice 0.592`

What did you get? Anything surprising or simply unclear? The list here makes a lot of sense to me. A historical note: "Easterling merchants" would have been merchants from the Baltic regions, so that works in this list. What we have created is a [corpus-specific semantic map]{.underline} for the concept of "merchant."

We will now use this list as a resource for weak supervision in the next steps in our classification task.

## Step 6: Build a tiered keyword list (A/B/C)

Word2Vec gives us a list of words “near” our seed term (`merchant`). But we do not want to blindly treat Word2Vec neighbors as truth. Instead, we use them as a *suggestions* and then impose an interpretive structure.

A useful strategy is to split candidate terms into tiers:

-   Tier A (high confidence): direct hits and spelling variants\
    e.g., `merchant`, `marchant`, `merchants`, `marchants`
-   Tier B (strongly related roles): terms that usually indicate commercial activity\
    e.g., `factor`, `chapman`, `adventurer`, `venturer`, `staple`
-   Tier C (maybe / adjacent): occupational neighborhood terms that might appear in commercial contexts but are often broader\
    e.g., `tailor`, `clothier`, `haberdasher`

This tiering is a modeling decision:

-   Tier A+B will become our **CORE triggers** (high precision).

-   Tier C will become a **MAYBE set** (to inspect separately).

Create a new file step6_define_tiers.py and run it:

``` python
# Tier A: direct spellings / variants of the seed concept
TIER_A = {
    "merchant", "merchants",
    "marchant", "marchants"
}

# Tier B: closely related commercial roles / terms
TIER_B = {
    "factor", "chapman",
    "adventurer", "adventurers",
    "venturer", "venturers",
    "staple", "staplers",
    "trade",
    "purser"
}

# Tier C: "maybe" occupational neighborhood (often adjacent, not always merchant-specific)
TIER_C = {
    "clothier", "clothyer",
    "tailor", "tayler",
    "haberdasher",
    "goldsmith",
    "vintner",
    "brewer",
    "banker",
    "grazier",
    "jeweller"
}

print("Tier A size:", len(TIER_A))
print("Tier B size:", len(TIER_B))
print("Tier C size:", len(TIER_C))

print("\nTier A:", sorted(TIER_A))
print("\nTier B:", sorted(TIER_B))
print("\nTier C:", sorted(TIER_C))
```

Let’s walk through what each tier represents and why I set them up this way:

-   Tier A: terms are direct references to the concept we care about (spelling variants common in early modern texts);

-   Tier B: these terms describe occupations and roles that often involve trade; institutional or economic positions adjacent to merchants; vocabulary that appears frequently in commercial contexts. But they are not perfect synonyms for “merchant.”

-   Tier C: these terms are occupational nouns that are often related to production, craft, or commerce, and they frequently appear near merchant discourse. But they are much more ambiguous.

This is where interpretive judgment enters the pipeline:

-   We decide what our category means.

-   We decide which words count as strong evidence vs. weak evidence.

-   We acknowledge that “merchant discourse” is fuzzy at the edges.

## Step 7: Label chunks using tiered keywords (weak supervision)

We now have:

-   a large set of sentence-based chunks,
-   tokenized into word lists,
-   and a tiered list of keywords (A / B / C).

The next step is to assign labels to chunks. This is where we can translate our decision making about tiers into rule-based labeling of the chunks. This is called **weak supervision**.

### Our labeling rules

For each chunk:

-   CORE (label = 1) → contains *any* Tier A or Tier B word\
    (high confidence merchant discourse)

-   MAYBE (label = 2) → contains Tier C words *only*\
    (ambiguous / adjacent cases)

-   NEG (label = 0) → contains none of the above

These rules encode our interpretive decisions from Step 6. OK, now we can get started with the actual labelling.

### Labeling the full corpus

Create a new file: `step7_label_chunks.py` and run it.

``` python
from pathlib import Path
import nltk
from gensim.utils import simple_preprocess
import json

nltk.download("punkt", quiet=True)
nltk.download("punkt_tab", quiet=True)

#
# Tier definitions (from Step 6)


TIER_A = {
    "merchant", "merchants",
    "marchant", "marchants"
}

TIER_B = {
    "factor", "chapman",
    "adventurer", "adventurers",
    "venturer", "venturers",
    "staple", "staplers",
    "trade", "purser"
}

TIER_C = {
    "clothier", "clothyer",
    "tailor", "tayler",
    "haberdasher",
    "goldsmith",
    "vintner",
    "brewer",
    "banker",
    "grazier",
    "jeweller"
}


# Load and process texts


TEXT_DIR = Path("texts")
txt_paths = sorted(TEXT_DIR.glob("*.txt"))

TARGET_WORDS = 120
MIN_WORDS = 5
MAX_WORDS = 200

def chunk_text(text, target_words=120):
    sentences = nltk.sent_tokenize(text)

    chunks = []
    current = []
    current_len = 0

    for sent in sentences:
        words = sent.split()
        if not words:
            continue

        if current_len + len(words) > target_words and current:
            chunks.append(" ".join(current))
            current = []
            current_len = 0

        current.append(sent)
        current_len += len(words)

    if current:
        chunks.append(" ".join(current))

    return chunks

labeled = []

print(f"Processing {len(txt_paths)} files...")

for path in txt_paths:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        text = f.read()

    chunks = chunk_text(text, TARGET_WORDS)

    for c in chunks:
        tokens = simple_preprocess(c, deacc=True)
        n_tokens = len(tokens)

        if not (MIN_WORDS <= n_tokens <= MAX_WORDS):
            continue

        token_set = set(tokens)

        if token_set & (TIER_A | TIER_B):
            label = 1   # CORE
        elif token_set & TIER_C:
            label = 2   # MAYBE
        else:
            label = 0   # NEG

        labeled.append((c, label))

print("Total chunks labeled:", len(labeled))


# Save labeled data


Path("data").mkdir(exist_ok=True)

with open(Path("data") / "merchant_labeled_chunks.json", "w", encoding="utf-8") as f:
    json.dump(labeled, f, ensure_ascii=False)

print("Saved labeled chunks to data/merchant_labeled_chunks.json")
```

What we did so far: we applied rules we designed, using the lexical triggers we chose, to create a labeled dataset. Obviously, these labels are imperfect and corpus-specific, but they are good enough to train a first classifier.

At this point, we have turned unstructured text into a supervised learning dataset. All the steps that we are doing next (train/test splits, classifiers, confusion matrices) depend on the choices made up to here. We can't quite train a classifier, yet. First, we need to separate CORE, MAYBE, and NEG examples, inspect them, and think carefully about evaluation.

This is where questions of [**precision** and **recall**](https://en.wikipedia.org/wiki/Precision_and_recall) enter the picture.

## Step 8: Sample and Inspect

At the end of `step7_label_chunks.py` add the following code and run it.

``` python
print("\nLabel distribution:")
print(f"  CORE (1): {sum(1 for _, y in labeled if y == 1)}")
print(f"  NEG  (0): {sum(1 for _, y in labeled if y == 0)}")
print(f"  MAYBE(2): {sum(1 for _, y in labeled if y == 2)}")

# Sample one from each category
for label_name, label_val in [("CORE", 1), ("MAYBE", 2), ("NEG", 0)]:
    example = next((text for text, y in labeled if y == label_val), None)
    if example:
        print(f"\n{label_name} example (first 200 chars):")
        print(example[:200])
```

This code will count how many chunks we got for each label and show examples of each type. When you get the results:

-   Does the CORE example actually contain merchant language?

<!-- -->

-   Does the MAYBE example feel ambiguous?

<!-- -->

-   Does the NEG example clearly not relate to merchants?

## Step 9: Decide what to train on

Now we have three kinds of labeled chunks of text:

-   CORE (label = 1): High-confidence merchant discourse\
-   MAYBE (label = 2): Ambiguous, adjacent, or borderline cases\
-   NEG (label = 0): Clearly not merchant discourse

Before we actually train a classifier, we must decide which of these labels should the model actually learn from. For this week, we will:

-   Train the classifier only on CORE vs. NEG
-   Exclude MAYBE from training
-   Save MAYBE for later inspection and interpretation

Why? The honest answer is: I tried a couple of options and compared with my own readings of the texts, this gave the best results at this stage. But I also want to note the MAYBE category contains texts that are close to commerce but not clearly merchant discourse and so require a lot of contextual interpretation. They are also texts where other scholars may disagree with my reading. This means that including them for this training, would reduce interpretability and make evaluation harder to explain.

Instead, we treat the MAYBE texts as a set of texts to which we apply the trained classifier later. This mirrors how supervised models are often used in practice: - trained on clear cases, - applied to ambiguous ones.

### Separate datasets

Create a new file `step9_prepare_datasets.py` . **Before** you run it, read the explanation about the NEG step after the code sample. *Then* run it.

``` python
import json
from pathlib import Path
import random

random.seed(42) # For reproducibility--same random sample each time [see (*) below]

DATA_PATH = Path("data") / "merchant_labeled_chunks.json"

with open(DATA_PATH, "r", encoding="utf-8") as f:
    labeled = json.load(f)

core = [(t, 1) for (t, y) in labeled if y == 1]
neg  = [(t, 0) for (t, y) in labeled if y == 0]
maybe = [t for (t, y) in labeled if y == 2]

print("Loaded:")
print("  CORE:", len(core))
print("  NEG :", len(neg))
print("  MAYBE:", len(maybe))

###### => NEG step [see explanation below]
neg_sample = random.sample(neg, len(core))

training_data = core + neg_sample
random.shuffle(training_data)

print("Training set size (CORE + NEG):", len(training_data))

### Split the data into train and test sets:

split = int(0.8 * len(training_data))
train_data = training_data[:split]
test_data  = training_data[split:]

print("Train size:", len(train_data))
print("Test size :", len(test_data))
print("MAYBE size:", len(maybe))

# Save the datasets:

Path("data").mkdir(exist_ok=True)

with open(Path("data") / "train_core_vs_neg.json", "w", encoding="utf-8") as f:
    json.dump(train_data, f, ensure_ascii=False)

with open(Path("data") / "test_core_vs_neg.json", "w", encoding="utf-8") as f:
    json.dump(test_data, f, ensure_ascii=False)

with open(Path("data") / "maybe_texts.json", "w", encoding="utf-8") as f:
    json.dump(maybe, f, ensure_ascii=False)

print("Saved training, test, and MAYBE datasets.")
```

With this step in the code we are [downsampling]{.underline} NEG to match the size of the CORE set. Why? \[Think for a second about what most of the texts might look like...\] Well, in most corpora, NEG examples vastly outnumber CORE examples: *most* of the text is *not* likely to be talking about the concept/topic you are targeting. Now, the texts that we have for class *are* a little skewed towards "merchant" and, as you will see after you run the code, we would still have an overwhelming number of NEG chunks, but I want you to learn best practices *in general*!

Training directly on all NEG data can cause the model to:

-   always predict “not merchant”

-   achieve high accuracy but low usefulness

OK, this is what I got back from this step:

`Loaded:`

`CORE: 11373`

`NEG : 527174` \[**!!!**\]

`MAYBE: 510`

`Training set size (CORE + NEG): 22746` \[**Important:** can you see where this is coming from based on the above explanation? And, can you see why we needed to downsample?\]

`Train size: 18196`

`Test size : 4550`

`MAYBE size: 510`

**One more point (\*)**: we use Python's built-in pseudo-random number generator to, first, `random.sample()` from the full NEG list when we are downsampling the NEG to the same size as CORE \[note that we are asking it to select as many NEG as `len(core)`\]. Second, we use random.shuffle() to randomly reorder the combined CORE+NEG list because *otherwise* when we do our 80/20, all the CORE examples would be first and all the NEG examples would be after (**not** what we want!).

## Week 9 (Part II--The Classification Task)

Now we are ready to create an actual classifier. So far, our training data looks like this:

-   a chunk of text (a string),

-   paired with a label (CORE = 1 or NEG = 0).

We can read these strings directly (and have done so to check things along the way), but obviously computers cannot. A classifier cannot operate on words, sentences, or meanings. It can only operate on numbers. So before we train a model, we must transform text into a numerical representation as we have been doing in previous weeks. In particular, we will revisit our old friend **TF-IDF** (remember our work in [week 4](https://astrid-giugni.github.io/IDS_570_TAD/week-04-representation.html)).

Our next steps:

1.  Convert text chunks into TF-IDF vectors. This helps emphasize *distinctive* vocabulary rather than common function words.

2.  Train a simple binary classifier

3.  Evaluate its performance on held-out data

4.  Apply it to the MAYBE set (review above if needed)

5.  Evaluate on what the model learned and what it did not, and see if it is useful for historical interpretation.

As with week 8, we are going to build everything step by step. Create a new file, `step10_tfidf_representation.py`. Don't run it yet, I will give explanations below and I want you to **read** and understand them before running the code.

``` python
import json
from pathlib import Path

from sklearn.feature_extraction.text import TfidfVectorizer
DATA_DIR = Path("data")

#We now load the datasets we prepared at the end of last week (step 9)
with open(DATA_DIR / "train_core_vs_neg.json", "r", encoding="utf-8") as f:
    train_data = json.load(f)

with open(DATA_DIR / "test_core_vs_neg.json", "r", encoding="utf-8") as f:
    test_data = json.load(f)

# Separate texts and labels
X_train_texts = [t for (t, y) in train_data]
y_train = [y for (t, y) in train_data]

X_test_texts = [t for (t, y) in test_data]
y_test = [y for (t, y) in test_data]

print("Train size:", len(X_train_texts))
print("Test size :", len(X_test_texts))

##### ==> See explanation [A] below

vectorizer = TfidfVectorizer(
    lowercase=True,
    min_df=5,        # ignore very rare words
    max_df=0.9       # ignore extremely common words; Explanation [B]
)
X_train = vectorizer.fit_transform(X_train_texts)
X_test = vectorizer.transform(X_test_texts)

print("TF-IDF matrix shapes:")
print("  Train:", X_train.shape)
print("  Test :", X_test.shape)

#####  ==> See explanation [C] below
```

-   Explanation \[A\]: after the lines above, we still have raw text, so we have to define a TF-IDF vectorizer.

-   Explanation \[B\]: we set the parameters as `min_df=5` so that we remove words that appear [in fewer than]{.underline} 5 documents; and `max_df=0.9` so that we remove words that appear in more than 90% of the documents.

-   Explanation \[C\]: we fit the vectorizer on the training data **only**, then apply it to the test data. Each text chunk is now a numeric vector.

We have:

| Object    | What it contains | What it’s used for                |
|-----------|------------------|-----------------------------------|
| `X_train` | TF-IDF vectors   | Model learns patterns             |
| `y_train` | Labels           | Supervision signal                |
| `X_test`  | TF-IDF vectors   | Model makes predictions           |
| `y_test`  | Labels           | We check how good predictions are |

I am going to belabor this more so that it's fully clear, you can think of the X, Y part of this as:

X = the input to the model → the text, represented numerically (TF-IDF vectors)

y = the output we want the model to predict → the labels (CORE = 1, NEG = 0)

So:

X_train = text chunks (as numbers)

y_train = labels for those chunks

Where **train** = data the model is allowed to learn from; **test** = data the model has *never seen* before.

[When you run step 10]{.underline}, your output will look like:

`Train size: 36874`

`Test size : 9218`

`TF-IDF matrix shapes:`

`Train: (36874, 27806)`

`Test : (9218, 27806)`

This is reassuring as it is what we want: we have the 80/20 split we created in step 9: 36,874 chunks were used to train the model; 9,218 chunks were held out for evaluation. So far so good.

In addition, the TF-IDF matrix has the shape (number of documents) by (number of features), that is (36874, 27806) translates to a matrix that has:

-   36874 rows, where each row is for a training chunk. Each row is a TF-IDF vector representing a chunk.

-   27,806 columns, where each column is for a word feature. Each column corresponds to a specific word in the vocabulary. Yes, we have 27,806 features! That's not a problem: remember, TF-IDF matrices are sparse, linear models do well with this kind of setting (see the info below about the choice of logistic regression), and this is normal for text data.

#### Looking ahead to the next step (confusion matrix):

Later, when we compute a confusion matrix:

-   predictions come from `X_test`

-   “true labels” come from `y_test`

The confusion matrix will help us understand how often the model’s predictions on unseen data match the labels produced by our weak supervision rules. I will remind you below, but you want to start keeping in mind that the confusion matrix does *not* measure truth, it just measures agreement with a labeling scheme.

### Train a classifier:

Now we can *finally* train a classifier using a **logistic regression** model. We discussed how a logistic regression works in class (or will do so in Week 9, if you are reading ahead). If you want an **advanced** version of this, see [chapter 4](https://web.stanford.edu/~jurafsky/slp3/4.pdf) in Jurafsky and Martin.

-   Note: we are **not** going to go into the details covered in section 4.4 forward. For the simple reason that it would require more mathematical background than I assume for the class to discuss the loss function and gradients. (We will touch on the materials from section 4.9 on how to build a confusion matrix.)

There are many classifiers we could use. We choose logistic regression for three main reasons:

-   It is simple and well understood. It works well with a binary task such as ours: CORE (1) vs NEG (0). It models:

Logistic regression is literally designed to model:

$$
P(y =1 \mid x)
$$

So it gives you: a probability of CORE for each chunk (via `predict_proba` ) and a clean decision rule: “predict CORE if probability ≥ 0.5”.

-   It works very well with TF-IDF features. TF-IDF produces very high-dimensional vectors (tens of thousands to 100k+ features) which are mostly zeros (sparse). Linear models such as logistic regression work well for these situations (they are fast to train and are stable).

-   **Note:** the default threshold in scikit-learn is 0.5. As we saw in class, logistic regression models the log-odds of class membership and a probability of 0.5 corresponds to equal odds. This is reasonable when classes are roughly balanced in the training setup (recall that we decided to downsample the negatives above). We *could* change the threshold if we had good reasons to do so. I will bring this point up again at the end.

There *are* other options (linear support vector machine or naive Bayes), but we choose logistic regression because it is a strong, interpretable baseline for binary text classification with TF-IDF features. TF-IDF creates high-dimensional sparse vectors, and linear models are known to perform very well in this setting. Logistic regression also outputs probabilities, which lets us interpret model confidence and apply the model to ambiguous cases (our MAYBE set).

Create a new file, `step11_train_classifier.py`. We will build the script in chunks with explanations at each step and you will run it once it's all put together.

``` python
from pathlib import Path
import json

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    roc_auc_score
)
```

Next, we load the data and use the TF-IDF representations we created in [step 10]{.underline} above.

``` python
from sklearn.feature_extraction.text import TfidfVectorizer

DATA_DIR = Path("data")

with open(DATA_DIR / "train_core_vs_neg.json", "r", encoding="utf-8") as f:
    train_data = json.load(f)

with open(DATA_DIR / "test_core_vs_neg.json", "r", encoding="utf-8") as f:
    test_data = json.load(f)

X_train_texts = [t for (t, y) in train_data]
y_train = [y for (t, y) in train_data]

X_test_texts = [t for (t, y) in test_data]
y_test = [y for (t, y) in test_data]

vectorizer = TfidfVectorizer(
    lowercase=True,
    min_df=5,
    max_df=0.9
)

X_train = vectorizer.fit_transform(X_train_texts)
X_test = vectorizer.transform(X_test_texts)
```

Now we actually [train]{.underline} the classifier (yes, I know, finally!). What will happen in this section of the script: the model examines the TF-IDF features and it learns the weights that separate CORE from NEG. We then make predictions on the test set.

``` python
clf = LogisticRegression(
    max_iter=1000,
    n_jobs=1
)

clf.fit(X_train, y_train)

#test set predictions
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]
```

At this point we evaluate our classifier with a confusion matrix that compares:

-   true labels (`y_test`)

-   model predictions (`y_pred`)

``` python
cm = confusion_matrix(y_test, y_pred)
print("Confusion matrix:")
print(cm)
```

I will go over what this does in more detail below (step 12). But the next step is to include two more evaluation components. [First]{.underline}, a classification report that summarizes how reliable positive predictions are (= **precision**); how many positives the model found (= **recall**); and the balance of precision and recall (**F1-score**). [Second]{.underline}, [**ROC AUC**](https://library.virginia.edu/data/articles/roc-curves-and-auc-for-models-used-for-binary-classification), which measures how well the model separates CORE from NEG across all thresholds.

Add the next bit of code to your script and then run everything in `step11_train_classifier.py`.

``` python
print("\nClassification report:")
print(classification_report(y_test, y_pred))

###ROC AUC
auc = roc_auc_score(y_test, y_prob)
print("ROC AUC:", round(auc, 3))
```

### Evaluating the classifier (step 12):

Now that we have trained the classifier, computed the confusion matrix and ROC AUC, we can evaluate how closely do the model’s predictions agree with the labels in our test set, given a particular decision rule.

Let's start with the confusion matrix. When you ran step 11, you should have gotten something like this:

\[\[6943 27\]

\[ 367 1881\]\]

What does this mean? I am going to present it as a table to make its entries legible:

|                     | Predicted NEG (0) | Predicted CORE (1) |
|---------------------|-------------------|--------------------|
| **Actual NEG (0)**  | 6943              | 27                 |
| **Actual CORE (1)** | 367               | 1881               |

Each cell tells us something different about model behavior. By convention in scikit-learn:

-   Rows = actual (true) labels

-   Columns = predicted labels

So we can rewrite it structurally as:

\begin{bmatrix} \text{TN} & \text{FP} \\ \text{FN} & \text{TP} \end{bmatrix}

To make this clear:

-   True negatives (TN) = 6943 → 6943 texts were labeled NEG in the test set, and the model correctly predicted NEG for them.

-   False Positive (FP) = 27 → 27 texts were labeled NEG in the test set, and the model incorrectly predicted CORE for them.

-   False Negatives (FN) = 367 → 367 texts were labeled CORE in the test set, and the model incorrectly predicted NEG for them.

-   True Positives (TP) = 1881 → 1881 texts were labeled CORE in the test set, and the model correctly predicted CORE for them.

**Interpretation:** our decision threshold in the logistic regression was 0.5. That is, if the predicted probability ≥ 0.5, label the text CORE; if the predicted probability \< 0.5, label the text NEG. The model is very good at recognizing texts that do *not* match our merchant criteria. It's fairly good at avoiding predicting as positive texts that are labelled negative (27 false positives), but we do lose a fair amount of CORE labeled texts (367).

The confusion matrix is not the only validation tool we have. Let's look at the classification report. You should have gotten something like this:

### Classification Report

| Class | Precision | Recall | F1-Score | Support |
|-------|-----------|--------|----------|---------|
| 0     | 0.95      | 1.00   | 0.97     | 6,970   |
| 1     | 0.99      | 0.84   | 0.91     | 2,248   |

What do these numbers mean?

[**For CORE (class 1)**]{.underline}**:**

[Precision]{.underline} (1) = 0.99

Formula: $\text{Precision} = \frac{TP}{TP+FP}$

In our case, this tells us that when the model predicts CORE, it is correct 99% of the time. This matches your tiny number of false positives (27).

[Recall]{.underline} (1) = 084

The formula: $\text{Recall} = \frac{TP}{TP + FN}$

The model finds 84% of the CORE texts and it misses 16% (those 367 false negatives).

[F1-score]{.underline} (1) = 0.91

F1 balances precision and recall.

The formula: $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$

0.91 is a strong F1 score, but lower than precision because recall is lower.

I will leave it up to you to think through the values for the NEG class.

As part of our report, we also had the following values:

| Metric       | Precision | Recall | F1-Score | Support (total) |
|--------------|-----------|--------|----------|-----------------|
| Accuracy     |           |        | 0.96\*   | 9,218           |
| Macro avg    | 0.97      | 0.92   | 0.94     | 9,218           |
| Weighted avg | 0.96      | 0.96   | 0.96     | 9,218           |

(\*) I know, I know, this is weird: the placement of **accuracy** in the F1-score column is just a formatting convention from sklearn's classification report. The 0.96 value had to go somewhere, and they chose that column.

So, what is actually going on besides formatting weirdness:

[Accuracy]{.underline} = 0.96

Formula: $\frac{TP + TN}{\text{Total}}$

Interpretation: the model agrees with the test labels 96% of the time. In general accuracy alone is not very informative in imbalanced tasks, which is why we look at precision and recall. But recall that we downsampled NEG to match CORE, so we had a balanced training set. Nonetheless, precision and recall are still very useful in understanding the *kinds* of errors the model is making.

### Macro average

Here we take a simple average of class metrics to treat both classes equally:

$$
\frac{Metric_0 + Metric_1}{2}
$$

### Weighted average

Weights each class by its support (number of examples). Since NEG is larger (6970 vs 2248), the weights workout to:

weight_neg = $\frac{6970}{9218}$ or about 0.756

weight_core = $\frac{2248}{9218}$ or about 0.244

The weighted average multiplies each class’s score by the proportion of examples in that class. Since NEG makes up about 76% of the test set and performs very well, the weighted average is pulled toward the NEG score and ends up close to overall accuracy.

[For example, the weighted F1]{.underline}:

Weighted F1 is:

$(w_0 \times F1_0) + (w_1 \times F1_1)$

Substitute values:

$(0.756 \times 0.97) + (0.244 \times 0.91)$

Compute each term:

$0.756 \times 0.97 ≈ 0.733$

$0.244 \times 0.91 ≈ 0.222$

Add them and round $≈0.96$

The weighted F1-score is close to overall accuracy because the model performs strongly on both classes and the larger class (NEG) dominates the weighting.

### ROC AUC = 0.996

Now the most important conceptual point in evaluating our model: ROC AUC does not use the 0.5 threshold. It evaluates how well the model separates the two classes across all possible thresholds.

How does ROC AUC work? Instead of using one specific cutoff, it changes the threshold from 0 to 1 and tests how well the model separates CORE from NEG *at each threshold.* Then it evaluates performance across all those possibilities.

Splitting this up into its components:

**ROC** stands for Receiver Operating Characteristic.

It plots:

-   TPR: True Positive Rate (same as Recall) on the y-axis

-   FPR: False Positive Rate on the x-axis

As the threshold moves from 0 to 1, the curve traces out the model's performance. If the model:

-   perfectly separates classes → curve hugs the top-left corner (high TPR, low FPR at all thresholds)

-   guesses randomly → curve follows a diagonal line (50/50 at every threshold)

**AUC** stands for Area Under the Curve. It compresses all this information into a single number (for us, 0.996). AUC is literally the area under the ROC curve. It's the integral of the True Positive Rate with respect to the False Positive Rate as we vary the threshold. This is why AUC ranges from 0.5 (diagonal line, area of triangle = 0.5) to 1.0 (perfect corner, full square = 1.0).

**Math aside:** you don't need to worry about computing this ourself, but here's the integral:

$$\text{AUC} = \int_0^1 \text{TPR}(t) \, d(\text{FPR}(t))$$

You can think of it this way: if I randomly choose one CORE chunk and one NEG chunk, the AUC tells you the probability that the model assigns a higher probability to the CORE chunk.

Just a reminder: you can see a full [**explanation here**](https://library.virginia.edu/data/articles/roc-curves-and-auc-for-models-used-for-binary-classification).

We have a value of 0.996, which means that the model almost perfectly ranks CORE texts above NEG texts. This is good! Unlike accuracy or F1-score which depend on choosing a threshold, ROC AUC tells us whether our weak supervision approach is fundamentally learning the right pattern. A high AUC (like 0.996) means our lexicon-based labels are capturing something real about "merchant" discourse.

### Coming back to threshold choice:

Remember that I mentioned that we could change the threshold for the classification task if we had a good reason to do so. We now want to go back and think through this issue.

First of all, you might want to ask: Does high ROC-AUC mean that our 0.5 threshold performed well? We want to be careful with this question:

Our high ROC-AUC (0.996) tells us the model is excellent at ranking texts. It consistently assigns higher probabilities to CORE than NEG. This means that if you sorted all chunks by their probability scores, true CORE chunks would appear near the top and true NEG chunks near the bottom 99.6% of the time, regardless of where you draw the threshold line.

-   True CORE chunks get higher probability scores than true NEG chunks (ranking). Whether they cross the 0.5 threshold is a separate question (classification)

As an example, you could have:

CORE chunk with prob = 0.48 (ranked above NEG chunks, but classified as NEG at 0.5) NEG chunk with prob = 0.02 (ranked below CORE chunks, classified correctly as NEG)

The model correctly ranked them (CORE \> NEG), which gives high AUC, even though the CORE chunk was misclassified at the 0.5 threshold.

This is important because we do **not** want to misread a high AUC as indicating that we picked (or were given by default) the optimal threshold for our specific research goals. As I mentioned earlier, the 0.5 threshold is a reasonable default when:

-   Classes are balanced (we downsampled)

-   False positives and false negatives are equally costly (In general, this depends on your research question)

Now, we *might* want to explore other thresholds. Then we *could* pick one of the following approaches:

-   Plot precision and recall at different thresholds

-   Optimize for F1-score

-   Make a domain-driven decision based on your research needs (if you ask me, this is always a question you should have in mind, but, at the end of the day, I am a humanist..)

I am going to give you a couple of scenarios where this might be reasonable to do:

-   I sometimes need a discovery tool to find texts (that talk about "merchant" concepts) that I might have missed. I would then lower the threshold (0.3-0.4) for high recall

-   On the other hand, if I am selecting passages for close reading and want only clear examples of the discourse, then I would pick a higher threshold (0.6-0.7) for high precision

-   If you want a balanced approach for further analysis (our case) → 0.5 is reasonable

For our purposes, we'll stick with 0.5 since our evaluation metrics (precision, recall, F1) all look strong at this threshold, and it aligns with our balanced training set.

We can also visualize this in one graph to see how different threshold values affect both precision and recall. As we move to the right on the x-axis and the threshold increases, we can see that precision goes up (we get fewer false positives) but recall goes down (more false negatives).

![](images/week_9-01.png)

### Saving TF-IDF vectorizer and classifier:

We will reuse what we did in this tutorial for Week 10, so we want to save our work!

``` Python
from pathlib import Path
import joblib

MODEL_DIR = Path.cwd() / "models"
MODEL_DIR.mkdir(exist_ok=True)

joblib.dump(vectorizer, MODEL_DIR / "tfidf_vectorizer.joblib")
joblib.dump(clf, MODEL_DIR / "merchant_logreg.joblib")

print("Saved TF-IDF vectorizer and classifier to /models/")

```
