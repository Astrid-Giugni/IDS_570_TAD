---
title: "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python"
editor: visual
format: html
execute:
  echo: true
  warning: false
  message: false
---

```{r}
#| include: false

library(reticulate)

use_python("C:/Users/astri/miniconda3/envs/tad/python.exe", required = TRUE)
py_config()
```

# Weeks 8 and 9 goals:

So far, we have done unsupervised and semi-structured approaches to text analysis (frequency, TF–IDF, similarity, co-occurrence, and Word2Vec). During these two weeks we are going to make a key transition to supervised methods. I am grouping the two together because we need to take some time to think about how to *thoughtfully* set up the classification task–language is flexible and conceptual boundaries are porous!

You can move across the code and steps for these two weeks at your own pace as you get familiar with the process. [In class]{.underline}: I will move through the lectures *assuming* that everyone will finish working through this entire tutorial by March 6th (the end of week 9).

[**For Week 8**]{.underline}:

The goal is to **train a supervised classifier** that learns to recognize a category of texts. In particular, we are going to figure out how to distinguish which texts engage in merchant-related discourse versus those that do not.

In humanities (and social sciences), supervised learning is often the workhorse method for answering questions like:

-   Is this text pro-trade or protectionist?
-   Does this paragraph contain “merchant discourse”?
-   Which documents are about X, versus not-X?

But supervised learning raises a methodological question immediately: Where do labels come from?

In humanities contexts, we often do **not** begin with a fully human-labeled dataset. So we will begin with **weak supervision**: we will generate *imperfect* labels using rules, then train a model, and validate it carefully. So, for week 8, you will:

1.  Define a category you want to detect in texts (a binary label). For us, it will be about the concept "merchant" (this will be our "seed" concept).
2.  Segment texts into a usable unit of analysis (chunks).
3.  Create a weakly supervised training dataset using a seed term and related terms.

[**For Week 9**]{.underline}**:**

1.  Train a **baseline text classifier** in Python using:

    -   TF–IDF features
    -   Logistic regression

2.  Evaluate the classifier using:

    -   a train/test split
    -   a confusion matrix
    -   precision, recall, and F1 score

3.  Reflect on what “accuracy” means when labels come from weak supervision.

## Big picture workflow

Here is the full pipeline we will build:

1.  Load texts from a folder (`texts/`): [you will **need** to get the 500+ files that I uploaded on Canvas under "Train_Text_Documents."]{.underline}
2.  Segment each text into sentence-based chunks (our “documents” for classification)
3.  Use Word2Vec (trained on our corpus) to expand a seed concept (seed: `merchant`)
4.  Build a search-word list (Tier A/B/C: more details below)
5.  Use the search-word list to create weak labels
6.  Create a CORE vs NEG dataset (and optionally a MAYBE set)
7.  Split into train and test
8.  Train a classifier (TF–IDF + logistic regression)
9.  Evaluate and interpret

We will move slowly and validate outputs at each stage. Again, I am keeping the training wheels on, but

## What you need before starting

-   VSCode installed
-   A project folder like: `IDS_570_TAD/`
-   A subfolder: `texts/` containing your `.txt` files
-   A Python virtual environment (`.venv`) activated in VSCode
-   Packages we will use:
    -   `nltk`
    -   `gensim`
    -   `scikit-learn`
    -   `tqdm`

(We will install these together in the steps below.)

## Week 8 (Part I--Week Supervision)

## Step 0: Project setup & environment

Before we do any modeling, we need to make sure our project is set up correctly. This step is mainly for those of you **new to Python**, but it also will help everyone learn good project organization skills: many errors in text analysis come from working in the wrong folder or environment.

Where:

-   `texts/` contains the raw `.txt` files you want to analyze.
-   `data/` will store intermediate outputs (JSON files, inspection samples).
-   `models/` will store trained models (e.g., Word2Vec, classifiers).
-   Each `stepX_*.py` file does one conceptual task.

We intentionally separate steps into different scripts so that: - each step is easy to test and debug - you can rerun part of the pipeline without rerunning everything - you can clearly see how the pipeline is constructed.

### Opening the terminal in VSCode; Virtual environment set up

Make sure you open VSCode [in the project folder]{.underline} (`IDS_570_TAD`).

Then open the [**terminal**](https://code.visualstudio.com/docs/terminal/getting-started): there are a ton of videos online, if you have never done it before. If you are using something other than VSCode, you can also find guides online.

You should see something like:

\`\`\`bash PS D:\Users...\IDS\_570_TAD\>

If you see a different folder, you are in the wrong place. This is important because we are going to set up a Python virtual environment to keep project-specific packages isolated.

Next, create the environment and activate it:

`python -m venv .venv`

then, in Windows:

`.venv\Scripts\Activate.ps1`

And in Mac/Linux

`source .venv/bin/activate`

For full instructions, see [**here**](https://docs.python.org/3/library/venv.html). Your terminal prompt should now include `.venv`.

### Installing required packages

Install the libraries we will use:

`pip install nltk gensim scikit-learn tqdm`

We will download one additional resource (NLTK sentence tokenizer) later.

Before moving on, check:

-   Can you see your `.txt` files from Python?

-   Does `python step1_load_texts.py` run without errors?

-   Is your terminal in the correct folder?

If something is off here, it is worth fixing **before** you continue: go back through the steps above and see if something is missing. There are also a lot of videos online that help you go through this process.

## Step 1: Loading, Inspecting, and Segmenting Texts

Before we segment, vectorize, or classify anything, we need to confirm that Python can see and read our texts correctly.

### Reading text files from a folder

We will assume all `.txt` files are stored in a folder called `texts/`. Now, create a new file and name it: `step1_load_texts.py` and then run it:

``` python
from pathlib import Path

TEXT_DIR = Path("texts")

# Collect all .txt files
files = sorted(TEXT_DIR.glob("*.txt"))

print(f"Found {len(files)} .txt files.")

# Print the first few filenames
print("First 10 files:")
for f in files[:10]:
    print(" ", f)
```

This should tell you that you found [about]{.underline} 516 .txt files. **Note**: you have a [very]{.underline} slightly different training set; you are **responsible** for keeping track of how this affects things down the line (though the first few steps should match). It's my way of keeping you on your toes!

You should see a list of filenames printed to the terminal. Now we want to inspect one file to make sure that we have set things up correctly (**note**: in the future, I will [**assume**]{.underline} that you will do this on your own!). To the same file as above, add the following and run the script again:

``` python
# Read one example file
example_file = files[0]

with open(example_file, "r", encoding="utf-8", errors="ignore") as f:
    text = f.read()

print("\nReading file:")
print(example_file)
print("-" * 40)
print("Number of characters:", len(text))
print("\nFirst 1,000 characters:\n")
print(text[:1000])
```

We are just making sure that we can load the data and that things look correct (that is, we have funky looking Early Modern texts). In my case (and it should be in your too), the file read was: `texts\A00419.txt`; number of characters: 2888252l and then you should get the first few lines of the text.

## Step 2: Segmenting Texts into Chunks

Our `.txt` files are far too large to classify as single units. A supervised classifier expects many medium-sized examples and some of the texts I gave you are *very* long (the one above had 2,888,252 characters!). So we need to decide what counts as a “document” for classification.

-   For this week, we will treat chunks of sentences as our unit of analysis. We're using sentence chunks because we want to capture enough context for semantic meaning (a single sentence might be ambiguous), but not so much that we get incorrect labels (a 50-page chapter likely discusses many topics and might lead to mislabelling: such a large "chunk" may not be about "merchants" in any meaningful sense, but, for example, it could be a biblical passage mentioning a merchants *metaphorically*–we would need a more careful approach for these kinds of texts).

**Note:** There is no universally correct choice here; segmentation is a modeling decision and I am basing the "chunk" size on experience.

We will begin by splitting each text into sentences using NLTK. I have two pieces of documentation for NLTK that I recommend, if you want to delve into it: [**here**](https://www.nltk.org/) and [[**here**]{.underline}](https://guides.library.upenn.edu/penntdm/python/nltk).

Create a new file: `step2_segment_and_chunk.py` . We will chunk the texts and check some basic diagnostics to make sure that the code is actually running properly.

``` python
from pathlib import Path
import nltk

# Download required tokenizers (run once)
nltk.download("punkt", quiet=True)
nltk.download("punkt_tab", quiet=True)

TEXT_DIR = Path("texts")
txt_paths = sorted(TEXT_DIR.glob("*.txt"))

sample_path = txt_paths[0]

with open(sample_path, "r", encoding="utf-8", errors="ignore") as f:
    text = f.read()

# Split text into sentences
sentences = nltk.sent_tokenize(text)

print("File:", sample_path)
print("Number of sentences:", len(sentences))
print()

# Group sentences into chunks of ~120 words
TARGET_WORDS = 120

chunks = []
current = []
current_len = 0

for sent in sentences:
    words = sent.split()
    if not words:
        continue

    # If adding this sentence would exceed the target, finalize the chunk
    if current_len + len(words) > TARGET_WORDS and current:
        chunks.append(" ".join(current))
        current = []
        current_len = 0

    current.append(sent)
    current_len += len(words)

# Add any leftover sentences
if current:
    chunks.append(" ".join(current))

print("Number of chunks:", len(chunks))

# Diagnostics on chunk length (rough word counts)
lengths = [len(c.split()) for c in chunks]
lengths_sorted = sorted(lengths)

print()
print("Approx word counts per chunk:")
print("  min:", min(lengths))
print("  median:", lengths_sorted[len(lengths_sorted)//2])
print("  max:", max(lengths))

lo, hi = 5, 200
in_range = sum(lo <= n <= hi for n in lengths)
print(f"Chunks with {lo}–{hi} words:", in_range)
print("Share in range:", round(in_range / len(lengths), 3))

print()
print("--- Chunk 1 preview (first 400 chars) ---")
print(chunks[0][:400])
```

And run the script.

[You should see]{.underline}:

-   Thousands of chunks (`5172`)

-   A median chunk length around 100–120 words (`median: 101`)

-   Most chunks falling in the 5–200 word range (`5009`; with the share in range: `0.968`)

-   And a preview of the first chunk.

This tells us that our segmentation choice is reasonable for classification.

## Step 3: Tokenize Chunks for Modelling

This step will be conceptually familiar from when we used `unnest_tokens()` in tidytext.\
we transform a text column into word tokens.

We will do this using [`gensim.utils.simple_preprocess()`](https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html).

Create a new file: `step3_tokenize_chunks.py`. This new script is going to **build on** what we did in the "step2" script, s you will find the same logic repeated at the beginning.

``` python
from pathlib import Path
import nltk
from gensim.utils import simple_preprocess

nltk.download("punkt", quiet=True)
nltk.download("punkt_tab", quiet=True)

TEXT_DIR = Path("texts")
txt_paths = sorted(TEXT_DIR.glob("*.txt"))
sample_path = txt_paths[0]

with open(sample_path, "r", encoding="utf-8", errors="ignore") as f:
    text = f.read()

# Step 2 logic: sentences -> chunks (~120 words)
sentences = nltk.sent_tokenize(text)

TARGET_WORDS = 120
chunks = []
current = []
current_len = 0

for sent in sentences:
    words = sent.split()
    if not words:
        continue

    if current_len + len(words) > TARGET_WORDS and current:
        chunks.append(" ".join(current))
        current = []
        current_len = 0

    current.append(sent)
    current_len += len(words)

if current:
    chunks.append(" ".join(current))

# Step 3: now we tokenize each chunk
token_lists = [simple_preprocess(c, deacc=True) for c in chunks]

print("File:", sample_path)
print("Chunks (strings):", len(chunks))
print("Chunks (token lists):", len(token_lists))

print("\n--- Token preview (first 60 tokens of first chunk) ---")
print(token_lists[0][:60])

print("\n--- Token count of first chunk ---")
print(len(token_lists[0]))
```

Now run it. What we are doing:

-   `simple_preprocess()` returns a **list of tokens**.

-   Tokens are lowercased and cleaned.

-   This is now in the format required by Word2Vec:

    -   Word2Vec expects `sentences=[["token","token",...], ...]`

The output should look like:

`Chunks (strings): 5172`

`Chunks (token lists): 5172`

`--- Token preview (first 60 tokens of first chunk) --- ['the', 'first', 'booke', 'of', 'the', 'covntrie', 'farme', 'chap', 'what', 'manner', 'of', 'husbandrie', 'is', 'entreated', 'of', 'in', 'the', 'discourse', 'following', 'even', 'as', 'the', 'manner', 'of', 'building', 'vsed', 'at', 'this', 'day', 'the', 'varietie', 'of', 'countries', 'causeth', 'diuers', 'manner', 'of', 'labouring', 'of', 'the', 'earth']`

`--- Token count of first chunk --- 41`

### ⚠️ **Note of warning:** 

gensim may look “stuck” the first time you import it. When you import `gensim` (especially on Windows), Python may appear to freeze for 10–60 seconds. This is normal and does **not** mean your code is broken.

-   `gensim` loads compiled components used for Word2Vec
-   on Windows, this initial load can be slow
-   Python does not print progress messages during this step If the terminal is not showing an error message, wait patiently before interrupting the process. Only stop the program if it has been unresponsive for several minutes.

## Step 4: Process all Files and Filter Chunks

So far, we have been working with a single example file to make sure that things are working. Now we are ready to apply the same segmentation and tokenization steps to the entire corpus. We are going to make the following sampling decisions:

-   discard chunks that have fewer than 5 tokens (not enough context)

-   discard chunks that have more than 200 tokens (too diffuse and hard to interpret)

Again, these thresholds are based on experience in working with Early Modern texts: they are reasonable defaults to start with and we could always change them if things don't work out. The **goal** is to define a reasonable and usable training example.

Create a new file: `step4_process_all_files.py`.

``` python
from pathlib import Path
import nltk
from gensim.utils import simple_preprocess

nltk.download("punkt", quiet=True)
nltk.download("punkt_tab", quiet=True)

TEXT_DIR = Path("texts")
txt_paths = sorted(TEXT_DIR.glob("*.txt"))

TARGET_WORDS = 120
MIN_WORDS = 5
MAX_WORDS = 200

all_chunks = []
all_token_lists = []

def chunk_text(text, target_words=120):
    sentences = nltk.sent_tokenize(text)

    chunks = []
    current = []
    current_len = 0

    for sent in sentences:
        words = sent.split()
        if not words:
            continue

        if current_len + len(words) > target_words and current:
            chunks.append(" ".join(current))
            current = []
            current_len = 0

        current.append(sent)
        current_len += len(words)

    if current:
        chunks.append(" ".join(current))

    return chunks

print(f"Found {len(txt_paths)} text files.")
print()

for path in txt_paths:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        text = f.read()

    chunks = chunk_text(text, TARGET_WORDS)

    for c in chunks:
        tokens = simple_preprocess(c, deacc=True)
        n_tokens = len(tokens)

        # Filter by length
        if MIN_WORDS <= n_tokens <= MAX_WORDS:
            all_chunks.append(c)
            all_token_lists.append(tokens)

print("Total chunks kept (after filtering):", len(all_chunks))
```

And run it. At this point, you can see:

-   total number of .txt files processed: `516`

-   total chunks kept (after filtering): `539057`

## Step 5: Train Word2 Vec on our Corpus

In Week 07, we used Word2Vec as a way to represent meaning geometrically: words that occur in similar contexts end up close together in vector space.

This week, we will use Word2Vec for a very practical purpose: expand a seed term into a list of conceptually related terms (so we can build a weakly supervised training set).

Our seed term will be:

-   `merchant`

We will then have Word2Vec look for terms that are in the same (geometric) neighborhood as [merchant]{.underline}.

OK, create a new file `step5_train_word2vec.py` and run it:

``` Python
from pathlib import Path
import nltk
from gensim.utils import simple_preprocess
from gensim.models import Word2Vec

nltk.download("punkt", quiet=True)
nltk.download("punkt_tab", quiet=True)


# Load and preprocess all texts (same logic as Step 4)


TEXT_DIR = Path("texts")
txt_paths = sorted(TEXT_DIR.glob("*.txt"))

TARGET_WORDS = 120
MIN_WORDS = 5
MAX_WORDS = 200

def chunk_text(text, target_words=120):
    sentences = nltk.sent_tokenize(text)

    chunks = []
    current = []
    current_len = 0

    for sent in sentences:
        words = sent.split()
        if not words:
            continue

        if current_len + len(words) > target_words and current:
            chunks.append(" ".join(current))
            current = []
            current_len = 0

        current.append(sent)
        current_len += len(words)

    if current:
        chunks.append(" ".join(current))

    return chunks

token_lists = []

print(f"Found {len(txt_paths)} text files.")
print("Building token lists...")

for path in txt_paths:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        text = f.read()

    chunks = chunk_text(text, TARGET_WORDS)

    for c in chunks:
        tokens = simple_preprocess(c, deacc=True)
        if MIN_WORDS <= len(tokens) <= MAX_WORDS:
            token_lists.append(tokens)

print("\nTotal tokenized chunks kept:", len(token_lists))

################
# Train Word2Vec (same logic as what we did in week 07)
################

print("\nTraining Word2Vec...")

model = Word2Vec(
    sentences=token_lists,
    vector_size=200,   # dimensionality of word vectors
    window=5,          # context window size
    min_count=5,       # ignore very rare words
    workers=4,         # adjust depending on your machine (see Week 07)
    sg=1               # 1 = skip-gram; 0 = CBOW
)

# Save model

Path("models").mkdir(exist_ok=True)
model_path = Path("models") / "w2v_full.bin"
model.save(str(model_path))

print("\nModel saved to:", model_path)
```

This will take a little while (or more than a little while, depending on your machine). Once it's done, it will tell you: `Model saved to: models\w2v_full.bin`.

### Checking the semantic neighborhood

Now, we are going to load the model and ask for neighbors of our seed concept (that is, what words are near "merchant" in the vector space?).

So, create another file, `step5b_query_word2vec.py`, and run it:

``` Python
from pathlib import Path
from gensim.models import Word2Vec

model_path = Path("models") / "w2v_full.bin"
model = Word2Vec.load(str(model_path))

seed = "merchant"

if seed not in model.wv:
    print(f"'{seed}' not found in the model vocabulary.")
    print("This usually means min_count is too high or the corpus is too small.")
else:
    print(f"Top 30 words similar to '{seed}':")
    for word, score in model.wv.similar_by_word(seed, topn=30):
        print(f"  {word:20s} {score:.3f}")
```

My output is:

`Top 30 words similar to 'merchant':`

`marchant 0.736`

`factor 0.702`

`merchants 0.689`

`jeweller 0.667`

`customer 0.665`

`purser 0.648`

`tailor 0.639`

`wholesale 0.637`

`clothier 0.635`

`worshipful 0.632`

`vintner 0.629`

`clothyer 0.620`

`adventurer 0.620`

`trade 0.620`

`brewer 0.619`

`venturer 0.617`

`chapman 0.616`

`factory 0.610`

`seller 0.610`

`staple 0.606`

`tradesman 0.606`

`adventurers 0.606`

`sailor 0.604`

`horner 0.603`

`staplers 0.599`

`easterling 0.599`

`banker 0.598`

`marchants 0.595`

`goldsmith 0.594`

`apprentice 0.592`

What did you get? Anything surprising or simply unclear? The list here makes a lot of sense to me. A historical note: "Easterling merchants" would have been merchants from the Baltic regions, so that works in this list. What we have created is a [corpus-specific semantic map]{.underline} for the concept of "merchant."

We will now use this list as a resource for weak supervision in the next steps in our classification task.

## Step 6: Build a tiered keyword list (A/B/C)

Word2Vec gives us a list of words “near” our seed term (`merchant`). But we do not want to blindly treat Word2Vec neighbors as truth. Instead, we use them as a *suggestions* and then impose an interpretive structure.

A useful strategy is to split candidate terms into tiers:

-   Tier A (high confidence): direct hits and spelling variants\
    e.g., `merchant`, `marchant`, `merchants`, `marchants`
-   Tier B (strongly related roles): terms that usually indicate commercial activity\
    e.g., `factor`, `chapman`, `adventurer`, `venturer`, `staple`
-   Tier C (maybe / adjacent): occupational neighborhood terms that might appear in commercial contexts but are often broader\
    e.g., `tailor`, `clothier`, `haberdasher`

This tiering is a modeling decision:

-   Tier A+B will become our **CORE triggers** (high precision).

-   Tier C will become a **MAYBE set** (to inspect separately).

Create a new file step6_define_tiers.py and run it:

``` Python
# Tier A: direct spellings / variants of the seed concept
TIER_A = {
    "merchant", "merchants",
    "marchant", "marchants"
}

# Tier B: closely related commercial roles / terms
TIER_B = {
    "factor", "chapman",
    "adventurer", "adventurers",
    "venturer", "venturers",
    "staple", "staplers",
    "trade",
    "purser"
}

# Tier C: "maybe" occupational neighborhood (often adjacent, not always merchant-specific)
TIER_C = {
    "clothier", "clothyer",
    "tailor", "tayler",
    "haberdasher",
    "goldsmith",
    "vintner",
    "brewer",
    "banker",
    "grazier",
    "jeweller"
}

print("Tier A size:", len(TIER_A))
print("Tier B size:", len(TIER_B))
print("Tier C size:", len(TIER_C))

print("\nTier A:", sorted(TIER_A))
print("\nTier B:", sorted(TIER_B))
print("\nTier C:", sorted(TIER_C))
```

Let’s walk through what each tier represents and why I set them up this way:

-   Tier A: terms are direct references to the concept we care about (spelling variants common in early modern texts);

-   Tier B: these terms describe occupations and roles that often involve trade; institutional or economic positions adjacent to merchants; vocabulary that appears frequently in commercial contexts. But they are not perfect synonyms for “merchant.”

-   Tier C: these terms are occupational nouns that are often related to production, craft, or commerce, and they frequently appear near merchant discourse. But they are much more ambiguous.

This is where interpretive judgment enters the pipeline:

-    We decide what our category means.

-    We decide which words count as strong evidence vs. weak evidence.

-    We acknowledge that “merchant discourse” is fuzzy at the edges.

## Step 7: Label chunks using tiered keywords (weak supervision)

We now have:

-   a large set of sentence-based chunks,
-   tokenized into word lists,
-   and a tiered list of keywords (A / B / C).

The next step is to assign labels to chunks. This is where we can translate our decision making about tiers into rule-based labeling of the chunks. This is called **weak supervision**.

### Our labeling rules

For each chunk:

-   CORE (label = 1) → contains *any* Tier A or Tier B word\
    (high confidence merchant discourse)

-   MAYBE (label = 2) → contains Tier C words *only*\
    (ambiguous / adjacent cases)

-   NEG (label = 0) → contains none of the above

These rules encode our interpretive decisions from Step 6. OK, now we can get started with the actual labelling.

### Labeling the full corpus

Create a new file: `step7_label_chunks.py` and run it.

``` Python
from pathlib import Path
import nltk
from gensim.utils import simple_preprocess
import json

nltk.download("punkt", quiet=True)
nltk.download("punkt_tab", quiet=True)

#
# Tier definitions (from Step 6)


TIER_A = {
    "merchant", "merchants",
    "marchant", "marchants"
}

TIER_B = {
    "factor", "chapman",
    "adventurer", "adventurers",
    "venturer", "venturers",
    "staple", "staplers",
    "trade", "purser"
}

TIER_C = {
    "clothier", "clothyer",
    "tailor", "tayler",
    "haberdasher",
    "goldsmith",
    "vintner",
    "brewer",
    "banker",
    "grazier",
    "jeweller"
}


# Load and process texts


TEXT_DIR = Path("texts")
txt_paths = sorted(TEXT_DIR.glob("*.txt"))

TARGET_WORDS = 120
MIN_WORDS = 5
MAX_WORDS = 200

def chunk_text(text, target_words=120):
    sentences = nltk.sent_tokenize(text)

    chunks = []
    current = []
    current_len = 0

    for sent in sentences:
        words = sent.split()
        if not words:
            continue

        if current_len + len(words) > target_words and current:
            chunks.append(" ".join(current))
            current = []
            current_len = 0

        current.append(sent)
        current_len += len(words)

    if current:
        chunks.append(" ".join(current))

    return chunks

labeled = []

print(f"Processing {len(txt_paths)} files...")

for path in txt_paths:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        text = f.read()

    chunks = chunk_text(text, TARGET_WORDS)

    for c in chunks:
        tokens = simple_preprocess(c, deacc=True)
        n_tokens = len(tokens)

        if not (MIN_WORDS <= n_tokens <= MAX_WORDS):
            continue

        token_set = set(tokens)

        if token_set & (TIER_A | TIER_B):
            label = 1   # CORE
        elif token_set & TIER_C:
            label = 2   # MAYBE
        else:
            label = 0   # NEG

        labeled.append((c, label))

print("Total chunks labeled:", len(labeled))


# Save labeled data


Path("data").mkdir(exist_ok=True)

with open(Path("data") / "merchant_labeled_chunks.json", "w", encoding="utf-8") as f:
    json.dump(labeled, f, ensure_ascii=False)

print("Saved labeled chunks to data/merchant_labeled_chunks.json")
```

What we did so far: we applied rules we designed, using the lexical triggers we chose, to create a labeled dataset. Obviously, these labels are imperfect and corpus-specific, but they are good enough to train a first classifier.

At this point, we have turned unstructured text into a supervised learning dataset. All the steps that we are doing next (train/test splits, classifiers, confusion matrices) depend on the choices made up to here. We can't quite train a classifier, yet. First, we need to separate CORE, MAYBE, and NEG examples, inspect them, and think carefully about evaluation.

This is where questions of [**precision** and **recall**](https://en.wikipedia.org/wiki/Precision_and_recall) enter the picture.

## Step 9: Decide what to train on

Now we have three kinds of labeled chunks of text:

-   CORE (label = 1): High-confidence merchant discourse\
-   MAYBE (label = 2): Ambiguous, adjacent, or borderline cases\
-   NEG (label = 0): Clearly not merchant discourse

Before we actually train a classifier, we must decide which of these labels should the model actually learn from. For this week, we will:

-   Train the classifier only on CORE vs. NEG
-   Exclude MAYBE from training
-   Save MAYBE for later inspection and interpretation

Why? The honest answer is: I tried a couple of options and compared with my own readings of the texts, this gave the best results at this stage. But I also want to note the MAYBE category contains texts that are close to commerce but not clearly merchant discourse and so require a lot of contextual interpretation. They are also texts where other scholars may disagree with my reading. This means that including them for this training, would reduce interpretability and make evaluation harder to explain.

Instead, we treat the MAYBE texts as a set of texts to which we apply the trained classifier later. This mirrors how supervised models are often used in practice: - trained on clear cases, - applied to ambiguous ones.

### Separate datasets

Create a new file `step9_prepare_datasets.py` . **Before** you run it, read the explanation about the NEG step after the code sample. *Then* run it.

``` Python
import json
from pathlib import Path
import random

random.seed(42)

DATA_PATH = Path("data") / "merchant_labeled_chunks.json"

with open(DATA_PATH, "r", encoding="utf-8") as f:
    labeled = json.load(f)

core = [(t, 1) for (t, y) in labeled if y == 1]
neg  = [(t, 0) for (t, y) in labeled if y == 0]
maybe = [t for (t, y) in labeled if y == 2]

print("Loaded:")
print("  CORE:", len(core))
print("  NEG :", len(neg))
print("  MAYBE:", len(maybe))

###### => NEG step [see explanation below]
neg_sample = random.sample(neg, len(core))

training_data = core + neg_sample
random.shuffle(training_data)

print("Training set size (CORE + NEG):", len(training_data))

### Split the data into train and test sets:

split = int(0.8 * len(training_data))
train_data = training_data[:split]
test_data  = training_data[split:]

print("Train size:", len(train_data))
print("Test size :", len(test_data))
print("MAYBE size:", len(maybe))

# Save the datasets:

Path("data").mkdir(exist_ok=True)

with open(Path("data") / "train_core_vs_neg.json", "w", encoding="utf-8") as f:
    json.dump(train_data, f, ensure_ascii=False)

with open(Path("data") / "test_core_vs_neg.json", "w", encoding="utf-8") as f:
    json.dump(test_data, f, ensure_ascii=False)

with open(Path("data") / "maybe_texts.json", "w", encoding="utf-8") as f:
    json.dump(maybe, f, ensure_ascii=False)

print("Saved training, test, and MAYBE datasets.")
```

With this step in the code we are [downsampling]{.underline} NEG to match the size of the CORE set. Why? \[Think for a second about what most of the texts might look like...\] Well, in most corpora, NEG examples vastly outnumber CORE examples: *most* of the text is *not* likely to be talking about the concept/topic you are targeting. Now, the texts that we have for class *are* a little skewed towards "merchant" and, as you will see after you run the code, we would still have an overwhelming number of NEG chunks, but I want you to learn best practices *in general*!

Training directly on all NEG data can cause the model to:

-   always predict “not merchant”

-   achieve high accuracy but low usefulness

OK, this is what I got back from this step:

`Loaded:`

`CORE: 11373`

`NEG : 527174` \[**!!!**\]

`MAYBE: 510`

`Training set size (CORE + NEG): 22746` \[**Important:** can you see where this is coming from based on the above explanation? And, can you see why we needed to downsample?\]

`Train size: 18196`

`Test size : 4550`

`MAYBE size: 510`

## Week 9 (Part II--The Classification Task)

Now we are ready to create an actual classifier. So far, our training data looks like this:

-   a chunk of text (a string),

-   paired with a label (CORE = 1 or NEG = 0).

We can read these strings directly (and have done so to check things along the way), but obviously computers cannot. A classifier cannot operate on words, sentences, or meanings. It can only operate on numbers. So before we train a model, we must transform text into a numerical representation as we have been doing in previous weeks. In particular, we will revisit our old friend **TF-IDF** (remember our work in [week 4](https://astrid-giugni.github.io/IDS_570_TAD/week-04-representation.html)).

Our next steps:

1.  Convert text chunks into TF-IDF vectors. This helps emphasize *distinctive* vocabulary rather than common function words.

2.  Train a simple binary classifier

3.  Evaluate its performance on held-out data

4.  Apply it to the MAYBE set (review above if needed)

5.  Evaluate on what the model learned and what it did not, and see if it is useful for historical interpretation.

As with week 8, we are going to build everything step by step. Create a new file, `step10_tfidf_representation.py`. Don't run it yet, I will give explanations below and I want you to **read** and understand them before running the code.

``` Python
import json
from pathlib import Path

from sklearn.feature_extraction.text import TfidfVectorizer
DATA_DIR = Path("data")

#We now load the datasets we prepared at the end of last week (step 9)
with open(DATA_DIR / "train_core_vs_neg.json", "r", encoding="utf-8") as f:
    train_data = json.load(f)

with open(DATA_DIR / "test_core_vs_neg.json", "r", encoding="utf-8") as f:
    test_data = json.load(f)

# Separate texts and labels
X_train_texts = [t for (t, y) in train_data]
y_train = [y for (t, y) in train_data]

X_test_texts = [t for (t, y) in test_data]
y_test = [y for (t, y) in test_data]

print("Train size:", len(X_train_texts))
print("Test size :", len(X_test_texts))

##### ==> See explanation [A] below

vectorizer = TfidfVectorizer(
    lowercase=True,
    min_df=5,        # ignore very rare words
    max_df=0.9       # ignore extremely common words; Explanation [B]
)
X_train = vectorizer.fit_transform(X_train_texts)
X_test = vectorizer.transform(X_test_texts)

print("TF-IDF matrix shapes:")
print("  Train:", X_train.shape)
print("  Test :", X_test.shape)

#####  ==> See explanation [C] below
```

-   Explanation \[A\]: after the lines above, we still have raw text, so we have to define a TF-IDF vectorizer.

-   Explanation \[B\]: we set the parameters as `min_df=5` so that we remove words that appear [in fewer than]{.underline} 5 documents; and `max_df=0.9` so that we remove words that appear in more than 90% of the documents.

Explanation \[C\]: we fit the vectorizer on the training data **only**, then apply it to the test data. Each text chunk is now a numeric vector.

We have:

| Object    | What it contains | What it’s used for                |
|-----------|------------------|-----------------------------------|
| `X_train` | TF-IDF vectors   | Model learns patterns             |
| `y_train` | Labels           | Supervision signal                |
| `X_test`  | TF-IDF vectors   | Model makes predictions           |
| `y_test`  | Labels           | We check how good predictions are |

I am going to belabor this more so that it's fully clear, you can think of the X, Y part of this as:

X = the input to the model → the text, represented numerically (TF-IDF vectors)

y = the output we want the model to predict → the labels (CORE = 1, NEG = 0)

So:

X_train = text chunks (as numbers)

y_train = labels for those chunks

Where **train** = data the model is allowed to learn from; **test** = data the model has *never seen* before.

#### Looking ahead to the next step (confusion matrix):

Later, when we compute a confusion matrix:

-    predictions come from `X_test`

-   “true labels” come from `y_test`

The confusion matrix will help us understand how often the model’s predictions on unseen data match the labels produced by our weak supervision rules. I will remind you below, but you want to start keeping in mind that the confusion matrix does *not* measure truth, it just measures agreement with a labeling scheme.
