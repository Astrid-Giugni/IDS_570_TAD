<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Weeks 08 &amp; 9: Weak Supervision &amp; Supervised Text Classification in Python – Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b57d1312aa7c10f38068bb8d7c282b76.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Text as Data</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./week-01-introduction.html">Weeks</a></li><li class="breadcrumb-item"><a href="./week-08-09-word2vec.html">Week 08 and 09:Weak Supervision &amp; Supervised Text Classification in Python</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Course</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Weeks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-01-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 01: Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-02-basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 02: Basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-03-dictionaries.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 03: Dictionaries</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-04-representation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 04: Text Representation (1)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-05-representation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 05: Text Representation (2)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-06-cooccurrence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 06: Co-occurrence &amp; PMI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-07-word2vec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 07: Word2Vec and LDA introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-08-09-word2vec.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Week 08 and 09:Weak Supervision &amp; Supervised Text Classification in Python</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./week-01-introduction.html">Weeks</a></li><li class="breadcrumb-item"><a href="./week-08-09-word2vec.html">Week 08 and 09:Weak Supervision &amp; Supervised Text Classification in Python</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Weeks 08 &amp; 9: Weak Supervision &amp; Supervised Text Classification in Python</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="weeks-8-and-9-goals" class="level1">
<h1>Weeks 8 and 9 goals:</h1>
<p>So far, we have done unsupervised and semi-structured approaches to text analysis (frequency, TF–IDF, similarity, co-occurrence, and Word2Vec). During these two weeks we are going to make a key transition to supervised methods. I am grouping the two together because we need to take some time to think about how to <em>thoughtfully</em> set up the classification task–language is flexible and conceptual boundaries are porous!</p>
<p>You can move across the code and steps for these two weeks at your own pace as you get familiar with the process. <u>In class</u>: I will move through the lectures <em>assuming</em> that everyone will finish working through this entire tutorial by March 6th (the end of week 9).</p>
<p><u><strong>For Week 8</strong></u>:</p>
<p>The goal is to <strong>train a supervised classifier</strong> that learns to recognize a category of texts. In particular, we are going to figure out how to distinguish which texts engage in merchant-related discourse versus those that do not.</p>
<p>In humanities (and social sciences), supervised learning is often the workhorse method for answering questions like:</p>
<ul>
<li>Is this text pro-trade or protectionist?</li>
<li>Does this paragraph contain “merchant discourse”?</li>
<li>Which documents are about X, versus not-X?</li>
</ul>
<p>But supervised learning raises a methodological question immediately: Where do labels come from?</p>
<p>In humanities contexts, we often do <strong>not</strong> begin with a fully human-labeled dataset. So we will begin with <strong>weak supervision</strong>: we will generate <em>imperfect</em> labels using rules, then train a model, and validate it carefully. So, for week 8, you will:</p>
<ol type="1">
<li>Define a category you want to detect in texts (a binary label). For us, it will be about the concept “merchant” (this will be our “seed” concept).</li>
<li>Segment texts into a usable unit of analysis (chunks).</li>
<li>Create a weakly supervised training dataset using a seed term and related terms.</li>
</ol>
<p><u><strong>For Week 9</strong></u><strong>:</strong></p>
<ol type="1">
<li><p>Train a <strong>baseline text classifier</strong> in Python using:</p>
<ul>
<li>TF–IDF features</li>
<li>Logistic regression</li>
</ul></li>
<li><p>Evaluate the classifier using:</p>
<ul>
<li>a train/test split</li>
<li>a confusion matrix</li>
<li>precision, recall, and F1 score</li>
</ul></li>
<li><p>Reflect on what “accuracy” means when labels come from weak supervision.</p></li>
</ol>
<section id="big-picture-workflow" class="level2">
<h2 class="anchored" data-anchor-id="big-picture-workflow">Big picture workflow</h2>
<p>Here is the full pipeline we will build:</p>
<ol type="1">
<li>Load texts from a folder (<code>texts/</code>): <u>you will <strong>need</strong> to get the 500+ files that I uploaded on Canvas under “Train_Text_Documents.”</u></li>
<li>Segment each text into sentence-based chunks (our “documents” for classification)</li>
<li>Use Word2Vec (trained on our corpus) to expand a seed concept (seed: <code>merchant</code>)</li>
<li>Build a search-word list (Tier A/B/C: more details below)</li>
<li>Use the search-word list to create weak labels</li>
<li>Create a CORE vs NEG dataset (and optionally a MAYBE set)</li>
<li>Split into train and test</li>
<li>Train a classifier (TF–IDF + logistic regression)</li>
<li>Evaluate and interpret</li>
</ol>
<p>We will move slowly and validate outputs at each stage. Again, I am keeping the training wheels on, but</p>
</section>
<section id="what-you-need-before-starting" class="level2">
<h2 class="anchored" data-anchor-id="what-you-need-before-starting">What you need before starting</h2>
<ul>
<li>VSCode installed</li>
<li>A project folder like: <code>IDS_570_TAD/</code></li>
<li>A subfolder: <code>texts/</code> containing your <code>.txt</code> files</li>
<li>A Python virtual environment (<code>.venv</code>) activated in VSCode</li>
<li>Packages we will use:
<ul>
<li><code>nltk</code></li>
<li><code>gensim</code></li>
<li><code>scikit-learn</code></li>
<li><code>tqdm</code></li>
</ul></li>
</ul>
<p>(We will install these together in the steps below.)</p>
</section>
<section id="week-8-part-iweek-supervision" class="level2">
<h2 class="anchored" data-anchor-id="week-8-part-iweek-supervision">Week 8 (Part I–Week Supervision)</h2>
</section>
<section id="step-0-project-setup-environment" class="level2">
<h2 class="anchored" data-anchor-id="step-0-project-setup-environment">Step 0: Project setup &amp; environment</h2>
<p>Before we do any modeling, we need to make sure our project is set up correctly. This step is mainly for those of you <strong>new to Python</strong>, but it also will help everyone learn good project organization skills: many errors in text analysis come from working in the wrong folder or environment.</p>
<p>Where:</p>
<ul>
<li><code>texts/</code> contains the raw <code>.txt</code> files you want to analyze.</li>
<li><code>data/</code> will store intermediate outputs (JSON files, inspection samples).</li>
<li><code>models/</code> will store trained models (e.g., Word2Vec, classifiers).</li>
<li>Each <code>stepX_*.py</code> file does one conceptual task.</li>
</ul>
<p>We intentionally separate steps into different scripts so that: - each step is easy to test and debug - you can rerun part of the pipeline without rerunning everything - you can clearly see how the pipeline is constructed.</p>
<section id="opening-the-terminal-in-vscode-virtual-environment-set-up" class="level3">
<h3 class="anchored" data-anchor-id="opening-the-terminal-in-vscode-virtual-environment-set-up">Opening the terminal in VSCode; Virtual environment set up</h3>
<p>Make sure you open VSCode <u>in the project folder</u> (<code>IDS_570_TAD</code>).</p>
<p>Then open the <a href="https://code.visualstudio.com/docs/terminal/getting-started"><strong>terminal</strong></a>: there are a ton of videos online, if you have never done it before. If you are using something other than VSCode, you can also find guides online.</p>
<p>You should see something like:</p>
<p>```bash PS D:…_570_TAD&gt;</p>
<p>If you see a different folder, you are in the wrong place. This is important because we are going to set up a Python virtual environment to keep project-specific packages isolated.</p>
<p>Next, create the environment and activate it:</p>
<p><code>python -m venv .venv</code></p>
<p>then, in Windows:</p>
<p><code>.venv\Scripts\Activate.ps1</code></p>
<p>And in Mac/Linux</p>
<p><code>source .venv/bin/activate</code></p>
<p>For full instructions, see <a href="https://docs.python.org/3/library/venv.html"><strong>here</strong></a>. Your terminal prompt should now include <code>.venv</code>.</p>
</section>
<section id="installing-required-packages" class="level3">
<h3 class="anchored" data-anchor-id="installing-required-packages">Installing required packages</h3>
<p>Install the libraries we will use:</p>
<p><code>pip install nltk gensim scikit-learn tqdm</code></p>
<p>We will download one additional resource (NLTK sentence tokenizer) later.</p>
<p>Before moving on, check:</p>
<ul>
<li><p>Can you see your <code>.txt</code> files from Python?</p></li>
<li><p>Does <code>python step1_load_texts.py</code> run without errors?</p></li>
<li><p>Is your terminal in the correct folder?</p></li>
</ul>
<p>If something is off here, it is worth fixing <strong>before</strong> you continue: go back through the steps above and see if something is missing. There are also a lot of videos online that help you go through this process.</p>
</section>
</section>
<section id="step-1-loading-inspecting-and-segmenting-texts" class="level2">
<h2 class="anchored" data-anchor-id="step-1-loading-inspecting-and-segmenting-texts">Step 1: Loading, Inspecting, and Segmenting Texts</h2>
<p>Before we segment, vectorize, or classify anything, we need to confirm that Python can see and read our texts correctly.</p>
<section id="reading-text-files-from-a-folder" class="level3">
<h3 class="anchored" data-anchor-id="reading-text-files-from-a-folder">Reading text files from a folder</h3>
<p>We will assume all <code>.txt</code> files are stored in a folder called <code>texts/</code>. Now, create a new file and name it: <code>step1_load_texts.py</code> and then run it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>TEXT_DIR <span class="op">=</span> Path(<span class="st">"texts"</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Collect all .txt files</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>files <span class="op">=</span> <span class="bu">sorted</span>(TEXT_DIR.glob(<span class="st">"*.txt"</span>))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Found </span><span class="sc">{</span><span class="bu">len</span>(files)<span class="sc">}</span><span class="ss"> .txt files."</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the first few filenames</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First 10 files:"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f <span class="kw">in</span> files[:<span class="dv">10</span>]:</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">" "</span>, f)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This should tell you that you found <u>about</u> 516 .txt files. <strong>Note</strong>: you have a <u>very</u> slightly different training set; you are <strong>responsible</strong> for keeping track of how this affects things down the line (though the first few steps should match). It’s my way of keeping you on your toes!</p>
<p>You should see a list of filenames printed to the terminal. Now we want to inspect one file to make sure that we have set things up correctly (<strong>note</strong>: in the future, I will <u><strong>assume</strong></u> that you will do this on your own!). To the same file as above, add the following and run the script again:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read one example file</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>example_file <span class="op">=</span> files[<span class="dv">0</span>]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(example_file, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>, errors<span class="op">=</span><span class="st">"ignore"</span>) <span class="im">as</span> f:</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Reading file:"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(example_file)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of characters:"</span>, <span class="bu">len</span>(text))</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">First 1,000 characters:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text[:<span class="dv">1000</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>We are just making sure that we can load the data and that things look correct (that is, we have funky looking Early Modern texts). In my case (and it should be in your too), the file read was: <code>texts\A00419.txt</code>; number of characters: 2888252l and then you should get the first few lines of the text.</p>
</section>
</section>
<section id="step-2-segmenting-texts-into-chunks" class="level2">
<h2 class="anchored" data-anchor-id="step-2-segmenting-texts-into-chunks">Step 2: Segmenting Texts into Chunks</h2>
<p>Our <code>.txt</code> files are far too large to classify as single units. A supervised classifier expects many medium-sized examples and some of the texts I gave you are <em>very</em> long (the one above had 2,888,252 characters!). So we need to decide what counts as a “document” for classification.</p>
<ul>
<li>For this week, we will treat chunks of sentences as our unit of analysis. We’re using sentence chunks because we want to capture enough context for semantic meaning (a single sentence might be ambiguous), but not so much that we get incorrect labels (a 50-page chapter likely discusses many topics and might lead to mislabelling: such a large “chunk” may not be about “merchants” in any meaningful sense, but, for example, it could be a biblical passage mentioning a merchants <em>metaphorically</em>–we would need a more careful approach for these kinds of texts).</li>
</ul>
<p><strong>Note:</strong> There is no universally correct choice here; segmentation is a modeling decision and I am basing the “chunk” size on experience.</p>
<p>We will begin by splitting each text into sentences using NLTK. I have two pieces of documentation for NLTK that I recommend, if you want to delve into it: <a href="https://www.nltk.org/"><strong>here</strong></a> and <a href="https://guides.library.upenn.edu/penntdm/python/nltk"><u><strong>here</strong></u></a>.</p>
<p>Create a new file: <code>step2_segment_and_chunk.py</code> . We will chunk the texts and check some basic diagnostics to make sure that the code is actually running properly.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Download required tokenizers (run once)</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"punkt"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"punkt_tab"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>TEXT_DIR <span class="op">=</span> Path(<span class="st">"texts"</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>txt_paths <span class="op">=</span> <span class="bu">sorted</span>(TEXT_DIR.glob(<span class="st">"*.txt"</span>))</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>sample_path <span class="op">=</span> txt_paths[<span class="dv">0</span>]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(sample_path, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>, errors<span class="op">=</span><span class="st">"ignore"</span>) <span class="im">as</span> f:</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Split text into sentences</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> nltk.sent_tokenize(text)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"File:"</span>, sample_path)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of sentences:"</span>, <span class="bu">len</span>(sentences))</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Group sentences into chunks of ~120 words</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>TARGET_WORDS <span class="op">=</span> <span class="dv">120</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>chunks <span class="op">=</span> []</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>current <span class="op">=</span> []</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>current_len <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> sentences:</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> sent.split()</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> words:</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If adding this sentence would exceed the target, finalize the chunk</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> current_len <span class="op">+</span> <span class="bu">len</span>(words) <span class="op">&gt;</span> TARGET_WORDS <span class="kw">and</span> current:</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        chunks.append(<span class="st">" "</span>.join(current))</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        current <span class="op">=</span> []</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        current_len <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    current.append(sent)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>    current_len <span class="op">+=</span> <span class="bu">len</span>(words)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Add any leftover sentences</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> current:</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>    chunks.append(<span class="st">" "</span>.join(current))</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of chunks:"</span>, <span class="bu">len</span>(chunks))</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Diagnostics on chunk length (rough word counts)</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>lengths <span class="op">=</span> [<span class="bu">len</span>(c.split()) <span class="cf">for</span> c <span class="kw">in</span> chunks]</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>lengths_sorted <span class="op">=</span> <span class="bu">sorted</span>(lengths)</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Approx word counts per chunk:"</span>)</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  min:"</span>, <span class="bu">min</span>(lengths))</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  median:"</span>, lengths_sorted[<span class="bu">len</span>(lengths_sorted)<span class="op">//</span><span class="dv">2</span>])</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  max:"</span>, <span class="bu">max</span>(lengths))</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>lo, hi <span class="op">=</span> <span class="dv">5</span>, <span class="dv">200</span></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>in_range <span class="op">=</span> <span class="bu">sum</span>(lo <span class="op">&lt;=</span> n <span class="op">&lt;=</span> hi <span class="cf">for</span> n <span class="kw">in</span> lengths)</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Chunks with </span><span class="sc">{</span>lo<span class="sc">}</span><span class="ss">–</span><span class="sc">{</span>hi<span class="sc">}</span><span class="ss"> words:"</span>, in_range)</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Share in range:"</span>, <span class="bu">round</span>(in_range <span class="op">/</span> <span class="bu">len</span>(lengths), <span class="dv">3</span>))</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"--- Chunk 1 preview (first 400 chars) ---"</span>)</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(chunks[<span class="dv">0</span>][:<span class="dv">400</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>And run the script.</p>
<p><u>You should see</u>:</p>
<ul>
<li><p>Thousands of chunks (<code>5172</code>)</p></li>
<li><p>A median chunk length around 100–120 words (<code>median: 101</code>)</p></li>
<li><p>Most chunks falling in the 5–200 word range (<code>5009</code>; with the share in range: <code>0.968</code>)</p></li>
<li><p>And a preview of the first chunk.</p></li>
</ul>
<p>This tells us that our segmentation choice is reasonable for classification.</p>
</section>
<section id="step-3-tokenize-chunks-for-modelling" class="level2">
<h2 class="anchored" data-anchor-id="step-3-tokenize-chunks-for-modelling">Step 3: Tokenize Chunks for Modelling</h2>
<p>This step will be conceptually familiar from when we used <code>unnest_tokens()</code> in tidytext.<br>
we transform a text column into word tokens.</p>
<p>We will do this using <a href="https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html"><code>gensim.utils.simple_preprocess()</code></a>.</p>
<p>Create a new file: <code>step3_tokenize_chunks.py</code>. This new script is going to <strong>build on</strong> what we did in the “step2” script, s you will find the same logic repeated at the beginning.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.utils <span class="im">import</span> simple_preprocess</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"punkt"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"punkt_tab"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>TEXT_DIR <span class="op">=</span> Path(<span class="st">"texts"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>txt_paths <span class="op">=</span> <span class="bu">sorted</span>(TEXT_DIR.glob(<span class="st">"*.txt"</span>))</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>sample_path <span class="op">=</span> txt_paths[<span class="dv">0</span>]</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(sample_path, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>, errors<span class="op">=</span><span class="st">"ignore"</span>) <span class="im">as</span> f:</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 logic: sentences -&gt; chunks (~120 words)</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> nltk.sent_tokenize(text)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>TARGET_WORDS <span class="op">=</span> <span class="dv">120</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>chunks <span class="op">=</span> []</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>current <span class="op">=</span> []</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>current_len <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> sentences:</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> sent.split()</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> words:</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> current_len <span class="op">+</span> <span class="bu">len</span>(words) <span class="op">&gt;</span> TARGET_WORDS <span class="kw">and</span> current:</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        chunks.append(<span class="st">" "</span>.join(current))</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        current <span class="op">=</span> []</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        current_len <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    current.append(sent)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    current_len <span class="op">+=</span> <span class="bu">len</span>(words)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> current:</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    chunks.append(<span class="st">" "</span>.join(current))</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: now we tokenize each chunk</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>token_lists <span class="op">=</span> [simple_preprocess(c, deacc<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> c <span class="kw">in</span> chunks]</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"File:"</span>, sample_path)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Chunks (strings):"</span>, <span class="bu">len</span>(chunks))</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Chunks (token lists):"</span>, <span class="bu">len</span>(token_lists))</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Token preview (first 60 tokens of first chunk) ---"</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(token_lists[<span class="dv">0</span>][:<span class="dv">60</span>])</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Token count of first chunk ---"</span>)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(token_lists[<span class="dv">0</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Now run it. What we are doing:</p>
<ul>
<li><p><code>simple_preprocess()</code> returns a <strong>list of tokens</strong>.</p></li>
<li><p>Tokens are lowercased and cleaned.</p></li>
<li><p>This is now in the format required by Word2Vec:</p>
<ul>
<li>Word2Vec expects <code>sentences=[["token","token",...], ...]</code></li>
</ul></li>
</ul>
<p>The output should look like:</p>
<p><code>Chunks (strings): 5172</code></p>
<p><code>Chunks (token lists): 5172</code></p>
<p><code>--- Token preview (first 60 tokens of first chunk) --- ['the', 'first', 'booke', 'of', 'the', 'covntrie', 'farme', 'chap', 'what', 'manner', 'of', 'husbandrie', 'is', 'entreated', 'of', 'in', 'the', 'discourse', 'following', 'even', 'as', 'the', 'manner', 'of', 'building', 'vsed', 'at', 'this', 'day', 'the', 'varietie', 'of', 'countries', 'causeth', 'diuers', 'manner', 'of', 'labouring', 'of', 'the', 'earth']</code></p>
<p><code>--- Token count of first chunk --- 41</code></p>
<section id="note-of-warning" class="level3">
<h3 class="anchored" data-anchor-id="note-of-warning">⚠️ <strong>Note of warning:</strong></h3>
<p>gensim may look “stuck” the first time you import it. When you import <code>gensim</code> (especially on Windows), Python may appear to freeze for 10–60 seconds. This is normal and does <strong>not</strong> mean your code is broken.</p>
<ul>
<li><code>gensim</code> loads compiled components used for Word2Vec</li>
<li>on Windows, this initial load can be slow</li>
<li>Python does not print progress messages during this step If the terminal is not showing an error message, wait patiently before interrupting the process. Only stop the program if it has been unresponsive for several minutes.</li>
</ul>
</section>
</section>
<section id="step-4-process-all-files-and-filter-chunks" class="level2">
<h2 class="anchored" data-anchor-id="step-4-process-all-files-and-filter-chunks">Step 4: Process all Files and Filter Chunks</h2>
<p>So far, we have been working with a single example file to make sure that things are working. Now we are ready to apply the same segmentation and tokenization steps to the entire corpus. We are going to make the following sampling decisions:</p>
<ul>
<li><p>discard chunks that have fewer than 5 tokens (not enough context)</p></li>
<li><p>discard chunks that have more than 200 tokens (too diffuse and hard to interpret)</p></li>
</ul>
<p>Again, these thresholds are based on experience in working with Early Modern texts: they are reasonable defaults to start with and we could always change them if things don’t work out. The <strong>goal</strong> is to define a reasonable and usable training example.</p>
<p>Create a new file: <code>step4_process_all_files.py</code>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.utils <span class="im">import</span> simple_preprocess</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"punkt"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"punkt_tab"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>TEXT_DIR <span class="op">=</span> Path(<span class="st">"texts"</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>txt_paths <span class="op">=</span> <span class="bu">sorted</span>(TEXT_DIR.glob(<span class="st">"*.txt"</span>))</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>TARGET_WORDS <span class="op">=</span> <span class="dv">120</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>MIN_WORDS <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>MAX_WORDS <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>all_chunks <span class="op">=</span> []</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>all_token_lists <span class="op">=</span> []</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chunk_text(text, target_words<span class="op">=</span><span class="dv">120</span>):</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> nltk.sent_tokenize(text)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> []</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    current <span class="op">=</span> []</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    current_len <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sent <span class="kw">in</span> sentences:</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        words <span class="op">=</span> sent.split()</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> words:</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> current_len <span class="op">+</span> <span class="bu">len</span>(words) <span class="op">&gt;</span> target_words <span class="kw">and</span> current:</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            chunks.append(<span class="st">" "</span>.join(current))</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>            current <span class="op">=</span> []</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>            current_len <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        current.append(sent)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        current_len <span class="op">+=</span> <span class="bu">len</span>(words)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> current:</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        chunks.append(<span class="st">" "</span>.join(current))</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> chunks</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Found </span><span class="sc">{</span><span class="bu">len</span>(txt_paths)<span class="sc">}</span><span class="ss"> text files."</span>)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> path <span class="kw">in</span> txt_paths:</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(path, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>, errors<span class="op">=</span><span class="st">"ignore"</span>) <span class="im">as</span> f:</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> f.read()</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> chunk_text(text, TARGET_WORDS)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> chunks:</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> simple_preprocess(c, deacc<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>        n_tokens <span class="op">=</span> <span class="bu">len</span>(tokens)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Filter by length</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> MIN_WORDS <span class="op">&lt;=</span> n_tokens <span class="op">&lt;=</span> MAX_WORDS:</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>            all_chunks.append(c)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>            all_token_lists.append(tokens)</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total chunks kept (after filtering):"</span>, <span class="bu">len</span>(all_chunks))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>And run it. At this point, you can see:</p>
<ul>
<li><p>total number of .txt files processed: <code>516</code></p></li>
<li><p>total chunks kept (after filtering): <code>539057</code></p></li>
</ul>
</section>
<section id="step-5-train-word2-vec-on-our-corpus" class="level2">
<h2 class="anchored" data-anchor-id="step-5-train-word2-vec-on-our-corpus">Step 5: Train Word2 Vec on our Corpus</h2>
<p>In Week 07, we used Word2Vec as a way to represent meaning geometrically: words that occur in similar contexts end up close together in vector space.</p>
<p>This week, we will use Word2Vec for a very practical purpose: expand a seed term into a list of conceptually related terms (so we can build a weakly supervised training set).</p>
<p>Our seed term will be:</p>
<ul>
<li><code>merchant</code></li>
</ul>
<p>We will then have Word2Vec look for terms that are in the same (geometric) neighborhood as <u>merchant</u>.</p>
<p>OK, create a new file <code>step5_train_word2vec.py</code> and run it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.utils <span class="im">import</span> simple_preprocess</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"punkt"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"punkt_tab"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and preprocess all texts (same logic as Step 4)</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>TEXT_DIR <span class="op">=</span> Path(<span class="st">"texts"</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>txt_paths <span class="op">=</span> <span class="bu">sorted</span>(TEXT_DIR.glob(<span class="st">"*.txt"</span>))</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>TARGET_WORDS <span class="op">=</span> <span class="dv">120</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>MIN_WORDS <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>MAX_WORDS <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chunk_text(text, target_words<span class="op">=</span><span class="dv">120</span>):</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> nltk.sent_tokenize(text)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> []</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    current <span class="op">=</span> []</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    current_len <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sent <span class="kw">in</span> sentences:</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>        words <span class="op">=</span> sent.split()</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> words:</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> current_len <span class="op">+</span> <span class="bu">len</span>(words) <span class="op">&gt;</span> target_words <span class="kw">and</span> current:</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>            chunks.append(<span class="st">" "</span>.join(current))</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>            current <span class="op">=</span> []</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>            current_len <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>        current.append(sent)</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>        current_len <span class="op">+=</span> <span class="bu">len</span>(words)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> current:</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>        chunks.append(<span class="st">" "</span>.join(current))</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> chunks</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>token_lists <span class="op">=</span> []</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Found </span><span class="sc">{</span><span class="bu">len</span>(txt_paths)<span class="sc">}</span><span class="ss"> text files."</span>)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Building token lists..."</span>)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> path <span class="kw">in</span> txt_paths:</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(path, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>, errors<span class="op">=</span><span class="st">"ignore"</span>) <span class="im">as</span> f:</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> f.read()</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> chunk_text(text, TARGET_WORDS)</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> chunks:</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> simple_preprocess(c, deacc<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> MIN_WORDS <span class="op">&lt;=</span> <span class="bu">len</span>(tokens) <span class="op">&lt;=</span> MAX_WORDS:</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>            token_lists.append(tokens)</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Total tokenized chunks kept:"</span>, <span class="bu">len</span>(token_lists))</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a><span class="co">################</span></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Word2Vec (same logic as what we did in week 07)</span></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a><span class="co">################</span></span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Training Word2Vec..."</span>)</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>    sentences<span class="op">=</span>token_lists,</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>    vector_size<span class="op">=</span><span class="dv">200</span>,   <span class="co"># dimensionality of word vectors</span></span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>    window<span class="op">=</span><span class="dv">5</span>,          <span class="co"># context window size</span></span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>    min_count<span class="op">=</span><span class="dv">5</span>,       <span class="co"># ignore very rare words</span></span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>    workers<span class="op">=</span><span class="dv">4</span>,         <span class="co"># adjust depending on your machine (see Week 07)</span></span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>    sg<span class="op">=</span><span class="dv">1</span>               <span class="co"># 1 = skip-gram; 0 = CBOW</span></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a><span class="co"># Save model</span></span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>Path(<span class="st">"models"</span>).mkdir(exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> Path(<span class="st">"models"</span>) <span class="op">/</span> <span class="st">"w2v_full.bin"</span></span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>model.save(<span class="bu">str</span>(model_path))</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Model saved to:"</span>, model_path)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This will take a little while (or more than a little while, depending on your machine). Once it’s done, it will tell you: <code>Model saved to: models\w2v_full.bin</code>.</p>
<section id="checking-the-semantic-neighborhood" class="level3">
<h3 class="anchored" data-anchor-id="checking-the-semantic-neighborhood">Checking the semantic neighborhood</h3>
<p>Now, we are going to load the model and ask for neighbors of our seed concept (that is, what words are near “merchant” in the vector space?).</p>
<p>So, create another file, <code>step5b_query_word2vec.py</code>, and run it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> Path(<span class="st">"models"</span>) <span class="op">/</span> <span class="st">"w2v_full.bin"</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec.load(<span class="bu">str</span>(model_path))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="st">"merchant"</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> seed <span class="kw">not</span> <span class="kw">in</span> model.wv:</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>seed<span class="sc">}</span><span class="ss">' not found in the model vocabulary."</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"This usually means min_count is too high or the corpus is too small."</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Top 30 words similar to '</span><span class="sc">{</span>seed<span class="sc">}</span><span class="ss">':"</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word, score <span class="kw">in</span> model.wv.similar_by_word(seed, topn<span class="op">=</span><span class="dv">30</span>):</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:20s}</span><span class="ss"> </span><span class="sc">{</span>score<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>My output is:</p>
<p><code>Top 30 words similar to 'merchant':</code></p>
<p><code>marchant 0.736</code></p>
<p><code>factor 0.702</code></p>
<p><code>merchants 0.689</code></p>
<p><code>jeweller 0.667</code></p>
<p><code>customer 0.665</code></p>
<p><code>purser 0.648</code></p>
<p><code>tailor 0.639</code></p>
<p><code>wholesale 0.637</code></p>
<p><code>clothier 0.635</code></p>
<p><code>worshipful 0.632</code></p>
<p><code>vintner 0.629</code></p>
<p><code>clothyer 0.620</code></p>
<p><code>adventurer 0.620</code></p>
<p><code>trade 0.620</code></p>
<p><code>brewer 0.619</code></p>
<p><code>venturer 0.617</code></p>
<p><code>chapman 0.616</code></p>
<p><code>factory 0.610</code></p>
<p><code>seller 0.610</code></p>
<p><code>staple 0.606</code></p>
<p><code>tradesman 0.606</code></p>
<p><code>adventurers 0.606</code></p>
<p><code>sailor 0.604</code></p>
<p><code>horner 0.603</code></p>
<p><code>staplers 0.599</code></p>
<p><code>easterling 0.599</code></p>
<p><code>banker 0.598</code></p>
<p><code>marchants 0.595</code></p>
<p><code>goldsmith 0.594</code></p>
<p><code>apprentice 0.592</code></p>
<p>What did you get? Anything surprising or simply unclear? The list here makes a lot of sense to me. A historical note: “Easterling merchants” would have been merchants from the Baltic regions, so that works in this list. What we have created is a <u>corpus-specific semantic map</u> for the concept of “merchant.”</p>
<p>We will now use this list as a resource for weak supervision in the next steps in our classification task.</p>
</section>
</section>
<section id="step-6-build-a-tiered-keyword-list-abc" class="level2">
<h2 class="anchored" data-anchor-id="step-6-build-a-tiered-keyword-list-abc">Step 6: Build a tiered keyword list (A/B/C)</h2>
<p>Word2Vec gives us a list of words “near” our seed term (<code>merchant</code>). But we do not want to blindly treat Word2Vec neighbors as truth. Instead, we use them as a <em>suggestions</em> and then impose an interpretive structure.</p>
<p>A useful strategy is to split candidate terms into tiers:</p>
<ul>
<li>Tier A (high confidence): direct hits and spelling variants<br>
e.g., <code>merchant</code>, <code>marchant</code>, <code>merchants</code>, <code>marchants</code></li>
<li>Tier B (strongly related roles): terms that usually indicate commercial activity<br>
e.g., <code>factor</code>, <code>chapman</code>, <code>adventurer</code>, <code>venturer</code>, <code>staple</code></li>
<li>Tier C (maybe / adjacent): occupational neighborhood terms that might appear in commercial contexts but are often broader<br>
e.g., <code>tailor</code>, <code>clothier</code>, <code>haberdasher</code></li>
</ul>
<p>This tiering is a modeling decision:</p>
<ul>
<li><p>Tier A+B will become our <strong>CORE triggers</strong> (high precision).</p></li>
<li><p>Tier C will become a <strong>MAYBE set</strong> (to inspect separately).</p></li>
</ul>
<p>Create a new file step6_define_tiers.py and run it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tier A: direct spellings / variants of the seed concept</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>TIER_A <span class="op">=</span> {</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"merchant"</span>, <span class="st">"merchants"</span>,</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"marchant"</span>, <span class="st">"marchants"</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Tier B: closely related commercial roles / terms</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>TIER_B <span class="op">=</span> {</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"factor"</span>, <span class="st">"chapman"</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"adventurer"</span>, <span class="st">"adventurers"</span>,</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"venturer"</span>, <span class="st">"venturers"</span>,</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"staple"</span>, <span class="st">"staplers"</span>,</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"trade"</span>,</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"purser"</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Tier C: "maybe" occupational neighborhood (often adjacent, not always merchant-specific)</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>TIER_C <span class="op">=</span> {</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"clothier"</span>, <span class="st">"clothyer"</span>,</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"tailor"</span>, <span class="st">"tayler"</span>,</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"haberdasher"</span>,</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"goldsmith"</span>,</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">"vintner"</span>,</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">"brewer"</span>,</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">"banker"</span>,</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">"grazier"</span>,</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">"jeweller"</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tier A size:"</span>, <span class="bu">len</span>(TIER_A))</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tier B size:"</span>, <span class="bu">len</span>(TIER_B))</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tier C size:"</span>, <span class="bu">len</span>(TIER_C))</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Tier A:"</span>, <span class="bu">sorted</span>(TIER_A))</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Tier B:"</span>, <span class="bu">sorted</span>(TIER_B))</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Tier C:"</span>, <span class="bu">sorted</span>(TIER_C))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Let’s walk through what each tier represents and why I set them up this way:</p>
<ul>
<li><p>Tier A: terms are direct references to the concept we care about (spelling variants common in early modern texts);</p></li>
<li><p>Tier B: these terms describe occupations and roles that often involve trade; institutional or economic positions adjacent to merchants; vocabulary that appears frequently in commercial contexts. But they are not perfect synonyms for “merchant.”</p></li>
<li><p>Tier C: these terms are occupational nouns that are often related to production, craft, or commerce, and they frequently appear near merchant discourse. But they are much more ambiguous.</p></li>
</ul>
<p>This is where interpretive judgment enters the pipeline:</p>
<ul>
<li><p>We decide what our category means.</p></li>
<li><p>We decide which words count as strong evidence vs.&nbsp;weak evidence.</p></li>
<li><p>We acknowledge that “merchant discourse” is fuzzy at the edges.</p></li>
</ul>
</section>
<section id="step-7-label-chunks-using-tiered-keywords-weak-supervision" class="level2">
<h2 class="anchored" data-anchor-id="step-7-label-chunks-using-tiered-keywords-weak-supervision">Step 7: Label chunks using tiered keywords (weak supervision)</h2>
<p>We now have:</p>
<ul>
<li>a large set of sentence-based chunks,</li>
<li>tokenized into word lists,</li>
<li>and a tiered list of keywords (A / B / C).</li>
</ul>
<p>The next step is to assign labels to chunks. This is where we can translate our decision making about tiers into rule-based labeling of the chunks. This is called <strong>weak supervision</strong>.</p>
<section id="our-labeling-rules" class="level3">
<h3 class="anchored" data-anchor-id="our-labeling-rules">Our labeling rules</h3>
<p>For each chunk:</p>
<ul>
<li><p>CORE (label = 1) → contains <em>any</em> Tier A or Tier B word<br>
(high confidence merchant discourse)</p></li>
<li><p>MAYBE (label = 2) → contains Tier C words <em>only</em><br>
(ambiguous / adjacent cases)</p></li>
<li><p>NEG (label = 0) → contains none of the above</p></li>
</ul>
<p>These rules encode our interpretive decisions from Step 6. OK, now we can get started with the actual labelling.</p>
</section>
<section id="labeling-the-full-corpus" class="level3">
<h3 class="anchored" data-anchor-id="labeling-the-full-corpus">Labeling the full corpus</h3>
<p>Create a new file: <code>step7_label_chunks.py</code> and run it.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.utils <span class="im">import</span> simple_preprocess</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"punkt"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"punkt_tab"</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Tier definitions (from Step 6)</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>TIER_A <span class="op">=</span> {</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"merchant"</span>, <span class="st">"merchants"</span>,</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"marchant"</span>, <span class="st">"marchants"</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>TIER_B <span class="op">=</span> {</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"factor"</span>, <span class="st">"chapman"</span>,</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"adventurer"</span>, <span class="st">"adventurers"</span>,</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"venturer"</span>, <span class="st">"venturers"</span>,</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"staple"</span>, <span class="st">"staplers"</span>,</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">"trade"</span>, <span class="st">"purser"</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>TIER_C <span class="op">=</span> {</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">"clothier"</span>, <span class="st">"clothyer"</span>,</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">"tailor"</span>, <span class="st">"tayler"</span>,</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">"haberdasher"</span>,</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">"goldsmith"</span>,</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">"vintner"</span>,</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">"brewer"</span>,</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">"banker"</span>,</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">"grazier"</span>,</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">"jeweller"</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and process texts</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>TEXT_DIR <span class="op">=</span> Path(<span class="st">"texts"</span>)</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>txt_paths <span class="op">=</span> <span class="bu">sorted</span>(TEXT_DIR.glob(<span class="st">"*.txt"</span>))</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>TARGET_WORDS <span class="op">=</span> <span class="dv">120</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>MIN_WORDS <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>MAX_WORDS <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chunk_text(text, target_words<span class="op">=</span><span class="dv">120</span>):</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> nltk.sent_tokenize(text)</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> []</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>    current <span class="op">=</span> []</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>    current_len <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sent <span class="kw">in</span> sentences:</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>        words <span class="op">=</span> sent.split()</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> words:</span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> current_len <span class="op">+</span> <span class="bu">len</span>(words) <span class="op">&gt;</span> target_words <span class="kw">and</span> current:</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>            chunks.append(<span class="st">" "</span>.join(current))</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>            current <span class="op">=</span> []</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>            current_len <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>        current.append(sent)</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>        current_len <span class="op">+=</span> <span class="bu">len</span>(words)</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> current:</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>        chunks.append(<span class="st">" "</span>.join(current))</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> chunks</span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>labeled <span class="op">=</span> []</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Processing </span><span class="sc">{</span><span class="bu">len</span>(txt_paths)<span class="sc">}</span><span class="ss"> files..."</span>)</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> path <span class="kw">in</span> txt_paths:</span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(path, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>, errors<span class="op">=</span><span class="st">"ignore"</span>) <span class="im">as</span> f:</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> f.read()</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> chunk_text(text, TARGET_WORDS)</span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> chunks:</span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> simple_preprocess(c, deacc<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>        n_tokens <span class="op">=</span> <span class="bu">len</span>(tokens)</span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> (MIN_WORDS <span class="op">&lt;=</span> n_tokens <span class="op">&lt;=</span> MAX_WORDS):</span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a>        token_set <span class="op">=</span> <span class="bu">set</span>(tokens)</span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> token_set <span class="op">&amp;</span> (TIER_A <span class="op">|</span> TIER_B):</span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true" tabindex="-1"></a>            label <span class="op">=</span> <span class="dv">1</span>   <span class="co"># CORE</span></span>
<span id="cb9-95"><a href="#cb9-95" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> token_set <span class="op">&amp;</span> TIER_C:</span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true" tabindex="-1"></a>            label <span class="op">=</span> <span class="dv">2</span>   <span class="co"># MAYBE</span></span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a>            label <span class="op">=</span> <span class="dv">0</span>   <span class="co"># NEG</span></span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true" tabindex="-1"></a>        labeled.append((c, label))</span>
<span id="cb9-101"><a href="#cb9-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-102"><a href="#cb9-102" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total chunks labeled:"</span>, <span class="bu">len</span>(labeled))</span>
<span id="cb9-103"><a href="#cb9-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-104"><a href="#cb9-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-105"><a href="#cb9-105" aria-hidden="true" tabindex="-1"></a><span class="co"># Save labeled data</span></span>
<span id="cb9-106"><a href="#cb9-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-107"><a href="#cb9-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-108"><a href="#cb9-108" aria-hidden="true" tabindex="-1"></a>Path(<span class="st">"data"</span>).mkdir(exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-109"><a href="#cb9-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-110"><a href="#cb9-110" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(Path(<span class="st">"data"</span>) <span class="op">/</span> <span class="st">"merchant_labeled_chunks.json"</span>, <span class="st">"w"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb9-111"><a href="#cb9-111" aria-hidden="true" tabindex="-1"></a>    json.dump(labeled, f, ensure_ascii<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-112"><a href="#cb9-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-113"><a href="#cb9-113" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Saved labeled chunks to data/merchant_labeled_chunks.json"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What we did so far: we applied rules we designed, using the lexical triggers we chose, to create a labeled dataset. Obviously, these labels are imperfect and corpus-specific, but they are good enough to train a first classifier.</p>
<p>At this point, we have turned unstructured text into a supervised learning dataset. All the steps that we are doing next (train/test splits, classifiers, confusion matrices) depend on the choices made up to here. We can’t quite train a classifier, yet. First, we need to separate CORE, MAYBE, and NEG examples, inspect them, and think carefully about evaluation.</p>
<p>This is where questions of <a href="https://en.wikipedia.org/wiki/Precision_and_recall"><strong>precision</strong> and <strong>recall</strong></a> enter the picture.</p>
</section>
</section>
<section id="step-9-decide-what-to-train-on" class="level2">
<h2 class="anchored" data-anchor-id="step-9-decide-what-to-train-on">Step 9: Decide what to train on</h2>
<p>Now we have three kinds of labeled chunks of text:</p>
<ul>
<li>CORE (label = 1): High-confidence merchant discourse<br>
</li>
<li>MAYBE (label = 2): Ambiguous, adjacent, or borderline cases<br>
</li>
<li>NEG (label = 0): Clearly not merchant discourse</li>
</ul>
<p>Before we actually train a classifier, we must decide which of these labels should the model actually learn from. For this week, we will:</p>
<ul>
<li>Train the classifier only on CORE vs.&nbsp;NEG</li>
<li>Exclude MAYBE from training</li>
<li>Save MAYBE for later inspection and interpretation</li>
</ul>
<p>Why? The honest answer is: I tried a couple of options and compared with my own readings of the texts, this gave the best results at this stage. But I also want to note the MAYBE category contains texts that are close to commerce but not clearly merchant discourse and so require a lot of contextual interpretation. They are also texts where other scholars may disagree with my reading. This means that including them for this training, would reduce interpretability and make evaluation harder to explain.</p>
<p>Instead, we treat the MAYBE texts as a set of texts to which we apply the trained classifier later. This mirrors how supervised models are often used in practice: - trained on clear cases, - applied to ambiguous ones.</p>
<section id="separate-datasets" class="level3">
<h3 class="anchored" data-anchor-id="separate-datasets">Separate datasets</h3>
<p>Create a new file <code>step9_prepare_datasets.py</code> . <strong>Before</strong> you run it, read the explanation about the NEG step after the code sample. <em>Then</em> run it.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>DATA_PATH <span class="op">=</span> Path(<span class="st">"data"</span>) <span class="op">/</span> <span class="st">"merchant_labeled_chunks.json"</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(DATA_PATH, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    labeled <span class="op">=</span> json.load(f)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>core <span class="op">=</span> [(t, <span class="dv">1</span>) <span class="cf">for</span> (t, y) <span class="kw">in</span> labeled <span class="cf">if</span> y <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>neg  <span class="op">=</span> [(t, <span class="dv">0</span>) <span class="cf">for</span> (t, y) <span class="kw">in</span> labeled <span class="cf">if</span> y <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>maybe <span class="op">=</span> [t <span class="cf">for</span> (t, y) <span class="kw">in</span> labeled <span class="cf">if</span> y <span class="op">==</span> <span class="dv">2</span>]</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loaded:"</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  CORE:"</span>, <span class="bu">len</span>(core))</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  NEG :"</span>, <span class="bu">len</span>(neg))</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  MAYBE:"</span>, <span class="bu">len</span>(maybe))</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co">###### =&gt; NEG step [see explanation below]</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>neg_sample <span class="op">=</span> random.sample(neg, <span class="bu">len</span>(core))</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>training_data <span class="op">=</span> core <span class="op">+</span> neg_sample</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>random.shuffle(training_data)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training set size (CORE + NEG):"</span>, <span class="bu">len</span>(training_data))</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="co">### Split the data into train and test sets:</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>split <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(training_data))</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> training_data[:split]</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>test_data  <span class="op">=</span> training_data[split:]</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Train size:"</span>, <span class="bu">len</span>(train_data))</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test size :"</span>, <span class="bu">len</span>(test_data))</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MAYBE size:"</span>, <span class="bu">len</span>(maybe))</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the datasets:</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>Path(<span class="st">"data"</span>).mkdir(exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(Path(<span class="st">"data"</span>) <span class="op">/</span> <span class="st">"train_core_vs_neg.json"</span>, <span class="st">"w"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>    json.dump(train_data, f, ensure_ascii<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(Path(<span class="st">"data"</span>) <span class="op">/</span> <span class="st">"test_core_vs_neg.json"</span>, <span class="st">"w"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>    json.dump(test_data, f, ensure_ascii<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(Path(<span class="st">"data"</span>) <span class="op">/</span> <span class="st">"maybe_texts.json"</span>, <span class="st">"w"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>    json.dump(maybe, f, ensure_ascii<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Saved training, test, and MAYBE datasets."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>With this step in the code we are <u>downsampling</u> NEG to match the size of the CORE set. Why? [Think for a second about what most of the texts might look like…] Well, in most corpora, NEG examples vastly outnumber CORE examples: <em>most</em> of the text is <em>not</em> likely to be talking about the concept/topic you are targeting. Now, the texts that we have for class <em>are</em> a little skewed towards “merchant” and, as you will see after you run the code, we would still have an overwhelming number of NEG chunks, but I want you to learn best practices <em>in general</em>!</p>
<p>Training directly on all NEG data can cause the model to:</p>
<ul>
<li><p>always predict “not merchant”</p></li>
<li><p>achieve high accuracy but low usefulness</p></li>
</ul>
<p>OK, this is what I got back from this step:</p>
<p><code>Loaded:</code></p>
<p><code>CORE: 11373</code></p>
<p><code>NEG : 527174</code> [<strong>!!!</strong>]</p>
<p><code>MAYBE: 510</code></p>
<p><code>Training set size (CORE + NEG): 22746</code> [<strong>Important:</strong> can you see where this is coming from based on the above explanation? And, can you see why we needed to downsample?]</p>
<p><code>Train size: 18196</code></p>
<p><code>Test size : 4550</code></p>
<p><code>MAYBE size: 510</code></p>
</section>
</section>
<section id="week-9-part-iithe-classification-task" class="level2">
<h2 class="anchored" data-anchor-id="week-9-part-iithe-classification-task">Week 9 (Part II–The Classification Task)</h2>
<p>Now we are ready to create an actual classifier. So far, our training data looks like this:</p>
<ul>
<li><p>a chunk of text (a string),</p></li>
<li><p>paired with a label (CORE = 1 or NEG = 0).</p></li>
</ul>
<p>We can read these strings directly (and have done so to check things along the way), but obviously computers cannot. A classifier cannot operate on words, sentences, or meanings. It can only operate on numbers. So before we train a model, we must transform text into a numerical representation as we have been doing in previous weeks. In particular, we will revisit our old friend <strong>TF-IDF</strong> (remember our work in <a href="https://astrid-giugni.github.io/IDS_570_TAD/week-04-representation.html">week 4</a>).</p>
<p>Our next steps:</p>
<ol type="1">
<li><p>Convert text chunks into TF-IDF vectors. This helps emphasize <em>distinctive</em> vocabulary rather than common function words.</p></li>
<li><p>Train a simple binary classifier</p></li>
<li><p>Evaluate its performance on held-out data</p></li>
<li><p>Apply it to the MAYBE set (review above if needed)</p></li>
<li><p>Evaluate on what the model learned and what it did not, and see if it is useful for historical interpretation.</p></li>
</ol>
<p>As with week 8, we are going to build everything step by step. Create a new file, <code>step10_tfidf_representation.py</code>. Don’t run it yet, I will give explanations below and I want you to <strong>read</strong> and understand them before running the code.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>DATA_DIR <span class="op">=</span> Path(<span class="st">"data"</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">#We now load the datasets we prepared at the end of last week (step 9)</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(DATA_DIR <span class="op">/</span> <span class="st">"train_core_vs_neg.json"</span>, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    train_data <span class="op">=</span> json.load(f)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(DATA_DIR <span class="op">/</span> <span class="st">"test_core_vs_neg.json"</span>, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    test_data <span class="op">=</span> json.load(f)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate texts and labels</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>X_train_texts <span class="op">=</span> [t <span class="cf">for</span> (t, y) <span class="kw">in</span> train_data]</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> [y <span class="cf">for</span> (t, y) <span class="kw">in</span> train_data]</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>X_test_texts <span class="op">=</span> [t <span class="cf">for</span> (t, y) <span class="kw">in</span> test_data]</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> [y <span class="cf">for</span> (t, y) <span class="kw">in</span> test_data]</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Train size:"</span>, <span class="bu">len</span>(X_train_texts))</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test size :"</span>, <span class="bu">len</span>(X_test_texts))</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="co">##### ==&gt; See explanation [A] below</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer(</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    lowercase<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    min_df<span class="op">=</span><span class="dv">5</span>,        <span class="co"># ignore very rare words</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    max_df<span class="op">=</span><span class="fl">0.9</span>       <span class="co"># ignore extremely common words; Explanation [B]</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> vectorizer.fit_transform(X_train_texts)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> vectorizer.transform(X_test_texts)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TF-IDF matrix shapes:"</span>)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  Train:"</span>, X_train.shape)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  Test :"</span>, X_test.shape)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a><span class="co">#####  ==&gt; See explanation [C] below</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><p>Explanation [A]: after the lines above, we still have raw text, so we have to define a TF-IDF vectorizer.</p></li>
<li><p>Explanation [B]: we set the parameters as <code>min_df=5</code> so that we remove words that appear <u>in fewer than</u> 5 documents; and <code>max_df=0.9</code> so that we remove words that appear in more than 90% of the documents.</p></li>
<li><p>Explanation [C]: we fit the vectorizer on the training data <strong>only</strong>, then apply it to the test data. Each text chunk is now a numeric vector.</p></li>
</ul>
<p>We have:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Object</th>
<th>What it contains</th>
<th>What it’s used for</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>X_train</code></td>
<td>TF-IDF vectors</td>
<td>Model learns patterns</td>
</tr>
<tr class="even">
<td><code>y_train</code></td>
<td>Labels</td>
<td>Supervision signal</td>
</tr>
<tr class="odd">
<td><code>X_test</code></td>
<td>TF-IDF vectors</td>
<td>Model makes predictions</td>
</tr>
<tr class="even">
<td><code>y_test</code></td>
<td>Labels</td>
<td>We check how good predictions are</td>
</tr>
</tbody>
</table>
<p>I am going to belabor this more so that it’s fully clear, you can think of the X, Y part of this as:</p>
<p>X = the input to the model → the text, represented numerically (TF-IDF vectors)</p>
<p>y = the output we want the model to predict → the labels (CORE = 1, NEG = 0)</p>
<p>So:</p>
<p>X_train = text chunks (as numbers)</p>
<p>y_train = labels for those chunks</p>
<p>Where <strong>train</strong> = data the model is allowed to learn from; <strong>test</strong> = data the model has <em>never seen</em> before.</p>
<p><u>When you run step 10</u>, your output will look like:</p>
<p><code>Train size: 36874</code></p>
<p><code>Test size : 9218</code></p>
<p><code>TF-IDF matrix shapes:</code></p>
<p><code>Train: (36874, 27806)</code></p>
<p><code>Test : (9218, 27806)</code></p>
<p>This is reassuring as it is what we want: we have the 80/20 split we created in step 9: 36,874 chunks were used to train the model; 9,218 chunks were held out for evaluation. So far so good.</p>
<p>In addition, the TF-IDF matrix has the shape (number of documents) by (number of features), that is (36874, 27806) translates to a matrix that has:</p>
<ul>
<li><p>36874 rows, where each row is for a training chunk. Each row is a TF-IDF vector representing a chunk.</p></li>
<li><p>27,806 columns, where each column is for a word feature. Each column corresponds to a specific word in the vocabulary. Yes, we have 27,806 features! That’s not a problem: remember, TF-IDF matrices are sparse, linear models do well with this kind of setting (see the info below about the choice of logistic regression), and this is normal for text data.</p></li>
</ul>
<section id="looking-ahead-to-the-next-step-confusion-matrix" class="level4">
<h4 class="anchored" data-anchor-id="looking-ahead-to-the-next-step-confusion-matrix">Looking ahead to the next step (confusion matrix):</h4>
<p>Later, when we compute a confusion matrix:</p>
<ul>
<li><p>predictions come from <code>X_test</code></p></li>
<li><p>“true labels” come from <code>y_test</code></p></li>
</ul>
<p>The confusion matrix will help us understand how often the model’s predictions on unseen data match the labels produced by our weak supervision rules. I will remind you below, but you want to start keeping in mind that the confusion matrix does <em>not</em> measure truth, it just measures agreement with a labeling scheme.</p>
</section>
<section id="train-a-classifier" class="level3">
<h3 class="anchored" data-anchor-id="train-a-classifier">Train a classifier:</h3>
<p>Now we can <em>finally</em> train a classifier using a <strong>logistic regression</strong> model. We discussed how a logistics regression works in class (or will do so in Week 9, if you are reading ahead). If you want an <strong>advanced</strong> version of this, see <a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf">chapter 4</a> in Jurafsky and Martin.</p>
<ul>
<li>Note: we are <strong>not</strong> going to go into the details covered in section 4.4 forward. For the simple reason that it would require more mathematical background than I assume for the class to discuss the loss function and gradients. (We will touch on the materials from section 4.9 on how to build a confusion matrix.)</li>
</ul>
<p>There are many classifiers we could use. We choose logistic regression for three main reasons:</p>
<ul>
<li>It is simple and well understood. It works well with a binary task such as ours: CORE (1) vs NEG (0). It models:</li>
</ul>
<p>Logistic regression is literally designed to model:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode latex code-with-copy"><code class="sourceCode latex"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a> P(y=1∣x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>So it gives you: a probability of CORE for each chunk (via <code>predict_proba</code> ) and a clean decision rule: “predict CORE if probability ≥ 0.5”.</p>
<ul>
<li><p>It works very well with TF-IDF features. TF-IDF produces very high-dimensional vectors (tens of thousands to 100k+ features) which are mostly zeros (sparse). Linear models such as logistic regression work well for these situations (they are fast to train and are stable).</p></li>
<li><p><strong>Note:</strong> the default threshold in scikit-learn is 0.5. As we saw in class, logistic regression models the log-odds of class membership and a probability of 0.5 corresponds to equal odds. This is reasonable when classes are roughly balanced in the training setup (recall that we decided to downsample the negatives above). We <em>could</em> change the threshold if we had good reasons to do so. I will bring this point up again at the end.</p></li>
</ul>
<p>There <em>are</em> other options (linear support vector machine or naive Bayes), but we choose logistic regression because it is a strong, interpretable baseline for binary text classification with TF-IDF features. TF-IDF creates high-dimensional sparse vectors, and linear models are known to perform very well in this setting. Logistic regression also outputs probabilities, which lets us interpret model confidence and apply the model to ambiguous cases (our MAYBE set).</p>
<p>Create a new file, <code>step11_train_classifier.py</code>. We will build the script in chunks with explanations at each step and you will run it once it’s all put together.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> (</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    confusion_matrix,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    classification_report,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    roc_auc_score</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Next, we load the data and use the TF-IDF representations we created in <u>step 10</u> above.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>DATA_DIR <span class="op">=</span> Path(<span class="st">"data"</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(DATA_DIR <span class="op">/</span> <span class="st">"train_core_vs_neg.json"</span>, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    train_data <span class="op">=</span> json.load(f)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(DATA_DIR <span class="op">/</span> <span class="st">"test_core_vs_neg.json"</span>, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    test_data <span class="op">=</span> json.load(f)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>X_train_texts <span class="op">=</span> [t <span class="cf">for</span> (t, y) <span class="kw">in</span> train_data]</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> [y <span class="cf">for</span> (t, y) <span class="kw">in</span> train_data]</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>X_test_texts <span class="op">=</span> [t <span class="cf">for</span> (t, y) <span class="kw">in</span> test_data]</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> [y <span class="cf">for</span> (t, y) <span class="kw">in</span> test_data]</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer(</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    lowercase<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    min_df<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    max_df<span class="op">=</span><span class="fl">0.9</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> vectorizer.fit_transform(X_train_texts)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> vectorizer.transform(X_test_texts)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Now we actually <u>train</u> the classifier (yes, I know, finally!). What will happen in this section of the script: the model examines the TF-IDF features and it learns the weights that separate CORE from NEG. We then make predictions on the test set.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LogisticRegression(</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    max_iter<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=</span><span class="dv">1</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">#test set predictions</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.predict(X_test)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>y_prob <span class="op">=</span> clf.predict_proba(X_test)[:, <span class="dv">1</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>At this point we evaluate our classifier with a confusion matrix that compares:</p>
<ul>
<li><p>true labels (<code>y_test</code>)</p></li>
<li><p>model predictions (<code>y_pred</code>)</p></li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion matrix:"</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cm)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>I will go over what this does in more detail below (step 12). But the next step is to include two more evaluation components. <u>First</u>, a classification report that summarizes how reliable positive predictions are (= <strong>precision</strong>); how many positives the model found (= <strong>recall</strong>); and the balance of precision and recall (<strong>F1-score</strong>). <u>Second</u>, <a href="https://library.virginia.edu/data/articles/roc-curves-and-auc-for-models-used-for-binary-classification"><strong>ROC AUC</strong></a>, which measures how well the model separates CORE from NEG across all thresholds.</p>
<p>Add the next bit of code to your script and then run everything in <code>step11_train_classifier.py</code>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification report:"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">###ROC AUC</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>auc <span class="op">=</span> roc_auc_score(y_test, y_prob)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ROC AUC:"</span>, <span class="bu">round</span>(auc, <span class="dv">3</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="evaluating-the-classifier-step-12" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-the-classifier-step-12">Evaluating the classifier (step 12):</h3>
<p>Now that we have trained the classifier, computed the confusion matrix and ROC AUC, we can evaluate how closely do the model’s predictions agree with the labels in our test set, given a particular decision rule.</p>
<p>Let’s start with the confusion matrix. When you ran step 11, you should have gotten something like this:</p>
<p>[[6943 27]</p>
<p>[ 367 1881]]</p>
<p>What does this mean? I am going to present it as a table to make its entries legible:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>Predicted NEG (0)</th>
<th>Predicted CORE (1)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual NEG (0)</strong></td>
<td>6943</td>
<td>27</td>
</tr>
<tr class="even">
<td><strong>Actual CORE (1)</strong></td>
<td>367</td>
<td>1881</td>
</tr>
</tbody>
</table>
<p>Each cell tells us something different about model behavior. By convention in scikit-learn:</p>
<ul>
<li><p>Rows = actual (true) labels</p></li>
<li><p>Columns = predicted labels</p></li>
</ul>
<p>So we can rewrite it structurally as:</p>
<span class="math display">\[\begin{bmatrix} \text{TN} &amp; \text{FP} \\ \text{FN} &amp; \text{TP} \end{bmatrix}\]</span>
<p>To make this clear:</p>
<ul>
<li><p>True negatives (TN) = 6943 → 6943 texts were labeled NEG in the test set, and the model correctly predicted NEG for them.</p></li>
<li><p>False Positive (FP) = 27 → 27 texts were labeled NEG in the test set, and the model incorrectly predicted CORE for them.</p></li>
<li><p>False Negatives (FN) = 367 → 367 texts were labeled CORE in the test set, and the model incorrectly predicted NEG for them.</p></li>
<li><p>True Positives (TP) = 1881 → 1881 texts were labeled CORE in the test set, and the model correctly predicted CORE for them.</p></li>
</ul>
<p><strong>Interpretation:</strong> our decision threshold in the logistic regression was 0.5. That is, if the predicted probability ≥ 0.5, label the text CORE; if the predicted probability &lt; 0.5, label the text NEG. The model is very good at recognizing texts that do <em>not</em> match our merchant criteria. It’s fairly good at avoiding predicting as positive texts that are labelled negative (27 false positives), but we do lose a fair amount of CORE labeled texts (367).</p>
<p>The confusion matrix is not the only validation tool we have. Let’s look at the classification report. You should have gotten something like this:</p>
</section>
<section id="classification-report" class="level3">
<h3 class="anchored" data-anchor-id="classification-report">Classification Report</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Class</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-Score</th>
<th>Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.95</td>
<td>1.00</td>
<td>0.97</td>
<td>6,970</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.99</td>
<td>0.84</td>
<td>0.91</td>
<td>2,248</td>
</tr>
</tbody>
</table>
<p>What do these numbers mean?</p>
<p><u><strong>For CORE (class 1)</strong></u><strong>:</strong></p>
<p><u>Precision</u> (1) = 0.99</p>
<p>Formula: <span class="math inline">\(\text{Precision} = \frac{TP}{TP+FP}\)</span></p>
<p>In our case, this tells us that when the model predicts CORE, it is correct 99% of the time. This matches your tiny number of false positives (27).</p>
<p><u>Recall</u> (1) = 084</p>
<p>The formula: <span class="math inline">\(\text{Recall} = \frac{TP}{TP + FN}\)</span></p>
<p>The model finds 84% of the CORE texts and it misses 16% (those 367 false negatives).</p>
<p><u>F1-score</u> (1) = 0.91</p>
<p>F1 balances precision and recall.</p>
<p>The formula: <span class="math inline">\(F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}\)</span></p>
<p>0.91 is a strong F1 score, but lower than precision because recall is lower.</p>
<p>I will leave it up to you to think through the values for the NEG class.</p>
<p>As part of our report, we also had the following values:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-Score</th>
<th>Support (total)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td></td>
<td></td>
<td>0.96*</td>
<td>9,218</td>
</tr>
<tr class="even">
<td>Macro avg</td>
<td>0.97</td>
<td>0.92</td>
<td>0.94</td>
<td>9,218</td>
</tr>
<tr class="odd">
<td>Weighted avg</td>
<td>0.96</td>
<td>0.96</td>
<td>0.96</td>
<td>9,218</td>
</tr>
</tbody>
</table>
<p>(*) I know, I know, this is weird: the placement of <strong>accuracy</strong> in the F1-score column is just a formatting convention from sklearn’s classification report. The 0.96 value had to go somewhere, and they chose that column.</p>
<p>So, what is actually going on besides formatting weirdness:</p>
<p><u>Accuracy</u> = 0.96</p>
<p>Formula: <span class="math inline">\(\frac{TP + TN}{\text{Total}}\)</span></p>
<p>Interpretation: the model agrees with the test labels 96% of the time. In general accuracy alone is not very informative in imbalanced tasks, which is why we look at precision and recall. But recall that we downsampled NEG to match CORE, so we had a balanced training set. Nonetheless, precision and recall are still very useful in understanding the <em>kinds</em>&nbsp;of errors the model is making.</p>
</section>
<section id="macro-average" class="level3">
<h3 class="anchored" data-anchor-id="macro-average">Macro average</h3>
<p>Here we take a simple average of class metrics to treat both classes equally:</p>
<p><span class="math display">\[
\frac{Metric_0 + Metric_1}{2}
\]</span></p>
</section>
<section id="weighted-average" class="level3">
<h3 class="anchored" data-anchor-id="weighted-average">Weighted average</h3>
<p>Weights each class by its support (number of examples). Since NEG is larger (6970 vs 2248), the weights workout to:</p>
<p>weight_neg = <span class="math inline">\(\frac{6970}{9218}\)</span> or about 0.756</p>
<p>weight_core = <span class="math inline">\(\frac{2248}{9218}\)</span> or about 0.244</p>
<p>The weighted average multiplies each class’s score by the proportion of examples in that class. Since NEG makes up about 76% of the test set and performs very well, the weighted average is pulled toward the NEG score and ends up close to overall accuracy.</p>
<p><u>For example, the weighted F1</u>:</p>
<p>Weighted F1 is:</p>
<p><span class="math inline">\((w_0 \times F1_0) + (w_1 \times F1_1)\)</span></p>
<p>Substitute values:</p>
<p><span class="math inline">\((0.756 \times 0.97) + (0.244 \times 0.91)\)</span></p>
<p>Compute each term:</p>
<p><span class="math inline">\(0.756 \times 0.97 ≈ 0.733\)</span></p>
<p><span class="math inline">\(0.244 \times 0.91 ≈ 0.222\)</span></p>
<p>Add them and round <span class="math inline">\(≈0.96\)</span></p>
<p>The weighted F1-score is close to overall accuracy because the model performs strongly on both classes and the larger class (NEG) dominates the weighting.</p>
</section>
<section id="roc-auc-0.996" class="level3">
<h3 class="anchored" data-anchor-id="roc-auc-0.996">ROC AUC = 0.996</h3>
<p>Now the most important conceptual point in evaluating our model: ROC AUC does not use the 0.5 threshold. It evaluates how well the model separates the two classes across all possible thresholds.</p>
<p>How does ROC AUC work? Instead of using one specific cutoff, it changes the threshold from 0 to 1 and tests how well the model separates CORE from NEG <em>at each threshold.</em> Then it evaluates performance across all those possibilities.</p>
<p>Splitting this up into its components:</p>
<p><strong>ROC</strong> stands for Receiver Operating Characteristic.</p>
<p>It plots:</p>
<ul>
<li><p>TPR: True Positive Rate (same as Recall) on the y-axis</p></li>
<li><p>FPR: False Positive Rate on the x-axis</p></li>
</ul>
<p>As the threshold moves from 0 to 1, the curve traces out the model’s performance. If the model:</p>
<ul>
<li><p>perfectly separates classes → curve hugs the top-left corner (high TPR, low FPR at all thresholds)</p></li>
<li><p>guesses randomly → curve follows a diagonal line (50/50 at every threshold)</p></li>
</ul>
<p><strong>AUC</strong> stands for Area Under the Curve. It compresses all this information into a single number (for us, 0.996). AUC is literally the area under the ROC curve. It’s the integral of the True Positive Rate with respect to the False Positive Rate as we vary the threshold. This is why AUC ranges from 0.5 (diagonal line, area of triangle = 0.5) to 1.0 (perfect corner, full square = 1.0).</p>
<p><strong>Math aside:</strong> you don’t need to worry about computing this ourself, but here’s the integral:</p>
<p><span class="math display">\[\text{AUC} = \int_0^1 \text{TPR}(t) \, d(\text{FPR}(t))\]</span></p>
<p>You can think of it this way: if I randomly choose one CORE chunk and one NEG chunk, the AUC tells you the probability that the model assigns a higher probability to the CORE chunk.</p>
<p>Just a reminder: you can see a full <a href="https://library.virginia.edu/data/articles/roc-curves-and-auc-for-models-used-for-binary-classification"><strong>explanation here</strong></a>.</p>
<p>We have a value of 0.996, which means that the model almost perfectly ranks CORE texts above NEG texts. This is good! Unlike accuracy or F1-score which depend on choosing a threshold, ROC AUC tells us whether our weak supervision approach is fundamentally learning the right pattern. A high AUC (like 0.996) means our lexicon-based labels are capturing something real about “merchant” discourse.</p>
</section>
<section id="coming-back-to-threshold-choice" class="level3">
<h3 class="anchored" data-anchor-id="coming-back-to-threshold-choice">Coming back to threshold choice:</h3>
<p>Remember that I mentioned that we could change the threshold for the classification task if we had a good reason to do so. We now want to go back and think through this issue.</p>
<p>First of all, you might want to ask: Does high ROC-AUC mean that our 0.5 threshold performed well? We want to be careful with this question:</p>
<p>Our high ROC-AUC (0.996) tells us the model is excellent at ranking texts. It consistently assigns higher probabilities to CORE than NEG. This means that if you sorted all chunks by their probability scores, true CORE chunks would appear near the top and true NEG chunks near the bottom 99.6% of the time, regardless of where you draw the threshold line.</p>
<ul>
<li>True CORE chunks get higher probability scores than true NEG chunks (ranking). Whether they cross the 0.5 threshold is a separate question (classification)</li>
</ul>
<p>As an example, you could have:</p>
<p>CORE chunk with prob = 0.48 (ranked above NEG chunks, but classified as NEG at 0.5) NEG chunk with prob = 0.02 (ranked below CORE chunks, classified correctly as NEG)</p>
<p>The model correctly ranked them (CORE &gt; NEG), which gives high AUC, even though the CORE chunk was misclassified at the 0.5 threshold.</p>
<p>This is important because we do <strong>not</strong> want to misread a high AUC as indicating that we picked (or were given by default) the optimal threshold for our specific research goals. As I mentioned earlier, the 0.5 threshold is a reasonable default when:</p>
<ul>
<li><p>Classes are balanced (we downsampled)</p></li>
<li><p>False positives and false negatives are equally costly (In general, this depends on your research question)</p></li>
</ul>
<p>Now, we <em>might</em> want to explore other thresholds. Then we <em>could</em> pick one of the following approaches:</p>
<ul>
<li><p>Plot precision and recall at different thresholds</p></li>
<li><p>Optimize for F1-score</p></li>
<li><p>Make a domain-driven decision based on your research needs (if you ask me, this is always a question you should have in mind, but, at the end of the day, I am a humanist..)</p></li>
</ul>
<p>I am going to give you a couple of scenarios where this might be reasonable to do:</p>
<ul>
<li><p>I sometimes need a discovery tool to find texts (that talk about “merchant” concepts) that I might have missed. I would then lower the threshold (0.3-0.4) for high recall</p></li>
<li><p>On the other hand, if I am selecting passages for close reading and want only clear examples of the discourse, then I would pick a higher threshold (0.6-0.7) for high precision</p></li>
<li><p>If you want a balanced approach for further analysis (our case) → 0.5 is reasonable</p></li>
</ul>
<p>For our purposes, we’ll stick with 0.5 since our evaluation metrics (precision, recall, F1) all look strong at this threshold, and it aligns with our balanced training set.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/Astrid-Giugni\.github\.io\/IDS_570_TAD\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>