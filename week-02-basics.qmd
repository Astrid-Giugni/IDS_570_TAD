---
title: "Week 02: Basics"
format: html
execute:
  echo: true
  warning: false
  message: false
---

## 

## Week's Goals:

This week, we compare two early modern economic texts by examining word frequencies and bigrams. The goal for you to become familiar with how to start implementing NLP workflows in RStudio. The two texts that we are going to be using are available on Canvas, under **files** in the folder named **Week 2**. Note: this will be the standard set up going forward. Sample code will be here and files will be on Canvas, unless otherwise specified.

## What you need in your project folder

-   Two plain-text files
-   The tidyverse and tidytext packages
-   A consistent folder structure

## Step-by-step: from raw texts to comparison plots

## Setup

This lesson assumes you are working inside an RStudio Project (recommended).\
We will read two plain-text files, tidy the tokens/bigrams, and compare frequency patterns.

```{r}
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tibble)
library(scales)
```

## What you need in your project folder

Create a folder named `texts/` inside your project. Put these files inside it:

-   `texts/Misselden_Free_Trade.txt`
-   `texts/Other_Text.txt`

(We will refer to them by these exact filenames in the code below.)

```{r}
# Define file paths (relative to the Quarto project root)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"

# Read the raw text files into R
text_a <- read_file(file_a)
text_b <- read_file(file_b)

# Combine into a tibble for tidytext workflows
texts <- tibble(
  doc_title = c("Text A", "Text B"),
  text = c(text_a, text_b)
)

texts
```

## Cleaning stage: stopwords and tokenization

We will turn each text into a table of one word per row (tokenization).\
Then we remove stopwords (common function words) so that the remaining words are more meaningful for comparison.

```{r}
# Start with tidytext's built-in stopword list
data("stop_words")

# Add early modern and project-specific stopwords (you can expand this list)
custom_stopwords <- tibble(
  word = c(
    "vnto", "haue", "doo", "hath", "bee", "ye", "thee"
  )
)

all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
  distinct(word)

all_stopwords %>% slice(1:10)
```

```{r}
word_counts <- texts %>%
  unnest_tokens(word, text) %>%
  mutate(word = str_to_lower(word)) %>%
  anti_join(all_stopwords, by = "word") %>%
  count(doc_title, word, sort = TRUE)

word_counts
```

```{r}
top_words <- word_counts %>%
  group_by(doc_title) %>%
  slice_max(n, n = 12) %>%
  ungroup() %>%
  mutate(word = fct_reorder(word, n))

ggplot(top_words, aes(x = n, y = word)) +
  geom_col() +
  facet_wrap(~ doc_title, scales = "free_y") +
  labs(x = "Count", y = NULL, title = "Top words after stopword removal")
```

## Bigrams: moving beyond single words

Single-word frequencies tell us what terms are common.\
Bigrams allow us to see which words *appear together*, capturing short phrases and recurring ideas.

A **bigram** is simply a sequence of two adjacent words.

```{r}
bigrams <- texts %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigrams
```

```{r}
bigrams_separated <- bigrams %>%
  separate(bigram, into = c("word1", "word2"), sep = " ")

bigrams_separated
```

```{r}
bigrams_filtered <- bigrams_separated %>%
  filter(
    !word1 %in% all_stopwords$word,
    !word2 %in% all_stopwords$word
  )

bigrams_filtered
```

We remove bigrams where *either* word is a stopword, since phrases like “of the” or “and the” are rarely meaningful analytically.

```{r}
bigram_counts <- bigrams_filtered %>%
  count(doc_title, word1, word2, sort = TRUE)

bigram_counts
```

```{r}
bigram_counts <- bigram_counts %>%
  unite(bigram, word1, word2, sep = " ")

bigram_counts
```
