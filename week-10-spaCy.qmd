---
title: "Week 10: Classifying New Text & spaCy"
editor: visual
format: html
execute:
  echo: true
  warning: false
  message: false
---

```{r}
#| include: false

library(reticulate)

use_python("C:/Users/astri/miniconda3/envs/tad/python.exe", required = TRUE)
py_config()
```

This week's plan: our goal is to use our classifier from Week 9 to learn how to do Named Entity Recognition (NER) using spaCy.

**Named Entity Recognition:** I mentioned NER earlier in the semester and it is a core task in NLP. It's goal is to identify and classify spans of text into predefined semantic categories: people, organizations, locations, dates, and quantities (and you train it to add your own categories to fit your own questions). Formally, it can be framed as a sequence labeling problem, where each token is assigned a tag indicating whether it begins, continues, or falls outside a named entity. Modern NER systems typically rely on contextual word embeddings (e.g., from transformer models like BERT--our next topic, so stay tune for that!) to capture long-range dependencies and disambiguate surface forms such as "Washington" as a person versus a place, for instance.

[**spaCy** aka, your new best friend]{.underline}: [spaCy](#0) is a powerful Python library for NLP. It provides an integrated pipeline architecture (tokenization, part-of-speech tagging, dependency parsing, lemmatization, and NER) all accessible through a clean, consistent API. Its NER component is quite powerful and it has served me well with complex NER questions on Early Modern Texts (which I find impressive given the various inconsistencies and idiosyncrasies in my corpora). The other option is [NLTK](#0), which was built primarily as a teaching toolkit. I have successfully worked with both and we use NLTK in the tutorial for Weeks 08 and 9, but spaCy prioritizes speed and memory efficiency, making it well-suited for processing large corpora. NLTK returns results more slowly than spaCy, but spaCy does trade off higher memory usage for that speed... ymmv. I want you to be exposed to both libraries.

You should feel free to experiment with both, but I will use spaCy for this tutorial as it tends to be more widely used outside of academia. I do want to emphasize that spaCy is not always superior:

-   I tend to work with NLTK in early stages of research as it tends to be used more often in academic contexts, making it easier to compare results. If you are planning to stay in academia, do look into NLTK. But NLTK doesn't have native word vector support at all, which is one of the meaningful gaps between it and more modern libraries.

-   spaCy ships with pre-trained pipelines that include word vectors, so when you call `token.vector` on spaCy, you are not training your own model, but you're getting a pre-trained word vector. Compare this with what we did with word2vec last week in order to expand on our seed term "merchant." SpaCy loads vectors that were trained externally and baked into the pipeline. So you can't easily swap in "custom" vectors the way `Gensim` lets you.

You need to think through your corpus and your research questions to make meaningful choices here.

[**Overview of this week's tutorial**]{.underline}:

1.  Reuse last week's model: organize your corpus and use the model we trained in Weeks 8-9 [to find the subset]{.underline} we want to focus on (that is, texts that discuss the theme/topic/concept of "merchant" as we defined last time)

2.  Switch to spaCy: learn to use spaCy's pipeline on our target subset of texts from step 1.

3.  Extract entities as structured data and analyze the results.

## Step 0: Load a folder of `.txt` files and build a JSON dataset

So far, we have been working with folders of .txt files. This was fine while we were working with one folder of texts and with only a few of them. Now that our corpus is growing, we want to find a better way to organize our data. Enter JSON (JavaScript Object Notation)! This is a structured text format that stores data as key-value pairs, similar to a Python dictionary. Instead of managing hundreds of separate .txt files scattered across folders, JSON lets us bundle all our texts together in a single, organized file where each text has a unique identifier and associated metadata (like filename, author, date, printer, publisher, location of publication...). This makes our life a lot easier: we can load everything with one command, texts can't accidentally get separated from their metadata, and we can easily add new information fields without reorganizing our entire file system.

This is the format that I (and pretty much everyone else in my line of work) uses. So, how do we do this? First of all, download the folder named "Post_Spring_Break_Texts" from **Canvas** (it should contain 147 files).

Start by creating a new python file (I named it `text_store.py`, we won't reuse it, so it doesn't matter all that much). Now we can read each .txt file, store it in JSON format, and save:

``` python
import json
from pathlib import Path

# Point to the folder with our new Post Spring Break Texts (or whatever you named it)
NEW_TEXTS_DIR = Path.cwd() / "Post_Spring_Break_Texts"


# Read all .txt files
paths = sorted(NEW_TEXTS_DIR.glob("*.txt"))
print("Found .txt files:", len(paths))

# How the JSON file will be organized
records = []
for p in paths:
    text = p.read_text(encoding="utf-8", errors="ignore")
    records.append({
        "doc_id": p.stem,           # filename without extension
        "filename": p.name,         # full filename
        "text": text                # raw text
    })

# Quick check
print("First record keys:", records[0].keys())
print("First doc_id:", records[0]["doc_id"])
print("First text snippet:", records[0]["text"][:200])

# Save the corpus as JSON
OUT_JSON = Path.cwd() / "new_texts.json"

with open(OUT_JSON, "w", encoding="utf-8") as f:
    json.dump(records, f, ensure_ascii=False, indent=2)

print("Saved:", OUT_JSON)
```

Run the code and take a look at the file by opening it. Each entry should look something like:

`{`

`"doc_id": "A01923",`

`"filename": "A01923.txt",`

`"text": "a Panegyrique of congratulation for the concord of the realm of great Britain in unity of religion under one King a ancient writer say that the ground and maintenance of all monarchy and empire be concord their ruin...."`

`}`

**A quick aside:**

The files that we have in this folder don't have much metadata associated with them. What if we had a different type of text file? Well, a lot will depend on the format, but I often have to turn folders of `.xml` files into JSON format. I know that you **all** remember the `.xml` info that I linked to on the syllabus in week 3 (here it is again in the very unlikely case that you don't have it memorized: TEI and XML: [guide here](https://guides.library.illinois.edu/xml/tei#:~:text=The%20Text%20Encoding%20Initiative%20(TEI)%20is%20an,the%20guidelines%20for%20text%20encoding%20in%20TEI.); make sure to look over the examples ♠).

I am going to give you an example of how to do this. We **won't** use this for our tutorial, but I want to give you this example as a resource. You can use this to help you modify the section of code under "How the JSON file will be organized" in the file we just created to look something like:

``` python
records = []
for p in paths:
    # Parse the XML file
    tree = ET.parse(p)
    root = tree.getroot()
    
    # Extract metadata (these paths depend on your XML structure--which are often inconsistent, yes, life does suck a lot of times)
    # Common patterns for TEI XML:
    
    # Try to find author
    author = None
    author_elem = root.find(".//{http://www.tei-c.org/ns/1.0}author")
    if author_elem is not None and author_elem.text:
        author = author_elem.text.strip()
    
    # Try to find publication date
    date = None
    date_elem = root.find(".//{http://www.tei-c.org/ns/1.0}date")
    if date_elem is not None and date_elem.text:
        date = date_elem.text.strip()
    
    # Extract the full text (removing XML tags)
    # Get all text content from the document
    text = " ".join(root.itertext())
    
    records.append({
        "doc_id": p.stem,
        "filename": p.name,
        "author": author,
        "date": date,
        "text": text
    })
```

This can get really messy unfortunately! OK, back to our regularly scheduled program.

## Step 1: Load the classifier artifacts from Week 9

Create one file for this step. Mine is: `week10_step1.py` because I am creative that way. Add the chunks of code as you move through the step. **NOTE**: You will want to add comments as well! I am commenting less than usual *within* the code because I am narrating what I am doing as part of the tutorial. This is your chance to start practicing good commenting!

As I mentioned at the end of Week 9 tutorial, we want to reuse what we created there. So we are going to load the TF-IDF vectorizer and classifier that we trained then and apply it to the texts we just organized as "new_texts.json":

``` python
from pathlib import Path
import joblib

MODEL_DIR = Path.cwd() / "models"

# Load the SAME TF-IDF vectorizer and classifier you trained in Week 08–09
vectorizer = joblib.load(MODEL_DIR / "tfidf_vectorizer.joblib")
clf        = joblib.load(MODEL_DIR / "merchant_logreg.joblib")

print("Loaded TF-IDF vectorizer + logistic regression classifier.")

### apply to JSON corpus

texts = [r["text"] for r in records]

X_new = vectorizer.transform(texts)    # IMPORTANT: use transform, not fit_transform
probs = clf.predict_proba(X_new)[:, 1]
preds = (probs >= 0.50).astype(int)

for r, p, yhat in zip(records, probs, preds):
    r["pred_prob_merchant"] = float(p)
    r["pred_merchant"] = int(yhat)

print("Classified:", len(records), "documents")
print("Predicted merchant (threshold .50):", sum(preds))
```

From this process, you should be seeing that:

-   You found and loaded **147** `.txt` files.

-   The JSON serialization worked `new_texts.json`

<!-- -->

-   You successfully loaded **both** classifier artifacts (vectorizer + logreg) from last week.

In my case, I classified all documents, and at threshold **0.50**, I got **36 “merchant”** predictions. You *might* have something slightly different (I am genuinely curious) because I changed the training set before loading it on Canvas for Weeks 08 and 09.

In any case, now we want to (1) save the classified JSON and (2) create a high-confidence “merchant” subset. That is, we set the threshold at 0.5, which is pretty forgiving. It would be good to have a subset with a stricter threshold for our analysis. For spaCy, you usually want precision over recall (fewer false positives

First, we will save the classified corpus by adding the predictions into the JSON file:

``` python
import json
from pathlib import Path

OUT_CLASSIFIED = Path.cwd() / "classified_texts.json"

with open(OUT_CLASSIFIED, "w", encoding="utf-8") as f:
    json.dump(records, f, ensure_ascii=False, indent=2)

print("Saved classified dataset:", OUT_CLASSIFIED.resolve())
```

Then, we will create our higher confidence subset, setting the threshold to 0.70, and then check the results. This is a good starting point for a higher confidence level for "merchant", but in real-life scenarios, we might have to adjust it based on the requirements of our project.

``` python
THRESH = 0.70

merchant_only = [r for r in records if r["pred_prob_merchant"] >= THRESH]

OUT_MERCHANT = Path.cwd() / "merchant_texts_for_spacy.json"

with open(OUT_MERCHANT, "w", encoding="utf-8") as f:
    json.dump(merchant_only, f, ensure_ascii=False, indent=2)

print(f"High-confidence merchant texts (p >= {THRESH}):", len(merchant_only))
print("Saved:", OUT_MERCHANT.resolve())

### checking how this higher threshold compares with the 0.50 one

top5 = sorted(records, key=lambda r: r["pred_prob_merchant"], reverse=True)[:5]
for r in top5:
    print(r["doc_id"], r["pred_prob_merchant"])
```

My results are promising. I had 36 predicted merchant texts at 0.50 threshold and now I have 23 "high-confidence" merchant texts at threshold 0.70. The top five confidence scores look very reassuring:

`B14801 => 0.9999994900493665`

``` A22547``=>``0.996078735040299 ```

``` B11348``=>``0.987688407922301 ```

``` A68617``=>``0.9688126041191412 ```

``` A02626``=>``0.9271125403503189 ```

Just to be safe, we can take a quick look at a couple of the results to double check that they are indeed "merchant" texts. Text B14801 (the highest confidence score) is Edward Misselden's *Free trade* (1622) and text A22547 (second highest confidence score) is a royal proclamation by Charles I [*Concerning the trade of Ginney, Binney, in the parts of Africa*](https://quod.lib.umich.edu/e/eebo/A22547.0001.001?rgn=main;view=fulltext) (1631) \[this is a short text, if you are interested in reading it!\]. Both are documents that I recognize as related, and highly so, to the topic of "merchant"--so far, so good!

### What about the "lost" documents:

Going from 0.50 threshold to 0.70 decreased out subset of text from 36 to 23, so we "lost" 13 documents. Does this matter? Maybe or maybe not! It's hard to know at this stage. My approach to a problem like this is the following:

-   work with the smaller, "high-confidence" subset

-   look at the results of the NER task and evaluate how well the model is performing

-   manually examine a few (2-3) of the "lost" 13 documents and see if I want to include them for a second analysis

## Step 2:

Now we are ready to move f[rom classification to linguistic annotation]{.underline}: we will run spaCy on our corpus in order to perform NER and extract entities that we want to analyze. We are going to do this *slowly* so as to make our reasoning explicit.

-   First, we will load one document into spaCy and inspect it

-   Then, we will extract entities from that one document

Once we understand what the process looks like for one document, we can loop over all 23 documents and export a structured entity table (we will do JSON, but CSV is also a common choice).

Before using spaCy, we have to install it and download the English model. Open the terminal [in your project folder]{.underline}, then:

`pip install spacy`

`python -m spacy download en_core_web_sm`

Here's the [**full installation guide**](https://spacy.io/usage) and below is a quick reference summarizing the steps (including creating and activating venv in case you need to redo this step from Weeks-08-09):

| Task | Windows | macOS |
|--------------------|-----------------------------|-----------------------|
| Create venv | `python -m venv .venv` | `python3 -m venv .venv` |
| Activate venv | `.venv\Scripts\Activate.ps1` | `source .venv/bin/activate` |
| Update pip | `python -m pip install --upgrade pip setuptools wheel` | `python -m pip install --upgrade pip setuptools wheel` |
| Install spaCy | `pip install spacy` | `pip install spacy` |
| Download model | `python -m spacy download en_core_web_sm` | `python -m spacy download en_core_web_sm` |
| Verify install | `python -m spacy info` | `python -m spacy info` |

[Note:]{.underline} there are a number of ways in which spaCy's installation can go wrong (you are in the wrong directory, you did other work in between and so you need to reactivate venv, you are a Mac user and my suggestions don't match your needs, ...): youtube is your friend. There are a lot of tutorials that go through all possible scenarios for installation.

For future reference: you can see that there are a number of languages with trained pipelines that you can use instead of English. We will stick to English for now.

Now that we have the tools that we need, we can load spaCy and our high-confidence merchant corpus. Create a new file named `week10-step2.py`, run the following code to import spaCy, the json we created, and process one file:

``` python
import spacy
import json
from pathlib import Path

# Load the English model
nlp = spacy.load("en_core_web_sm")

print("spaCy model loaded.")
MERCHANT_PATH = Path.cwd() / "merchant_texts_for_spacy.json"

with open(MERCHANT_PATH, "r", encoding="utf-8") as f:
    merchant_records = json.load(f)

print("Number of merchant texts:", len(merchant_records))

sample_text = merchant_records[0]["text"]

doc = nlp(sample_text) ## All the good stuff happens here: see note below at [1]

print("Processed one document.")
print("Number of tokens:", len(doc))
```

\[1\] The line `doc = nlp (sample_text)` passes raw text into a processing pipeline that:

-   tokenizes the text

-   splits sentences

-   assigns part-of-speech tag

-   computes syntactic dependencies

-   identifies named entities

The result of this pipeline is a structured [spaCy Doc](https://spacy.io/api/doc)object that contains:

-   The original text

-   A sequence of tokens

-   Sentence boundaries

-   POS tags

-   Dependency relations

-   Named entities

-   Character offsets

Imagine you have a small, sample sentence: "William went to London in 1603." It's now going to be in a data structure:

| Token  | POS   | Dependency | Entity |
|--------|-------|------------|--------|
| John   | PROPN | nsubj      | PERSON |
| went   | VERB  | ROOT       | —      |
| to     | ADP   | prep       | —      |
| London | PROPN | pobj       | GPE    |
| in     | ADP   | prep       | —      |
| 1605   | NUM   | pobj       | DATE   |

Once you run the code, the output will look like this:

`spaCy model loaded.`

`Number of merchant texts: 23`

`Processed one document.`

`Number of tokens: 6341`

Now we are going to extract the named entities for that single document and inspect them:

``` Python
for ent in doc.ents:
    print(ent.text, "|", ent.label_)

from collections import Counter

Counter([ent.label_ for ent in doc.ents]) 
```

What is going on in this section of code? First of all, `doc.ents` is a list of entity span objects, where each `ent` has attributes like: `ent.text` and `ent.label_`. For example:

```         
[Span("christian", label="NORP"),
 Span("America", label="GPE"),
 Span("two", label="Cardinal")]
```

You can see this when you `print(ent.text, "|", ent.label_)`. We then extract the labels (NORP, GPE, etc) by creating a [new]{.underline} list with this syntax `[ent.label_ for ent in doc.ents]`. This is called a list comprehension ([see here](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)) and, in our case, it will produce something like this: `["PERSON", "PERSON", "GPE", "NORP", "Ordinal", "DATE", "DATE",...]`. We then use `Counter()` to create a dictionary-like object (you can review [dictionaries here](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)) that counts the frequency of each entity:

```         
Counter({
    "PERSON": 2,
    "DATE": 2,
    "GPE": 1
})
```

This may not be very meaningful yet, but this is a helpful step in understanding the composition of texts for steps down the line in your process. For example, I will use this kind of dictionary to filter and/or route documents based on the kind of entities it contains. A document with, say, more than 5 "PERSON" entities will be flagged for further analysis when I am trying to track demographic data for migration.

Now we are ready to scale up to all 23 documents:

``` Python
all_entities = []

for record in merchant_records:
    doc = nlp(record["text"])
    
    for ent in doc.ents:
        all_entities.append({
            "doc_id": record["doc_id"],
            "entity_text": ent.text,
            "entity_label": ent.label_
        })

print("Total entities extracted:", len(all_entities))
```

When you run this, you **will** get an error message:

> ValueError: \[E088\] Text of length 1100120 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.

The problem that we are running into is that one of our texts is very long: 1,100,120 characters. SpaCy's default maximum length is [1,000,000](https://spacy.io/api/language). You *could* increase the maximum length (see the link I just gave you), but that's probably going to be a bad idea for your machine (when you look at the error message, you can see that the "parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input").

The solution is to break up our texts into smaller segments and process them in chunks. Remove the code that we just added and [instead]{.underline} add the following:

``` Python
all_entities = []

CHUNK_SIZE = 50000  # characters per chunk

for record in merchant_records:
    text = record["text"]
    
    # Split text into chunks
    for i in range(0, len(text), CHUNK_SIZE):
        chunk = text[i:i + CHUNK_SIZE]
        doc = nlp(chunk)
        
        for ent in doc.ents:
            all_entities.append({
                "doc_id": record["doc_id"],
                "entity_text": ent.text,
                "entity_label": ent.label_
            })

print("Total entities extracted:", len(all_entities)) 

#### IMPORTANT STEP: save all the raw entities

import json
from pathlib import Path

OUT_ENTS_RAW = Path.cwd() / "merchant_entities_raw.json"

with open(OUT_ENTS_RAW, "w", encoding="utf-8") as f:
    json.dump(all_entities, f, ensure_ascii=False, indent=2)

print("Saved raw entity mentions:", OUT_ENTS_RAW.resolve())
```

Now try running it: this will take a while, but that's ok! At the end, you should see:

`Total entities extracted: 269702`

Running spaCy is the slowest step in today's tutorial, so we are going to do it once, save the results, and then create a new file fo the rest of our work!

### An aside: refining this process

This isn't without risks. There is a real possibility that an entity might split across segment boundary. For example, something like "the Virginia Company of London" might end up with "Virginia Company" in one chunk and "of London" in another.

We can improve on this (at least a bit!) by doing the following instead:

-   Cut at \~50,000 characters.

-   Look forward a limited distance (a “lookahead window”) for the first boundary mark among: `. ! ? ; :` (and optionally a newline if this makes sense for your type of text).

-   Extend the chunk to that boundary.

We are **not** going to implement this code, but it would look like this:

``` Python
import re

CHUNK_SIZE = 50000
LOOKAHEAD  = 5000   # don't search forever; keeps chunks bounded

boundary_re = re.compile(r"[.!?;:\n]")

def next_boundary(text: str, start_idx: int) -> int:
    """
    Return index *after* the next boundary punctuation (or newline) found
    between start_idx and start_idx+LOOKAHEAD.
    If none found, return start_idx (meaning: no extension).
    """
    m = boundary_re.search(text, pos=start_idx, endpos=min(len(text), start_idx + LOOKAHEAD))
    if m:
        return m.end()
    return start_idx  # no boundary found in lookahead window

all_entities = []

for record in merchant_records:
    text = record["text"]
    start = 0

    while start < len(text):
        end = min(len(text), start + CHUNK_SIZE)

        if end < len(text):
            extended_end = next_boundary(text, end)
            if extended_end > end:
                end = extended_end

        chunk = text[start:end]
        doc = nlp(chunk)

        for ent in doc.ents:
            all_entities.append({
                "doc_id": record["doc_id"],
                "entity_text": ent.text,
                "entity_label": ent.label_
            })

        start = end

print("Total entities extracted:", len(all_entities))
```

Why am I not running this version? Well, it's going to potentially create its own problems (especially for Early Modern texts). While this makes chunk boundaries more linguistically plausible, it still won’t guarantee you never split an entity. Entities can cross punctuation and in Early Modern English, they do so in unexpected ways: “Mr. John Smith” = "Mr: John Smith" = "Mr J. Smith" and many other horrors. I still want you to see this option in case you need it.

### Back to our task:

Create a new file named `week10-step3.py` and run it:

``` Python
# Imports
import json
import re
from pathlib import Path
from collections import Counter

# Load entity mentions from our spaCy work
ENT_PATH = Path.cwd() / "merchant_entities_raw.json"

with open(ENT_PATH, "r", encoding="utf-8") as f:
    all_entities = json.load(f)

print("Loaded entity mentions:", len(all_entities))

# Minimal normalization to remove unecessary whitespace
def normalize_ent(text: str) -> str:
    text = text.strip()
    text = re.sub(r"\s+", " ", text)
    return text

# 4. Count
entity_text_counts = Counter(
    normalize_ent(e["entity_text"]) for e in all_entities
)

print("Unique entity strings:", len(entity_text_counts))
print("Top 20 entity strings:")
for ent_text, n in entity_text_counts.most_common(20):
    print(f"{n:>7}  {ent_text}")
```

**Regarding "normalization":** we want to be very restrained with this step. SpaCy gives us mentions as they appear in the text. Before counting them, I want to remove trivial formatting differences so that whitespace inconsistencies do not create artificial duplicates. For example: we want `"   London"` = `"London"`.

I am **not** lowercasing, removing punctuation, striping hyphens, or modernizing spelling! This would risk collapsing *important* differences: while "company" = "companie"; a given instance of the lastname "Smith" may or *may not* equal "Smythe" even though sometimes the same person spelled his name both ways (it depends on the person and who wrote their name in that particular document).

When you run this step, you should get:

Loaded entity mentions: 269702
Unique entity strings: 48301
Top 20 entity strings:
8193 two
7015 one
6141 three
5782 first
4260 four
3178 indian
2843 christian
2757 five
2750 english
2308 six
2243 England
1867 second
1811 seven
1663 eight
1591 ten
1527 Spain
1473 twenty
1435 Portugal
1325 half
1175 this day

Now, some of these entities are not all that interesting to us (or maybe not to me). I really don't care for all the numbers ("one" "first" "two"...). This is obviously dependent on the research question, but I am going to filter for the kinds of entities that I am interested in:

``` Python
#### Count entity strings for desired labels ONLY

from collections import Counter

KEEP_LABELS = {"PERSON", "ORG", "GPE", "LOC", "NORP"} #these are the labels I want

filtered = [                                        #filter for them
    e for e in all_entities
    if e["entity_label"] in KEEP_LABELS
]

print("Filtered entity mentions:", len(filtered))


## Count by label and separate counts by entity type

counts_by_label = {lab: Counter() for lab in KEEP_LABELS}

for e in filtered:
    lab = e["entity_label"]
    txt = normalize_ent(e["entity_text"])
    counts_by_label[lab][txt] += 1

for lab in ["ORG", "GPE", "PERSON", "LOC", "NORP"]:
    print(f"\nTop 15 {lab}:")
    for ent_text, n in counts_by_label[lab].most_common(15):
        print(f"{n:>7}  {ent_text}")
```

This is what we get:

[Top 20 entity strings (filtered)]{.underline}:

> 3178 indian 2843 christian 2243 England 1611 english 1527 Spain 1435 Portugal 1143 China 1036 India 1012 portugal 972 spanish 881 french 746 Fez 711 dutch 676 Africa 661 Rome 593 Italy 569 Egypt 569 Peru 565 France 552 John

[Top 15 ORG:]{.underline}

> 371 Tyrone 271 Constantinople 209 senate 181 Banda 153 Christendom 134 Inca 133 Connaght 119 Nanquin 103 rey 97 Pegu 96 Florence 93 Cortes 93 Helena 93 Tercera 91 Caesar

[Top 15 GPE]{.underline}:

> 2242 England 1527 Spain 1435 Portugal 1143 China 1036 India 1012 portugal 660 Rome 593 Italy 569 Egypt 568 Peru 565 France 551 jerusalem 511 Mexico 499 London 472 Ireland

[Top 15 PERSON]{.underline}:

> 552 John 507 Thomas 392 Turk 342 Bantam 301 Fez 295 Solomon 292 Chan 283 Peter 279 Henry 278 William 278 Chinois 272 Francis 267 Alexander 247 Charles 237 Guiana

[Top 15 LOC]{.underline}:

> 675 Africa 522 Europe 418 Asia 318 Sea 231 the red sea 187 the south sea 91 the ocean sea 88 the sea coast 79 Alps 72 west south-west 67 east north-east 62 the north sea 59 the Mediterran sea 49 gulf 45 Peninsula

[Top 15 NORP]{.underline}:

> 3178 indian 2843 christian 1611 english 972 spanish 881 french 708 dutch 541 italian 528 greek 350 irish 325 german 322 latin 287 persian 283 Spaniard 274 african 252 mexican

Well, these are mixed results, clearly. Entities like "Christendom" or "Constantinople" are [not]{.underline} ORG. Similarly, entities like "Bantam" and "Guiana" are [nor]{.underline} PERSON.

Overall this isn't surprising: I am asking spaCy to work with Early Modern texts and this is not what it was designed to do. At the same time, spaCy should be able to recognize "Guiana" as a LOC (the term "Guianas" is still used to talk about Guyana, Suriname, and French Guiana).
