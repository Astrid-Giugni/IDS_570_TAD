---
title: "Week 03: Dictionaries"
format: html
execute:
  echo: true
  warning: false
  message: false
---

### A Quick Aside: Fixing Spelling with Regex

When looking at the data from week 2, you may have noticed some unusual characters (see, for example, the print out of the tibble "word_counts"). We need to make some decisions on how to handle them. This is where regular expressions come in. Regular expressions, abbreviated to regex, are a way to define patterns within strings. It will be helpful to get familiar with the package `stringr` (which we have used before without comment) by looking at the documentation [[here]{.underline}](https://stringr.tidyverse.org/).

```{r}
library(readr) 
library(dplyr) 
library(stringr) 
library(tibble)  

circle_raw <- read_file("C:/Users/astri/Desktop/R-work/IDS_570_TAD/texts/A07594__Circle_of_Commerce.txt") 

text_tbl <- tibble(   
  doc_title = "The Circle of Commerce",   
  text = circle_raw 
  )  

# Taking a look at text_tbl (or at least part of it since it's very long):
text_tbl %>% select(doc_title)
nchar(text_tbl$text)
str_sub(text_tbl$text, 1, 400)
```

Before we change any parts of the text, we are going to preserve the original text in case we change our mind later on. So we are creating a new `tibble` named `text_tbl` that contains the original text in a column named `text_original`:

```{r}
text_tbl <- text_tbl %>%
  mutate(text_original = text)
```

In early modern print, the long *s* (ſ) represents the same letter as the modern *s* (both were used in print). When it appears explicitly in transcription or OCR, we can safely *normalize* it to a modern *s*. In historical texts, regex is often used for *normalization*, not “correction." the long *s* isn't an error, it's just a variant (as the British "colour" vs. the American "color" are variants of the same word).

**Regular expressions** allow us to:

-   identify patterns in text, and

-   replace them systematically.

```{r}
 text_tbl <- text_tbl %>%   
  mutate(     
    text_clean = str_replace_all(text_original, "ſ", "s")   
    )  


# Let's count how many "ſ" we had before and after to check that the substitution worked:

tibble(
  long_s_before = str_count(text_tbl$text_original, "ſ"),
  long_s_after  = str_count(text_tbl$text_clean, "ſ")
)
```

A bit of warning:

-   We replace **only the explicit character ſ**

-   Sometimes, the long S is represented as an "f" in transcribed/OCR'ed texts, but I do *not* recommending trying to guess when an `f` “should be” an `s`. Make sure that you know why! If not, ask!

#### Standardizing Name Spellings:

Early Modern spelling was not standardized, meaning that the same word may have been spelled in a number of different ways by the same author within the same document. Name spelling was also not standardized and this fact presents some serious problems for us. While it’s pretty easy to decide that “friend” = “freind” (a peculiar spelling by the poet John Milton) for normalization processes, standardizing names in historical texts is not a neutral preprocessing step. It requires biographical and historical research.

If we decide that, for example, Smythe, Smyth, and Smith all refer to the same name, we are making a scholarly claim about identity, authorship, and equivalence across spelling variation. Different research questions may require different choices. Therefore, name standardization rules should always be documented and justified as part of your research.

Having said that, how would we go about standardizing the variant of Smith?

The first step is to define an explicit standardization map. This step will seem excessive right now, but if you are standardizing a large number of words, this vector will keep track of your decisions and it can be expanded as you go.

```{r}
name_map <- c(   
"Smythe" = "Smith",   "Smyth"  = "Smith",   "Smithe" = "Smith" 
)
```

Now, we do **not** want to simply "search and replace" based on this map. The name-place "Smythfield" (which I am making up right now) should not be replaced by "Smithfield." What we want to replace is the *specific* string "Smyth". To do this, we have to think about word boundaries and typesetting (we want to catch all versions of the name regardless of case):

```{r}
text_standard <- text_tbl %>%
  mutate(
    text_norm = text_clean %>%
      str_replace_all(regex("\\bSmythe\\b", ignore_case = TRUE), "Smith") %>%
      str_replace_all(regex("\\bSmyth\\b",  ignore_case = TRUE), "Smith") %>%
      str_replace_all(regex("\\bSmithe\\b", ignore_case = TRUE), "Smith")
  )
```

The code above uses `\\b` to denote the word boundary. This syntax is peculiar to R (in Python is would be `\b` instead). The boundary matches the position between: a word character (`[A–Z a–z 0–9 _]`) AND a non-word character (space, punctuation, start/end of string). So, `\\bSmithe\\b` will catch: "Smithe", "Smithe.", "Smithe)", etc. but **not** Smithefield.

Note also that I added a new column with `mutate`, called `text_norm`. This way, the original column `text_clean` is not changed.

### Describing the Text: N-grams and Trade

Let's start to explore how *The Circle of Commerce* talks about "trade." Last week we focused on single words, which is fine. But there are limitations to what word counts tell us about a text. What if we wanted to know more than just how often the word "trade" is used? An obvious, first question is to ask how Misselden describes trade in *The Circle*: is he focusing on "domestic trade" or "foreign trade"? Does the think that "trade is growing" or that "trade is declining"? We can start exploring these kinds of relationship by using **n-grams** containing the word trade. An n-gram is simply a sequence of n words. So a bi-gram is two words, such as "free trade"; a tri-gram is three words, such as "trade is growing."

I am going to focus on bigrams as it will keep the lesson clean and easy to follow. Let's start by setting up the needed packages (in addition to the ones we already loaded):

```{r}
library(tidyr) 
library(tidytext) 
library(ggplot2) 
library(forcats)

#We are going to load two more libraries that are going to be used for the next goal for today (collocations): more info at https://quanteda.io/ 
library(quanteda) 
library(quanteda.textstats)
```

Bigrams tell us which words tend to appear next to each other in our text. But their raw frequencies can be dominated by turns of phrase (such as "the trade" or "this trade") that may or may not be helpful in our analysis. So we will have to make some decisions on how to clean the texts and how to interpret our results.

We are going to start by tokenizing into bigrams. To do so, we are going to take two words at the time rather than one at the time when we use `unnest_tokens`.

```{r}

bigrams_raw <- text_standard %>%
 select(doc_title, text_norm) %>%
 unnest_tokens(output = "bigram", input = text_norm, token = "ngrams", n = 2)

#let's take a look

bigrams_raw %>% count(bigram, sort = TRUE) %>% slice_head(n = 10)

```

#### Cleaning the bigrams:

As you can see from the list above, we never removed stopwords from `text_standard`. We are going to do this now. We will have to split up the bigrams into single words and remove stopwords for each "half" of the bigram:

```{r}
data("stop_words") # the standard list from tidytext, but you can adapt the process from week 2 to include custom stop words

bigrams_clean <- bigrams_raw %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(str_detect(word1, "^[a-z]+$")) %>% # keep alphabetic tokens
filter(str_detect(word2, "^[a-z]+$"))

bigrams_clean %>% count(word1, word2, sort = TRUE) %>% slice_head(n = 10)
```

Now we glue the bigrams back together:

```{r}
bigram_counts <- bigrams_clean %>%
count(word1, word2, sort = TRUE) %>%
unite("bigram", word1, word2, sep = " ")

bigram_counts %>%
slice_head(n = 20) %>%
mutate(bigram = fct_reorder(bigram, n)) %>%
ggplot(aes(x = n, y = bigram)) +
geom_col() +
labs(
title = "Most frequent bigrams (after stopword filtering)",
x = "Count",
y = NULL
)
```

[**Important conceptual point**]{.underline}**:** why are we creating bigrams first, cleaning them from stopwords, and *then* gluing them back together? Shouldn't we just tokenize the text into individual words, clean the stopwords, and then find the bigrams?

Let's test this out with a phrase such as: "strong economy of international trade." If we do bigrams first, we get: "strong economy", "economy of", "of international", "international trade." Removing stopwords will give us: "strong economy", "international trade" (make sure this makes sense to you).

If we cleaned the stopwords *first*, the phrase would turn into: "strong economy international trade." The bigrams would then be: "strong economy", "economy international", "international trade." We now have an "extra" bigram, "economy international" (can you see why?).

**NB**: there will be situations where the second method (clean first, bigrams second) *may be* the better option! Think of the phrase: "the state of the art." The clean first method will give us "state art," while our method (bigrams first, clean second) won't capture this at all. The bigram "state art" is a distortion of the concept "the state of the art"--which method is better is up to you.
