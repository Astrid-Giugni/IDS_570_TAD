[
  {
    "objectID": "week-08-09-word2vec.html",
    "href": "week-08-09-word2vec.html",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "So far, we have done unsupervised and semi-structured approaches to text analysis (frequency, TF–IDF, similarity, co-occurrence, and Word2Vec). During these two weeks we are going to make a key transition to supervised methods. I am grouping the two together because we need to take some time to think about how to thoughtfully set up the classification task–language is flexible and conceptual boundaries are porous!\nYou can move across the code and steps for these two weeks at your own pace as you get familiar with the process. In class: I will move through the lectures assuming that everyone will finish working through this entire tutorial by March 6th (the end of week 9).\nFor Week 8:\nThe goal is to train a supervised classifier that learns to recognize a category of texts. In particular, we are going to figure out how to distinguish which texts engage in merchant-related discourse versus those that do not.\nIn humanities (and social sciences), supervised learning is often the workhorse method for answering questions like:\n\nIs this text pro-trade or protectionist?\nDoes this paragraph contain “merchant discourse”?\nWhich documents are about X, versus not-X?\n\nBut supervised learning raises a methodological question immediately: Where do labels come from?\nIn humanities contexts, we often do not begin with a fully human-labeled dataset. So we will begin with weak supervision: we will generate imperfect labels using rules, then train a model, and validate it carefully. So, for week 8, you will:\n\nDefine a category you want to detect in texts (a binary label). For us, it will be about the concept “merchant” (this will be our “seed” concept).\nSegment texts into a usable unit of analysis (chunks).\nCreate a weakly supervised training dataset using a seed term and related terms.\n\nFor Week 9:\n\nTrain a baseline text classifier in Python using:\n\nTF–IDF features\nLogistic regression\n\nEvaluate the classifier using:\n\na train/test split\na confusion matrix\nprecision, recall, and F1 score\n\nReflect on what “accuracy” means when labels come from weak supervision.\n\n\n\nHere is the full pipeline we will build:\n\nLoad texts from a folder (texts/): you will need to get the 500+ files that I uploaded on Canvas under “Train_Text_Documents.”\nSegment each text into sentence-based chunks (our “documents” for classification)\nUse Word2Vec (trained on our corpus) to expand a seed concept (seed: merchant)\nBuild a search-word list (Tier A/B/C: more details below)\nUse the search-word list to create weak labels\nCreate a CORE vs NEG dataset (and optionally a MAYBE set)\nSplit into train and test\nTrain a classifier (TF–IDF + logistic regression)\nEvaluate and interpret\n\nWe will move slowly and validate outputs at each stage. Again, I am keeping the training wheels on, but\n\n\n\n\nVSCode installed\nA project folder like: IDS_570_TAD/\nA subfolder: texts/ containing your .txt files\nA Python virtual environment (.venv) activated in VSCode\nPackages we will use:\n\nnltk\ngensim\nscikit-learn\ntqdm\n\n\n(We will install these together in the steps below.)\n\n\n\n\n\n\nBefore we do any modeling, we need to make sure our project is set up correctly. This step is mainly for those of you new to Python, but it also will help everyone learn good project organization skills: many errors in text analysis come from working in the wrong folder or environment.\nWhere:\n\ntexts/ contains the raw .txt files you want to analyze.\ndata/ will store intermediate outputs (JSON files, inspection samples).\nmodels/ will store trained models (e.g., Word2Vec, classifiers).\nEach stepX_*.py file does one conceptual task.\n\nWe intentionally separate steps into different scripts so that: - each step is easy to test and debug - you can rerun part of the pipeline without rerunning everything - you can clearly see how the pipeline is constructed.\n\n\nMake sure you open VSCode in the project folder (IDS_570_TAD).\nThen open the terminal: there are a ton of videos online, if you have never done it before. If you are using something other than VSCode, you can also find guides online.\nYou should see something like:\n```bash PS D:…_570_TAD&gt;\nIf you see a different folder, you are in the wrong place. This is important because we are going to set up a Python virtual environment to keep project-specific packages isolated.\nNext, create the environment and activate it:\npython -m venv .venv\nthen, in Windows:\n.venv\\Scripts\\Activate.ps1\nNote, if this fails try:\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\nThen try activation again.\nOR in Mac/Linux\nsource .venv/bin/activate\nFor full instructions, see here. Your terminal prompt should now include .venv.\n\n\n\nInstall the libraries we will use:\npip install nltk gensim scikit-learn tqdm\nWe will download one additional resource (NLTK sentence tokenizer) later.\nBefore moving on, check:\n\nCan you see your .txt files from Python?\nDoes python step1_load_texts.py run without errors?\nIs your terminal in the correct folder?\n\nIf something is off here, it is worth fixing before you continue: go back through the steps above and see if something is missing. There are also a lot of videos online that help you go through this process.\n\n\n\n\nBefore we segment, vectorize, or classify anything, we need to confirm that Python can see and read our texts correctly.\n\n\nWe will assume all .txt files are stored in a folder called texts/. Now, create a new file and name it: step1_load_texts.py and then run it:\nfrom pathlib import Path\n\nTEXT_DIR = Path(\"texts\")\n\n# Collect all .txt files\nfiles = sorted(TEXT_DIR.glob(\"*.txt\"))\n\nprint(f\"Found {len(files)} .txt files.\")\n\n# Print the first few filenames\nprint(\"First 10 files:\")\nfor f in files[:10]:\n    print(\" \", f)\nThis should tell you that you found about 516 .txt files. Note: you have a very slightly different training set; you are responsible for keeping track of how this affects things down the line (though the first few steps should match). It’s my way of keeping you on your toes!\nYou should see a list of filenames printed to the terminal. Now we want to inspect one file to make sure that we have set things up correctly (note: in the future, I will assume that you will do this on your own!). To the same file as above, add the following and run the script again:\n# Read one example file\nexample_file = files[0]\n\nwith open(example_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n    text = f.read()\n\nprint(\"\\nReading file:\")\nprint(example_file)\nprint(\"-\" * 40)\nprint(\"Number of characters:\", len(text))\nprint(\"\\nFirst 1,000 characters:\\n\")\nprint(text[:1000])\nWe are just making sure that we can load the data and that things look correct (that is, we have funky looking Early Modern texts). In my case (and it should be in your too), the file read was: texts\\A00419.txt; number of characters: 2888252l and then you should get the first few lines of the text.\n\n\n\n\nOur .txt files are far too large to classify as single units. A supervised classifier expects many medium-sized examples and some of the texts I gave you are very long (the one above had 2,888,252 characters!). So we need to decide what counts as a “document” for classification.\n\nFor this week, we will treat chunks of sentences as our unit of analysis. We’re using sentence chunks because we want to capture enough context for semantic meaning (a single sentence might be ambiguous), but not so much that we get incorrect labels (a 50-page chapter likely discusses many topics and might lead to mislabelling: such a large “chunk” may not be about “merchants” in any meaningful sense, but, for example, it could be a biblical passage mentioning a merchants metaphorically–we would need a more careful approach for these kinds of texts).\n\nNote: There is no universally correct choice here; segmentation is a modeling decision and I am basing the “chunk” size on experience.\nWe will begin by splitting each text into sentences using NLTK. I have two pieces of documentation for NLTK that I recommend, if you want to delve into it: here and here.\nCreate a new file: step2_segment_and_chunk.py . We will chunk the texts and check some basic diagnostics to make sure that the code is actually running properly.\nfrom pathlib import Path\nimport nltk\n\n# Download required tokenizers (run once)\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\n\nTEXT_DIR = Path(\"texts\")\ntxt_paths = sorted(TEXT_DIR.glob(\"*.txt\"))\n\nsample_path = txt_paths[0]\n\nwith open(sample_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n    text = f.read()\n\n# Split text into sentences\nsentences = nltk.sent_tokenize(text)\n\nprint(\"File:\", sample_path)\nprint(\"Number of sentences:\", len(sentences))\nprint()\n\n# Group sentences into chunks of ~120 words\nTARGET_WORDS = 120\n\nchunks = []\ncurrent = []\ncurrent_len = 0\n\nfor sent in sentences:\n    words = sent.split()\n    if not words:\n        continue\n\n    # If adding this sentence would exceed the target, finalize the chunk\n    if current_len + len(words) &gt; TARGET_WORDS and current:\n        chunks.append(\" \".join(current))\n        current = []\n        current_len = 0\n\n    current.append(sent)\n    current_len += len(words)\n\n# Add any leftover sentences\nif current:\n    chunks.append(\" \".join(current))\n\nprint(\"Number of chunks:\", len(chunks))\n\n# Diagnostics on chunk length (rough word counts)\nlengths = [len(c.split()) for c in chunks]\nlengths_sorted = sorted(lengths)\n\nprint()\nprint(\"Approx word counts per chunk:\")\nprint(\"  min:\", min(lengths))\nprint(\"  median:\", lengths_sorted[len(lengths_sorted)//2])\nprint(\"  max:\", max(lengths))\n\nlo, hi = 5, 200\nin_range = sum(lo &lt;= n &lt;= hi for n in lengths)\nprint(f\"Chunks with {lo}–{hi} words:\", in_range)\nprint(\"Share in range:\", round(in_range / len(lengths), 3))\n\nprint()\nprint(\"--- Chunk 1 preview (first 400 chars) ---\")\nprint(chunks[0][:400])\nAnd run the script.\nYou should see:\n\nThousands of chunks (5172)\nA median chunk length around 100–120 words (median: 101)\nMost chunks falling in the 5–200 word range (5009; with the share in range: 0.968)\nAnd a preview of the first chunk.\n\nThis tells us that our segmentation choice is reasonable for classification.\n\n\n\nThis step will be conceptually familiar from when we used unnest_tokens() in tidytext.\nwe transform a text column into word tokens.\nWe will do this using gensim.utils.simple_preprocess().\nCreate a new file: step3_tokenize_chunks.py. This new script is going to build on what we did in the “step2” script, s you will find the same logic repeated at the beginning.\nfrom pathlib import Path\nimport nltk\nfrom gensim.utils import simple_preprocess\n\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\n\nTEXT_DIR = Path(\"texts\")\ntxt_paths = sorted(TEXT_DIR.glob(\"*.txt\"))\nsample_path = txt_paths[0]\n\nwith open(sample_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n    text = f.read()\n\n# Step 2 logic: sentences -&gt; chunks (~120 words)\nsentences = nltk.sent_tokenize(text)\n\nTARGET_WORDS = 120\nchunks = []\ncurrent = []\ncurrent_len = 0\n\nfor sent in sentences:\n    words = sent.split()\n    if not words:\n        continue\n\n    if current_len + len(words) &gt; TARGET_WORDS and current:\n        chunks.append(\" \".join(current))\n        current = []\n        current_len = 0\n\n    current.append(sent)\n    current_len += len(words)\n\nif current:\n    chunks.append(\" \".join(current))\n\n# Step 3: now we tokenize each chunk\ntoken_lists = [simple_preprocess(c, deacc=True) for c in chunks]\n\nprint(\"File:\", sample_path)\nprint(\"Chunks (strings):\", len(chunks))\nprint(\"Chunks (token lists):\", len(token_lists))\n\nprint(\"\\n--- Token preview (first 60 tokens of first chunk) ---\")\nprint(token_lists[0][:60])\n\nprint(\"\\n--- Token count of first chunk ---\")\nprint(len(token_lists[0]))\nNow run it. What we are doing:\n\nsimple_preprocess() returns a list of tokens.\nTokens are lowercased and cleaned.\nThis is now in the format required by Word2Vec:\n\nWord2Vec expects sentences=[[\"token\",\"token\",...], ...]\n\n\nThe output should look like:\nChunks (strings): 5172\nChunks (token lists): 5172\n--- Token preview (first 60 tokens of first chunk) --- ['the', 'first', 'booke', 'of', 'the', 'covntrie', 'farme', 'chap', 'what', 'manner', 'of', 'husbandrie', 'is', 'entreated', 'of', 'in', 'the', 'discourse', 'following', 'even', 'as', 'the', 'manner', 'of', 'building', 'vsed', 'at', 'this', 'day', 'the', 'varietie', 'of', 'countries', 'causeth', 'diuers', 'manner', 'of', 'labouring', 'of', 'the', 'earth']\n--- Token count of first chunk --- 41\n\n\ngensim may look “stuck” the first time you import it. When you import gensim (especially on Windows), Python may appear to freeze for 10–60 seconds. This is normal and does not mean your code is broken.\n\ngensim loads compiled components used for Word2Vec\non Windows, this initial load can be slow\nPython does not print progress messages during this step If the terminal is not showing an error message, wait patiently before interrupting the process. Only stop the program if it has been unresponsive for several minutes.\n\n\n\n\n\nSo far, we have been working with a single example file to make sure that things are working. Now we are ready to apply the same segmentation and tokenization steps to the entire corpus. We are going to make the following sampling decisions:\n\ndiscard chunks that have fewer than 5 tokens (not enough context)\ndiscard chunks that have more than 200 tokens (too diffuse and hard to interpret)\n\nAgain, these thresholds are based on experience in working with Early Modern texts: they are reasonable defaults to start with and we could always change them if things don’t work out. The goal is to define a reasonable and usable training example.\nCreate a new file: step4_process_all_files.py.\nfrom pathlib import Path\nimport nltk\nfrom gensim.utils import simple_preprocess\n\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\n\nTEXT_DIR = Path(\"texts\")\ntxt_paths = sorted(TEXT_DIR.glob(\"*.txt\"))\n\nTARGET_WORDS = 120\nMIN_WORDS = 5\nMAX_WORDS = 200\n\nall_chunks = []\nall_token_lists = []\n\ndef chunk_text(text, target_words=120):\n    sentences = nltk.sent_tokenize(text)\n\n    chunks = []\n    current = []\n    current_len = 0\n\n    for sent in sentences:\n        words = sent.split()\n        if not words:\n            continue\n\n        if current_len + len(words) &gt; target_words and current:\n            chunks.append(\" \".join(current))\n            current = []\n            current_len = 0\n\n        current.append(sent)\n        current_len += len(words)\n\n    if current:\n        chunks.append(\" \".join(current))\n\n    return chunks\n\nprint(f\"Found {len(txt_paths)} text files.\")\nprint()\n\nfor path in txt_paths:\n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        text = f.read()\n\n    chunks = chunk_text(text, TARGET_WORDS)\n\n    for c in chunks:\n        tokens = simple_preprocess(c, deacc=True)\n        n_tokens = len(tokens)\n\n        # Filter by length\n        if MIN_WORDS &lt;= n_tokens &lt;= MAX_WORDS:\n            all_chunks.append(c)\n            all_token_lists.append(tokens)\n\nprint(\"Total chunks kept (after filtering):\", len(all_chunks))\nAnd run it. At this point, you can see:\n\ntotal number of .txt files processed: 516\ntotal chunks kept (after filtering): 539057\n\n\n\n\nIn Week 07, we used Word2Vec as a way to represent meaning geometrically: words that occur in similar contexts end up close together in vector space.\nThis week, we will use Word2Vec for a very practical purpose: expand a seed term into a list of conceptually related terms (so we can build a weakly supervised training set).\nOur seed term will be:\n\nmerchant\n\nWe will then have Word2Vec look for terms that are in the same (geometric) neighborhood as merchant.\nOK, create a new file step5_train_word2vec.py and run it:\nfrom pathlib import Path\nimport nltk\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import Word2Vec\n\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\n\n\n# Load and preprocess all texts (same logic as Step 4)\n\n\nTEXT_DIR = Path(\"texts\")\ntxt_paths = sorted(TEXT_DIR.glob(\"*.txt\"))\n\nTARGET_WORDS = 120\nMIN_WORDS = 5\nMAX_WORDS = 200\n\ndef chunk_text(text, target_words=120):\n    sentences = nltk.sent_tokenize(text)\n\n    chunks = []\n    current = []\n    current_len = 0\n\n    for sent in sentences:\n        words = sent.split()\n        if not words:\n            continue\n\n        if current_len + len(words) &gt; target_words and current:\n            chunks.append(\" \".join(current))\n            current = []\n            current_len = 0\n\n        current.append(sent)\n        current_len += len(words)\n\n    if current:\n        chunks.append(\" \".join(current))\n\n    return chunks\n\ntoken_lists = []\n\nprint(f\"Found {len(txt_paths)} text files.\")\nprint(\"Building token lists...\")\n\nfor path in txt_paths:\n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        text = f.read()\n\n    chunks = chunk_text(text, TARGET_WORDS)\n\n    for c in chunks:\n        tokens = simple_preprocess(c, deacc=True)\n        if MIN_WORDS &lt;= len(tokens) &lt;= MAX_WORDS:\n            token_lists.append(tokens)\n\nprint(\"\\nTotal tokenized chunks kept:\", len(token_lists))\n\n################\n# Train Word2Vec (same logic as what we did in week 07)\n################\n\nprint(\"\\nTraining Word2Vec...\")\n\nmodel = Word2Vec(\n    sentences=token_lists,\n    vector_size=200,   # dimensionality of word vectors\n    window=5,          # context window size\n    min_count=5,       # ignore very rare words\n    workers=4,         # adjust depending on your machine (see Week 07)\n    sg=1               # 1 = skip-gram; 0 = CBOW\n)\n\n# Save model\n\nPath(\"models\").mkdir(exist_ok=True)\nmodel_path = Path(\"models\") / \"w2v_full.bin\"\nmodel.save(str(model_path))\n\nprint(\"\\nModel saved to:\", model_path)\nThis will take a little while (or more than a little while, depending on your machine). Once it’s done, it will tell you: Model saved to: models\\w2v_full.bin.\n\n\nNow, we are going to load the model and ask for neighbors of our seed concept (that is, what words are near “merchant” in the vector space?).\nSo, create another file, step5b_query_word2vec.py, and run it:\nfrom pathlib import Path\nfrom gensim.models import Word2Vec\n\nmodel_path = Path(\"models\") / \"w2v_full.bin\"\nmodel = Word2Vec.load(str(model_path))\n\nseed = \"merchant\"\n\nif seed not in model.wv:\n    print(f\"'{seed}' not found in the model vocabulary.\")\n    print(\"This usually means min_count is too high or the corpus is too small.\")\nelse:\n    print(f\"Top 30 words similar to '{seed}':\")\n    for word, score in model.wv.similar_by_word(seed, topn=30):\n        print(f\"  {word:20s} {score:.3f}\")\nMy output is:\nTop 30 words similar to 'merchant':\nmarchant 0.736\nfactor 0.702\nmerchants 0.689\njeweller 0.667\ncustomer 0.665\npurser 0.648\ntailor 0.639\nwholesale 0.637\nclothier 0.635\nworshipful 0.632\nvintner 0.629\nclothyer 0.620\nadventurer 0.620\ntrade 0.620\nbrewer 0.619\nventurer 0.617\nchapman 0.616\nfactory 0.610\nseller 0.610\nstaple 0.606\ntradesman 0.606\nadventurers 0.606\nsailor 0.604\nhorner 0.603\nstaplers 0.599\neasterling 0.599\nbanker 0.598\nmarchants 0.595\ngoldsmith 0.594\napprentice 0.592\nWhat did you get? Anything surprising or simply unclear? The list here makes a lot of sense to me. A historical note: “Easterling merchants” would have been merchants from the Baltic regions, so that works in this list. What we have created is a corpus-specific semantic map for the concept of “merchant.”\nWe will now use this list as a resource for weak supervision in the next steps in our classification task.\n\n\n\n\nWord2Vec gives us a list of words “near” our seed term (merchant). But we do not want to blindly treat Word2Vec neighbors as truth. Instead, we use them as a suggestions and then impose an interpretive structure.\nA useful strategy is to split candidate terms into tiers:\n\nTier A (high confidence): direct hits and spelling variants\ne.g., merchant, marchant, merchants, marchants\nTier B (strongly related roles): terms that usually indicate commercial activity\ne.g., factor, chapman, adventurer, venturer, staple\nTier C (maybe / adjacent): occupational neighborhood terms that might appear in commercial contexts but are often broader\ne.g., tailor, clothier, haberdasher\n\nThis tiering is a modeling decision:\n\nTier A+B will become our CORE triggers (high precision).\nTier C will become a MAYBE set (to inspect separately).\n\nCreate a new file step6_define_tiers.py and run it:\n# Tier A: direct spellings / variants of the seed concept\nTIER_A = {\n    \"merchant\", \"merchants\",\n    \"marchant\", \"marchants\"\n}\n\n# Tier B: closely related commercial roles / terms\nTIER_B = {\n    \"factor\", \"chapman\",\n    \"adventurer\", \"adventurers\",\n    \"venturer\", \"venturers\",\n    \"staple\", \"staplers\",\n    \"trade\",\n    \"purser\"\n}\n\n# Tier C: \"maybe\" occupational neighborhood (often adjacent, not always merchant-specific)\nTIER_C = {\n    \"clothier\", \"clothyer\",\n    \"tailor\", \"tayler\",\n    \"haberdasher\",\n    \"goldsmith\",\n    \"vintner\",\n    \"brewer\",\n    \"banker\",\n    \"grazier\",\n    \"jeweller\"\n}\n\nprint(\"Tier A size:\", len(TIER_A))\nprint(\"Tier B size:\", len(TIER_B))\nprint(\"Tier C size:\", len(TIER_C))\n\nprint(\"\\nTier A:\", sorted(TIER_A))\nprint(\"\\nTier B:\", sorted(TIER_B))\nprint(\"\\nTier C:\", sorted(TIER_C))\nLet’s walk through what each tier represents and why I set them up this way:\n\nTier A: terms are direct references to the concept we care about (spelling variants common in early modern texts);\nTier B: these terms describe occupations and roles that often involve trade; institutional or economic positions adjacent to merchants; vocabulary that appears frequently in commercial contexts. But they are not perfect synonyms for “merchant.”\nTier C: these terms are occupational nouns that are often related to production, craft, or commerce, and they frequently appear near merchant discourse. But they are much more ambiguous.\n\nThis is where interpretive judgment enters the pipeline:\n\nWe decide what our category means.\nWe decide which words count as strong evidence vs. weak evidence.\nWe acknowledge that “merchant discourse” is fuzzy at the edges.\n\n\n\n\nWe now have:\n\na large set of sentence-based chunks,\ntokenized into word lists,\nand a tiered list of keywords (A / B / C).\n\nThe next step is to assign labels to chunks. This is where we can translate our decision making about tiers into rule-based labeling of the chunks. This is called weak supervision.\n\n\nFor each chunk:\n\nCORE (label = 1) → contains any Tier A or Tier B word\n(high confidence merchant discourse)\nMAYBE (label = 2) → contains Tier C words only\n(ambiguous / adjacent cases)\nNEG (label = 0) → contains none of the above\n\nThese rules encode our interpretive decisions from Step 6. OK, now we can get started with the actual labelling.\n\n\n\nCreate a new file: step7_label_chunks.py and run it.\nfrom pathlib import Path\nimport nltk\nfrom gensim.utils import simple_preprocess\nimport json\n\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\n\n#\n# Tier definitions (from Step 6)\n\n\nTIER_A = {\n    \"merchant\", \"merchants\",\n    \"marchant\", \"marchants\"\n}\n\nTIER_B = {\n    \"factor\", \"chapman\",\n    \"adventurer\", \"adventurers\",\n    \"venturer\", \"venturers\",\n    \"staple\", \"staplers\",\n    \"trade\", \"purser\"\n}\n\nTIER_C = {\n    \"clothier\", \"clothyer\",\n    \"tailor\", \"tayler\",\n    \"haberdasher\",\n    \"goldsmith\",\n    \"vintner\",\n    \"brewer\",\n    \"banker\",\n    \"grazier\",\n    \"jeweller\"\n}\n\n\n# Load and process texts\n\n\nTEXT_DIR = Path(\"texts\")\ntxt_paths = sorted(TEXT_DIR.glob(\"*.txt\"))\n\nTARGET_WORDS = 120\nMIN_WORDS = 5\nMAX_WORDS = 200\n\ndef chunk_text(text, target_words=120):\n    sentences = nltk.sent_tokenize(text)\n\n    chunks = []\n    current = []\n    current_len = 0\n\n    for sent in sentences:\n        words = sent.split()\n        if not words:\n            continue\n\n        if current_len + len(words) &gt; target_words and current:\n            chunks.append(\" \".join(current))\n            current = []\n            current_len = 0\n\n        current.append(sent)\n        current_len += len(words)\n\n    if current:\n        chunks.append(\" \".join(current))\n\n    return chunks\n\nlabeled = []\n\nprint(f\"Processing {len(txt_paths)} files...\")\n\nfor path in txt_paths:\n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        text = f.read()\n\n    chunks = chunk_text(text, TARGET_WORDS)\n\n    for c in chunks:\n        tokens = simple_preprocess(c, deacc=True)\n        n_tokens = len(tokens)\n\n        if not (MIN_WORDS &lt;= n_tokens &lt;= MAX_WORDS):\n            continue\n\n        token_set = set(tokens)\n\n        if token_set & (TIER_A | TIER_B):\n            label = 1   # CORE\n        elif token_set & TIER_C:\n            label = 2   # MAYBE\n        else:\n            label = 0   # NEG\n\n        labeled.append((c, label))\n\nprint(\"Total chunks labeled:\", len(labeled))\n\n\n# Save labeled data\n\n\nPath(\"data\").mkdir(exist_ok=True)\n\nwith open(Path(\"data\") / \"merchant_labeled_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(labeled, f, ensure_ascii=False)\n\nprint(\"Saved labeled chunks to data/merchant_labeled_chunks.json\")\nWhat we did so far: we applied rules we designed, using the lexical triggers we chose, to create a labeled dataset. Obviously, these labels are imperfect and corpus-specific, but they are good enough to train a first classifier.\nAt this point, we have turned unstructured text into a supervised learning dataset. All the steps that we are doing next (train/test splits, classifiers, confusion matrices) depend on the choices made up to here. We can’t quite train a classifier, yet. First, we need to separate CORE, MAYBE, and NEG examples, inspect them, and think carefully about evaluation.\nThis is where questions of precision and recall enter the picture.\n\n\n\n\nAt the end of step7_label_chunks.py add the following code and run it.\nprint(\"\\nLabel distribution:\")\nprint(f\"  CORE (1): {sum(1 for _, y in labeled if y == 1)}\")\nprint(f\"  NEG  (0): {sum(1 for _, y in labeled if y == 0)}\")\nprint(f\"  MAYBE(2): {sum(1 for _, y in labeled if y == 2)}\")\n\n# Sample one from each category\nfor label_name, label_val in [(\"CORE\", 1), (\"MAYBE\", 2), (\"NEG\", 0)]:\n    example = next((text for text, y in labeled if y == label_val), None)\n    if example:\n        print(f\"\\n{label_name} example (first 200 chars):\")\n        print(example[:200])\nThis code will count how many chunks we got for each label and show examples of each type. When you get the results:\n\nDoes the CORE example actually contain merchant language?\n\n\n\nDoes the MAYBE example feel ambiguous?\n\n\n\nDoes the NEG example clearly not relate to merchants?\n\n\n\n\nNow we have three kinds of labeled chunks of text:\n\nCORE (label = 1): High-confidence merchant discourse\n\nMAYBE (label = 2): Ambiguous, adjacent, or borderline cases\n\nNEG (label = 0): Clearly not merchant discourse\n\nBefore we actually train a classifier, we must decide which of these labels should the model actually learn from. For this week, we will:\n\nTrain the classifier only on CORE vs. NEG\nExclude MAYBE from training\nSave MAYBE for later inspection and interpretation\n\nWhy? The honest answer is: I tried a couple of options and compared with my own readings of the texts, this gave the best results at this stage. But I also want to note the MAYBE category contains texts that are close to commerce but not clearly merchant discourse and so require a lot of contextual interpretation. They are also texts where other scholars may disagree with my reading. This means that including them for this training, would reduce interpretability and make evaluation harder to explain.\nInstead, we treat the MAYBE texts as a set of texts to which we apply the trained classifier later. This mirrors how supervised models are often used in practice: - trained on clear cases, - applied to ambiguous ones.\n\n\nCreate a new file step9_prepare_datasets.py . Before you run it, read the explanation about the NEG step after the code sample. Then run it.\nimport json\nfrom pathlib import Path\nimport random\n\nrandom.seed(42) # For reproducibility--same random sample each time [see (*) below]\n\nDATA_PATH = Path(\"data\") / \"merchant_labeled_chunks.json\"\n\nwith open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n    labeled = json.load(f)\n\ncore = [(t, 1) for (t, y) in labeled if y == 1]\nneg  = [(t, 0) for (t, y) in labeled if y == 0]\nmaybe = [t for (t, y) in labeled if y == 2]\n\nprint(\"Loaded:\")\nprint(\"  CORE:\", len(core))\nprint(\"  NEG :\", len(neg))\nprint(\"  MAYBE:\", len(maybe))\n\n###### =&gt; NEG step [see explanation below]\nneg_sample = random.sample(neg, len(core))\n\ntraining_data = core + neg_sample\nrandom.shuffle(training_data)\n\nprint(\"Training set size (CORE + NEG):\", len(training_data))\n\n### Split the data into train and test sets:\n\nsplit = int(0.8 * len(training_data))\ntrain_data = training_data[:split]\ntest_data  = training_data[split:]\n\nprint(\"Train size:\", len(train_data))\nprint(\"Test size :\", len(test_data))\nprint(\"MAYBE size:\", len(maybe))\n\n# Save the datasets:\n\nPath(\"data\").mkdir(exist_ok=True)\n\nwith open(Path(\"data\") / \"train_core_vs_neg.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(train_data, f, ensure_ascii=False)\n\nwith open(Path(\"data\") / \"test_core_vs_neg.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(test_data, f, ensure_ascii=False)\n\nwith open(Path(\"data\") / \"maybe_texts.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(maybe, f, ensure_ascii=False)\n\nprint(\"Saved training, test, and MAYBE datasets.\")\nWith this step in the code we are downsampling NEG to match the size of the CORE set. Why? [Think for a second about what most of the texts might look like…] Well, in most corpora, NEG examples vastly outnumber CORE examples: most of the text is not likely to be talking about the concept/topic you are targeting. Now, the texts that we have for class are a little skewed towards “merchant” and, as you will see after you run the code, we would still have an overwhelming number of NEG chunks, but I want you to learn best practices in general!\nTraining directly on all NEG data can cause the model to:\n\nalways predict “not merchant”\nachieve high accuracy but low usefulness\n\nOK, this is what I got back from this step:\nLoaded:\nCORE: 11373\nNEG : 527174 [!!!]\nMAYBE: 510\nTraining set size (CORE + NEG): 22746 [Important: can you see where this is coming from based on the above explanation? And, can you see why we needed to downsample?]\nTrain size: 18196\nTest size : 4550\nMAYBE size: 510\nOne more point (*): we use Python’s built-in pseudo-random number generator to, first, random.sample() from the full NEG list when we are downsampling the NEG to the same size as CORE [note that we are asking it to select as many NEG as len(core)]. Second, we use random.shuffle() to randomly reorder the combined CORE+NEG list because otherwise when we do our 80/20, all the CORE examples would be first and all the NEG examples would be after (not what we want!).\n\n\n\n\nNow we are ready to create an actual classifier. So far, our training data looks like this:\n\na chunk of text (a string),\npaired with a label (CORE = 1 or NEG = 0).\n\nWe can read these strings directly (and have done so to check things along the way), but obviously computers cannot. A classifier cannot operate on words, sentences, or meanings. It can only operate on numbers. So before we train a model, we must transform text into a numerical representation as we have been doing in previous weeks. In particular, we will revisit our old friend TF-IDF (remember our work in week 4).\nOur next steps:\n\nConvert text chunks into TF-IDF vectors. This helps emphasize distinctive vocabulary rather than common function words.\nTrain a simple binary classifier\nEvaluate its performance on held-out data\nApply it to the MAYBE set (review above if needed)\nEvaluate on what the model learned and what it did not, and see if it is useful for historical interpretation.\n\nAs with week 8, we are going to build everything step by step. Create a new file, step10_tfidf_representation.py. Don’t run it yet, I will give explanations below and I want you to read and understand them before running the code.\nimport json\nfrom pathlib import Path\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nDATA_DIR = Path(\"data\")\n\n#We now load the datasets we prepared at the end of last week (step 9)\nwith open(DATA_DIR / \"train_core_vs_neg.json\", \"r\", encoding=\"utf-8\") as f:\n    train_data = json.load(f)\n\nwith open(DATA_DIR / \"test_core_vs_neg.json\", \"r\", encoding=\"utf-8\") as f:\n    test_data = json.load(f)\n\n# Separate texts and labels\nX_train_texts = [t for (t, y) in train_data]\ny_train = [y for (t, y) in train_data]\n\nX_test_texts = [t for (t, y) in test_data]\ny_test = [y for (t, y) in test_data]\n\nprint(\"Train size:\", len(X_train_texts))\nprint(\"Test size :\", len(X_test_texts))\n\n##### ==&gt; See explanation [A] below\n\nvectorizer = TfidfVectorizer(\n    lowercase=True,\n    min_df=5,        # ignore very rare words\n    max_df=0.9       # ignore extremely common words; Explanation [B]\n)\nX_train = vectorizer.fit_transform(X_train_texts)\nX_test = vectorizer.transform(X_test_texts)\n\nprint(\"TF-IDF matrix shapes:\")\nprint(\"  Train:\", X_train.shape)\nprint(\"  Test :\", X_test.shape)\n\n#####  ==&gt; See explanation [C] below\n\nExplanation [A]: after the lines above, we still have raw text, so we have to define a TF-IDF vectorizer.\nExplanation [B]: we set the parameters as min_df=5 so that we remove words that appear in fewer than 5 documents; and max_df=0.9 so that we remove words that appear in more than 90% of the documents.\nExplanation [C]: we fit the vectorizer on the training data only, then apply it to the test data. Each text chunk is now a numeric vector.\n\nWe have:\n\n\n\nObject\nWhat it contains\nWhat it’s used for\n\n\n\n\nX_train\nTF-IDF vectors\nModel learns patterns\n\n\ny_train\nLabels\nSupervision signal\n\n\nX_test\nTF-IDF vectors\nModel makes predictions\n\n\ny_test\nLabels\nWe check how good predictions are\n\n\n\nI am going to belabor this more so that it’s fully clear, you can think of the X, Y part of this as:\nX = the input to the model → the text, represented numerically (TF-IDF vectors)\ny = the output we want the model to predict → the labels (CORE = 1, NEG = 0)\nSo:\nX_train = text chunks (as numbers)\ny_train = labels for those chunks\nWhere train = data the model is allowed to learn from; test = data the model has never seen before.\nWhen you run step 10, your output will look like:\nTrain size: 36874\nTest size : 9218\nTF-IDF matrix shapes:\nTrain: (36874, 27806)\nTest : (9218, 27806)\nThis is reassuring as it is what we want: we have the 80/20 split we created in step 9: 36,874 chunks were used to train the model; 9,218 chunks were held out for evaluation. So far so good.\nIn addition, the TF-IDF matrix has the shape (number of documents) by (number of features), that is (36874, 27806) translates to a matrix that has:\n\n36874 rows, where each row is for a training chunk. Each row is a TF-IDF vector representing a chunk.\n27,806 columns, where each column is for a word feature. Each column corresponds to a specific word in the vocabulary. Yes, we have 27,806 features! That’s not a problem: remember, TF-IDF matrices are sparse, linear models do well with this kind of setting (see the info below about the choice of logistic regression), and this is normal for text data.\n\n\n\nLater, when we compute a confusion matrix:\n\npredictions come from X_test\n“true labels” come from y_test\n\nThe confusion matrix will help us understand how often the model’s predictions on unseen data match the labels produced by our weak supervision rules. I will remind you below, but you want to start keeping in mind that the confusion matrix does not measure truth, it just measures agreement with a labeling scheme.\n\n\n\nNow we can finally train a classifier using a logistic regression model. We discussed how a logistic regression works in class (or will do so in Week 9, if you are reading ahead). If you want an advanced version of this, see chapter 4 in Jurafsky and Martin.\n\nNote: we are not going to go into the details covered in section 4.4 forward. For the simple reason that it would require more mathematical background than I assume for the class to discuss the loss function and gradients. (We will touch on the materials from section 4.9 on how to build a confusion matrix.)\n\nThere are many classifiers we could use. We choose logistic regression for three main reasons:\n\nIt is simple and well understood. It works well with a binary task such as ours: CORE (1) vs NEG (0). It models:\n\nLogistic regression is literally designed to model:\n\\[\nP(y =1 \\mid x)\n\\]\nSo it gives you: a probability of CORE for each chunk (via predict_proba ) and a clean decision rule: “predict CORE if probability ≥ 0.5”.\n\nIt works very well with TF-IDF features. TF-IDF produces very high-dimensional vectors (tens of thousands to 100k+ features) which are mostly zeros (sparse). Linear models such as logistic regression work well for these situations (they are fast to train and are stable).\nNote: the default threshold in scikit-learn is 0.5. As we saw in class, logistic regression models the log-odds of class membership and a probability of 0.5 corresponds to equal odds. This is reasonable when classes are roughly balanced in the training setup (recall that we decided to downsample the negatives above). We could change the threshold if we had good reasons to do so. I will bring this point up again at the end.\n\nThere are other options (linear support vector machine or naive Bayes), but we choose logistic regression because it is a strong, interpretable baseline for binary text classification with TF-IDF features. TF-IDF creates high-dimensional sparse vectors, and linear models are known to perform very well in this setting. Logistic regression also outputs probabilities, which lets us interpret model confidence and apply the model to ambiguous cases (our MAYBE set).\nCreate a new file, step11_train_classifier.py. We will build the script in chunks with explanations at each step and you will run it once it’s all put together.\nfrom pathlib import Path\nimport json\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    confusion_matrix,\n    classification_report,\n    roc_auc_score\n)\nNext, we load the data and use the TF-IDF representations we created in step 10 above.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nDATA_DIR = Path(\"data\")\n\nwith open(DATA_DIR / \"train_core_vs_neg.json\", \"r\", encoding=\"utf-8\") as f:\n    train_data = json.load(f)\n\nwith open(DATA_DIR / \"test_core_vs_neg.json\", \"r\", encoding=\"utf-8\") as f:\n    test_data = json.load(f)\n\nX_train_texts = [t for (t, y) in train_data]\ny_train = [y for (t, y) in train_data]\n\nX_test_texts = [t for (t, y) in test_data]\ny_test = [y for (t, y) in test_data]\n\nvectorizer = TfidfVectorizer(\n    lowercase=True,\n    min_df=5,\n    max_df=0.9\n)\n\nX_train = vectorizer.fit_transform(X_train_texts)\nX_test = vectorizer.transform(X_test_texts)\nNow we actually train the classifier (yes, I know, finally!). What will happen in this section of the script: the model examines the TF-IDF features and it learns the weights that separate CORE from NEG. We then make predictions on the test set.\nclf = LogisticRegression(\n    max_iter=1000,\n    n_jobs=1\n)\n\nclf.fit(X_train, y_train)\n\n#test set predictions\ny_pred = clf.predict(X_test)\ny_prob = clf.predict_proba(X_test)[:, 1]\nAt this point we evaluate our classifier with a confusion matrix that compares:\n\ntrue labels (y_test)\nmodel predictions (y_pred)\n\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion matrix:\")\nprint(cm)\nI will go over what this does in more detail below (step 12). But the next step is to include two more evaluation components. First, a classification report that summarizes how reliable positive predictions are (= precision); how many positives the model found (= recall); and the balance of precision and recall (F1-score). Second, ROC AUC, which measures how well the model separates CORE from NEG across all thresholds.\nAdd the next bit of code to your script and then run everything in step11_train_classifier.py.\nprint(\"\\nClassification report:\")\nprint(classification_report(y_test, y_pred))\n\n###ROC AUC\nauc = roc_auc_score(y_test, y_prob)\nprint(\"ROC AUC:\", round(auc, 3))\n\n\n\nNow that we have trained the classifier, computed the confusion matrix and ROC AUC, we can evaluate how closely do the model’s predictions agree with the labels in our test set, given a particular decision rule.\nLet’s start with the confusion matrix. When you ran step 11, you should have gotten something like this:\n[[6943 27]\n[ 367 1881]]\nWhat does this mean? I am going to present it as a table to make its entries legible:\n\n\n\n\nPredicted NEG (0)\nPredicted CORE (1)\n\n\n\n\nActual NEG (0)\n6943\n27\n\n\nActual CORE (1)\n367\n1881\n\n\n\nEach cell tells us something different about model behavior. By convention in scikit-learn:\n\nRows = actual (true) labels\nColumns = predicted labels\n\nSo we can rewrite it structurally as:\n\\[\\begin{bmatrix} \\text{TN} & \\text{FP} \\\\ \\text{FN} & \\text{TP} \\end{bmatrix}\\]\nTo make this clear:\n\nTrue negatives (TN) = 6943 → 6943 texts were labeled NEG in the test set, and the model correctly predicted NEG for them.\nFalse Positive (FP) = 27 → 27 texts were labeled NEG in the test set, and the model incorrectly predicted CORE for them.\nFalse Negatives (FN) = 367 → 367 texts were labeled CORE in the test set, and the model incorrectly predicted NEG for them.\nTrue Positives (TP) = 1881 → 1881 texts were labeled CORE in the test set, and the model correctly predicted CORE for them.\n\nInterpretation: our decision threshold in the logistic regression was 0.5. That is, if the predicted probability ≥ 0.5, label the text CORE; if the predicted probability &lt; 0.5, label the text NEG. The model is very good at recognizing texts that do not match our merchant criteria. It’s fairly good at avoiding predicting as positive texts that are labelled negative (27 false positives), but we do lose a fair amount of CORE labeled texts (367).\nThe confusion matrix is not the only validation tool we have. Let’s look at the classification report. You should have gotten something like this:\n\n\n\n\n\n\nClass\nPrecision\nRecall\nF1-Score\nSupport\n\n\n\n\n0\n0.95\n1.00\n0.97\n6,970\n\n\n1\n0.99\n0.84\n0.91\n2,248\n\n\n\nWhat do these numbers mean?\nFor CORE (class 1):\nPrecision (1) = 0.99\nFormula: \\(\\text{Precision} = \\frac{TP}{TP+FP}\\)\nIn our case, this tells us that when the model predicts CORE, it is correct 99% of the time. This matches your tiny number of false positives (27).\nRecall (1) = 084\nThe formula: \\(\\text{Recall} = \\frac{TP}{TP + FN}\\)\nThe model finds 84% of the CORE texts and it misses 16% (those 367 false negatives).\nF1-score (1) = 0.91\nF1 balances precision and recall.\nThe formula: \\(F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\)\n0.91 is a strong F1 score, but lower than precision because recall is lower.\nI will leave it up to you to think through the values for the NEG class.\nAs part of our report, we also had the following values:\n\n\n\nMetric\nPrecision\nRecall\nF1-Score\nSupport (total)\n\n\n\n\nAccuracy\n\n\n0.96*\n9,218\n\n\nMacro avg\n0.97\n0.92\n0.94\n9,218\n\n\nWeighted avg\n0.96\n0.96\n0.96\n9,218\n\n\n\n(*) I know, I know, this is weird: the placement of accuracy in the F1-score column is just a formatting convention from sklearn’s classification report. The 0.96 value had to go somewhere, and they chose that column.\nSo, what is actually going on besides formatting weirdness:\nAccuracy = 0.96\nFormula: \\(\\frac{TP + TN}{\\text{Total}}\\)\nInterpretation: the model agrees with the test labels 96% of the time. In general accuracy alone is not very informative in imbalanced tasks, which is why we look at precision and recall. But recall that we downsampled NEG to match CORE, so we had a balanced training set. Nonetheless, precision and recall are still very useful in understanding the kinds of errors the model is making.\n\n\n\nHere we take a simple average of class metrics to treat both classes equally:\n\\[\n\\frac{Metric_0 + Metric_1}{2}\n\\]\n\n\n\nWeights each class by its support (number of examples). Since NEG is larger (6970 vs 2248), the weights workout to:\nweight_neg = \\(\\frac{6970}{9218}\\) or about 0.756\nweight_core = \\(\\frac{2248}{9218}\\) or about 0.244\nThe weighted average multiplies each class’s score by the proportion of examples in that class. Since NEG makes up about 76% of the test set and performs very well, the weighted average is pulled toward the NEG score and ends up close to overall accuracy.\nFor example, the weighted F1:\nWeighted F1 is:\n\\((w_0 \\times F1_0) + (w_1 \\times F1_1)\\)\nSubstitute values:\n\\((0.756 \\times 0.97) + (0.244 \\times 0.91)\\)\nCompute each term:\n\\(0.756 \\times 0.97 ≈ 0.733\\)\n\\(0.244 \\times 0.91 ≈ 0.222\\)\nAdd them and round \\(≈0.96\\)\nThe weighted F1-score is close to overall accuracy because the model performs strongly on both classes and the larger class (NEG) dominates the weighting.\n\n\n\nNow the most important conceptual point in evaluating our model: ROC AUC does not use the 0.5 threshold. It evaluates how well the model separates the two classes across all possible thresholds.\nHow does ROC AUC work? Instead of using one specific cutoff, it changes the threshold from 0 to 1 and tests how well the model separates CORE from NEG at each threshold. Then it evaluates performance across all those possibilities.\nSplitting this up into its components:\nROC stands for Receiver Operating Characteristic.\nIt plots:\n\nTPR: True Positive Rate (same as Recall) on the y-axis\nFPR: False Positive Rate on the x-axis\n\nAs the threshold moves from 0 to 1, the curve traces out the model’s performance. If the model:\n\nperfectly separates classes → curve hugs the top-left corner (high TPR, low FPR at all thresholds)\nguesses randomly → curve follows a diagonal line (50/50 at every threshold)\n\nAUC stands for Area Under the Curve. It compresses all this information into a single number (for us, 0.996). AUC is literally the area under the ROC curve. It’s the integral of the True Positive Rate with respect to the False Positive Rate as we vary the threshold. This is why AUC ranges from 0.5 (diagonal line, area of triangle = 0.5) to 1.0 (perfect corner, full square = 1.0).\nMath aside: you don’t need to worry about computing this ourself, but here’s the integral:\n\\[\\text{AUC} = \\int_0^1 \\text{TPR}(t) \\, d(\\text{FPR}(t))\\]\nYou can think of it this way: if I randomly choose one CORE chunk and one NEG chunk, the AUC tells you the probability that the model assigns a higher probability to the CORE chunk.\nJust a reminder: you can see a full explanation here.\nWe have a value of 0.996, which means that the model almost perfectly ranks CORE texts above NEG texts. This is good! Unlike accuracy or F1-score which depend on choosing a threshold, ROC AUC tells us whether our weak supervision approach is fundamentally learning the right pattern. A high AUC (like 0.996) means our lexicon-based labels are capturing something real about “merchant” discourse.\n\n\n\nRemember that I mentioned that we could change the threshold for the classification task if we had a good reason to do so. We now want to go back and think through this issue.\nFirst of all, you might want to ask: Does high ROC-AUC mean that our 0.5 threshold performed well? We want to be careful with this question:\nOur high ROC-AUC (0.996) tells us the model is excellent at ranking texts. It consistently assigns higher probabilities to CORE than NEG. This means that if you sorted all chunks by their probability scores, true CORE chunks would appear near the top and true NEG chunks near the bottom 99.6% of the time, regardless of where you draw the threshold line.\n\nTrue CORE chunks get higher probability scores than true NEG chunks (ranking). Whether they cross the 0.5 threshold is a separate question (classification)\n\nAs an example, you could have:\nCORE chunk with prob = 0.48 (ranked above NEG chunks, but classified as NEG at 0.5) NEG chunk with prob = 0.02 (ranked below CORE chunks, classified correctly as NEG)\nThe model correctly ranked them (CORE &gt; NEG), which gives high AUC, even though the CORE chunk was misclassified at the 0.5 threshold.\nThis is important because we do not want to misread a high AUC as indicating that we picked (or were given by default) the optimal threshold for our specific research goals. As I mentioned earlier, the 0.5 threshold is a reasonable default when:\n\nClasses are balanced (we downsampled)\nFalse positives and false negatives are equally costly (In general, this depends on your research question)\n\nNow, we might want to explore other thresholds. Then we could pick one of the following approaches:\n\nPlot precision and recall at different thresholds\nOptimize for F1-score\nMake a domain-driven decision based on your research needs (if you ask me, this is always a question you should have in mind, but, at the end of the day, I am a humanist..)\n\nI am going to give you a couple of scenarios where this might be reasonable to do:\n\nI sometimes need a discovery tool to find texts (that talk about “merchant” concepts) that I might have missed. I would then lower the threshold (0.3-0.4) for high recall\nOn the other hand, if I am selecting passages for close reading and want only clear examples of the discourse, then I would pick a higher threshold (0.6-0.7) for high precision\nIf you want a balanced approach for further analysis (our case) → 0.5 is reasonable\n\nFor our purposes, we’ll stick with 0.5 since our evaluation metrics (precision, recall, F1) all look strong at this threshold, and it aligns with our balanced training set.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#big-picture-workflow",
    "href": "week-08-09-word2vec.html#big-picture-workflow",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "Here is the full pipeline we will build:\n\nLoad texts from a folder (texts/): you will need to get the 500+ files that I uploaded on Canvas under “Train_Text_Documents.”\nSegment each text into sentence-based chunks (our “documents” for classification)\nUse Word2Vec (trained on our corpus) to expand a seed concept (seed: merchant)\nBuild a search-word list (Tier A/B/C: more details below)\nUse the search-word list to create weak labels\nCreate a CORE vs NEG dataset (and optionally a MAYBE set)\nSplit into train and test\nTrain a classifier (TF–IDF + logistic regression)\nEvaluate and interpret\n\nWe will move slowly and validate outputs at each stage. Again, I am keeping the training wheels on, but",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#what-you-need-before-starting",
    "href": "week-08-09-word2vec.html#what-you-need-before-starting",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "VSCode installed\nA project folder like: IDS_570_TAD/\nA subfolder: texts/ containing your .txt files\nA Python virtual environment (.venv) activated in VSCode\nPackages we will use:\n\nnltk\ngensim\nscikit-learn\ntqdm\n\n\n(We will install these together in the steps below.)",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#step-0-project-setup-environment",
    "href": "week-08-09-word2vec.html#step-0-project-setup-environment",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "Before we do any modeling, we need to make sure our project is set up correctly. This step is mainly for those of you new to Python, but it also will help everyone learn good project organization skills: many errors in text analysis come from working in the wrong folder or environment.\nWhere:\n\ntexts/ contains the raw .txt files you want to analyze.\ndata/ will store intermediate outputs (JSON files, inspection samples).\nmodels/ will store trained models (e.g., Word2Vec, classifiers).\nEach stepX_*.py file does one conceptual task.\n\nWe intentionally separate steps into different scripts so that: - each step is easy to test and debug - you can rerun part of the pipeline without rerunning everything - you can clearly see how the pipeline is constructed.\n\n\nMake sure you open VSCode in the project folder (IDS_570_TAD).\nThen open the terminal: there are a ton of videos online, if you have never done it before. If you are using something other than VSCode, you can also find guides online.\nYou should see something like:\n```bash PS D:…_570_TAD&gt;\nIf you see a different folder, you are in the wrong place. This is important because we are going to set up a Python virtual environment to keep project-specific packages isolated.\nNext, create the environment and activate it:\npython -m venv .venv\nthen, in Windows:\n.venv\\Scripts\\Activate.ps1\nNote, if this fails try:\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\nThen try activation again.\nOR in Mac/Linux\nsource .venv/bin/activate\nFor full instructions, see here. Your terminal prompt should now include .venv.\n\n\n\nInstall the libraries we will use:\npip install nltk gensim scikit-learn tqdm\nWe will download one additional resource (NLTK sentence tokenizer) later.\nBefore moving on, check:\n\nCan you see your .txt files from Python?\nDoes python step1_load_texts.py run without errors?\nIs your terminal in the correct folder?\n\nIf something is off here, it is worth fixing before you continue: go back through the steps above and see if something is missing. There are also a lot of videos online that help you go through this process.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#step-1-loading-inspecting-and-segmenting-texts",
    "href": "week-08-09-word2vec.html#step-1-loading-inspecting-and-segmenting-texts",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "Before we segment, vectorize, or classify anything, we need to confirm that Python can see and read our texts correctly.\n\n\nWe will assume all .txt files are stored in a folder called texts/. Now, create a new file and name it: step1_load_texts.py and then run it:\nfrom pathlib import Path\n\nTEXT_DIR = Path(\"texts\")\n\n# Collect all .txt files\nfiles = sorted(TEXT_DIR.glob(\"*.txt\"))\n\nprint(f\"Found {len(files)} .txt files.\")\n\n# Print the first few filenames\nprint(\"First 10 files:\")\nfor f in files[:10]:\n    print(\" \", f)\nThis should tell you that you found about 516 .txt files. Note: you have a very slightly different training set; you are responsible for keeping track of how this affects things down the line (though the first few steps should match). It’s my way of keeping you on your toes!\nYou should see a list of filenames printed to the terminal. Now we want to inspect one file to make sure that we have set things up correctly (note: in the future, I will assume that you will do this on your own!). To the same file as above, add the following and run the script again:\n# Read one example file\nexample_file = files[0]\n\nwith open(example_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n    text = f.read()\n\nprint(\"\\nReading file:\")\nprint(example_file)\nprint(\"-\" * 40)\nprint(\"Number of characters:\", len(text))\nprint(\"\\nFirst 1,000 characters:\\n\")\nprint(text[:1000])\nWe are just making sure that we can load the data and that things look correct (that is, we have funky looking Early Modern texts). In my case (and it should be in your too), the file read was: texts\\A00419.txt; number of characters: 2888252l and then you should get the first few lines of the text.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#step-2-segmenting-texts-into-chunks",
    "href": "week-08-09-word2vec.html#step-2-segmenting-texts-into-chunks",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "Our .txt files are far too large to classify as single units. A supervised classifier expects many medium-sized examples and some of the texts I gave you are very long (the one above had 2,888,252 characters!). So we need to decide what counts as a “document” for classification.\n\nFor this week, we will treat chunks of sentences as our unit of analysis. We’re using sentence chunks because we want to capture enough context for semantic meaning (a single sentence might be ambiguous), but not so much that we get incorrect labels (a 50-page chapter likely discusses many topics and might lead to mislabelling: such a large “chunk” may not be about “merchants” in any meaningful sense, but, for example, it could be a biblical passage mentioning a merchants metaphorically–we would need a more careful approach for these kinds of texts).\n\nNote: There is no universally correct choice here; segmentation is a modeling decision and I am basing the “chunk” size on experience.\nWe will begin by splitting each text into sentences using NLTK. I have two pieces of documentation for NLTK that I recommend, if you want to delve into it: here and here.\nCreate a new file: step2_segment_and_chunk.py . We will chunk the texts and check some basic diagnostics to make sure that the code is actually running properly.\nfrom pathlib import Path\nimport nltk\n\n# Download required tokenizers (run once)\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\n\nTEXT_DIR = Path(\"texts\")\ntxt_paths = sorted(TEXT_DIR.glob(\"*.txt\"))\n\nsample_path = txt_paths[0]\n\nwith open(sample_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n    text = f.read()\n\n# Split text into sentences\nsentences = nltk.sent_tokenize(text)\n\nprint(\"File:\", sample_path)\nprint(\"Number of sentences:\", len(sentences))\nprint()\n\n# Group sentences into chunks of ~120 words\nTARGET_WORDS = 120\n\nchunks = []\ncurrent = []\ncurrent_len = 0\n\nfor sent in sentences:\n    words = sent.split()\n    if not words:\n        continue\n\n    # If adding this sentence would exceed the target, finalize the chunk\n    if current_len + len(words) &gt; TARGET_WORDS and current:\n        chunks.append(\" \".join(current))\n        current = []\n        current_len = 0\n\n    current.append(sent)\n    current_len += len(words)\n\n# Add any leftover sentences\nif current:\n    chunks.append(\" \".join(current))\n\nprint(\"Number of chunks:\", len(chunks))\n\n# Diagnostics on chunk length (rough word counts)\nlengths = [len(c.split()) for c in chunks]\nlengths_sorted = sorted(lengths)\n\nprint()\nprint(\"Approx word counts per chunk:\")\nprint(\"  min:\", min(lengths))\nprint(\"  median:\", lengths_sorted[len(lengths_sorted)//2])\nprint(\"  max:\", max(lengths))\n\nlo, hi = 5, 200\nin_range = sum(lo &lt;= n &lt;= hi for n in lengths)\nprint(f\"Chunks with {lo}–{hi} words:\", in_range)\nprint(\"Share in range:\", round(in_range / len(lengths), 3))\n\nprint()\nprint(\"--- Chunk 1 preview (first 400 chars) ---\")\nprint(chunks[0][:400])\nAnd run the script.\nYou should see:\n\nThousands of chunks (5172)\nA median chunk length around 100–120 words (median: 101)\nMost chunks falling in the 5–200 word range (5009; with the share in range: 0.968)\nAnd a preview of the first chunk.\n\nThis tells us that our segmentation choice is reasonable for classification.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#step-3-tokenize-chunks-for-modelling",
    "href": "week-08-09-word2vec.html#step-3-tokenize-chunks-for-modelling",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "This step will be conceptually familiar from when we used unnest_tokens() in tidytext.\nwe transform a text column into word tokens.\nWe will do this using gensim.utils.simple_preprocess().\nCreate a new file: step3_tokenize_chunks.py. This new script is going to build on what we did in the “step2” script, s you will find the same logic repeated at the beginning.\nfrom pathlib import Path\nimport nltk\nfrom gensim.utils import simple_preprocess\n\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\n\nTEXT_DIR = Path(\"texts\")\ntxt_paths = sorted(TEXT_DIR.glob(\"*.txt\"))\nsample_path = txt_paths[0]\n\nwith open(sample_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n    text = f.read()\n\n# Step 2 logic: sentences -&gt; chunks (~120 words)\nsentences = nltk.sent_tokenize(text)\n\nTARGET_WORDS = 120\nchunks = []\ncurrent = []\ncurrent_len = 0\n\nfor sent in sentences:\n    words = sent.split()\n    if not words:\n        continue\n\n    if current_len + len(words) &gt; TARGET_WORDS and current:\n        chunks.append(\" \".join(current))\n        current = []\n        current_len = 0\n\n    current.append(sent)\n    current_len += len(words)\n\nif current:\n    chunks.append(\" \".join(current))\n\n# Step 3: now we tokenize each chunk\ntoken_lists = [simple_preprocess(c, deacc=True) for c in chunks]\n\nprint(\"File:\", sample_path)\nprint(\"Chunks (strings):\", len(chunks))\nprint(\"Chunks (token lists):\", len(token_lists))\n\nprint(\"\\n--- Token preview (first 60 tokens of first chunk) ---\")\nprint(token_lists[0][:60])\n\nprint(\"\\n--- Token count of first chunk ---\")\nprint(len(token_lists[0]))\nNow run it. What we are doing:\n\nsimple_preprocess() returns a list of tokens.\nTokens are lowercased and cleaned.\nThis is now in the format required by Word2Vec:\n\nWord2Vec expects sentences=[[\"token\",\"token\",...], ...]\n\n\nThe output should look like:\nChunks (strings): 5172\nChunks (token lists): 5172\n--- Token preview (first 60 tokens of first chunk) --- ['the', 'first', 'booke', 'of', 'the', 'covntrie', 'farme', 'chap', 'what', 'manner', 'of', 'husbandrie', 'is', 'entreated', 'of', 'in', 'the', 'discourse', 'following', 'even', 'as', 'the', 'manner', 'of', 'building', 'vsed', 'at', 'this', 'day', 'the', 'varietie', 'of', 'countries', 'causeth', 'diuers', 'manner', 'of', 'labouring', 'of', 'the', 'earth']\n--- Token count of first chunk --- 41\n\n\ngensim may look “stuck” the first time you import it. When you import gensim (especially on Windows), Python may appear to freeze for 10–60 seconds. This is normal and does not mean your code is broken.\n\ngensim loads compiled components used for Word2Vec\non Windows, this initial load can be slow\nPython does not print progress messages during this step If the terminal is not showing an error message, wait patiently before interrupting the process. Only stop the program if it has been unresponsive for several minutes.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#step-4-process-all-files-and-filter-chunks",
    "href": "week-08-09-word2vec.html#step-4-process-all-files-and-filter-chunks",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "So far, we have been working with a single example file to make sure that things are working. Now we are ready to apply the same segmentation and tokenization steps to the entire corpus. We are going to make the following sampling decisions:\n\ndiscard chunks that have fewer than 5 tokens (not enough context)\ndiscard chunks that have more than 200 tokens (too diffuse and hard to interpret)\n\nAgain, these thresholds are based on experience in working with Early Modern texts: they are reasonable defaults to start with and we could always change them if things don’t work out. The goal is to define a reasonable and usable training example.\nCreate a new file: step4_process_all_files.py.\nfrom pathlib import Path\nimport nltk\nfrom gensim.utils import simple_preprocess\n\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\n\nTEXT_DIR = Path(\"texts\")\ntxt_paths = sorted(TEXT_DIR.glob(\"*.txt\"))\n\nTARGET_WORDS = 120\nMIN_WORDS = 5\nMAX_WORDS = 200\n\nall_chunks = []\nall_token_lists = []\n\ndef chunk_text(text, target_words=120):\n    sentences = nltk.sent_tokenize(text)\n\n    chunks = []\n    current = []\n    current_len = 0\n\n    for sent in sentences:\n        words = sent.split()\n        if not words:\n            continue\n\n        if current_len + len(words) &gt; target_words and current:\n            chunks.append(\" \".join(current))\n            current = []\n            current_len = 0\n\n        current.append(sent)\n        current_len += len(words)\n\n    if current:\n        chunks.append(\" \".join(current))\n\n    return chunks\n\nprint(f\"Found {len(txt_paths)} text files.\")\nprint()\n\nfor path in txt_paths:\n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        text = f.read()\n\n    chunks = chunk_text(text, TARGET_WORDS)\n\n    for c in chunks:\n        tokens = simple_preprocess(c, deacc=True)\n        n_tokens = len(tokens)\n\n        # Filter by length\n        if MIN_WORDS &lt;= n_tokens &lt;= MAX_WORDS:\n            all_chunks.append(c)\n            all_token_lists.append(tokens)\n\nprint(\"Total chunks kept (after filtering):\", len(all_chunks))\nAnd run it. At this point, you can see:\n\ntotal number of .txt files processed: 516\ntotal chunks kept (after filtering): 539057",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#step-5-train-word2-vec-on-our-corpus",
    "href": "week-08-09-word2vec.html#step-5-train-word2-vec-on-our-corpus",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "In Week 07, we used Word2Vec as a way to represent meaning geometrically: words that occur in similar contexts end up close together in vector space.\nThis week, we will use Word2Vec for a very practical purpose: expand a seed term into a list of conceptually related terms (so we can build a weakly supervised training set).\nOur seed term will be:\n\nmerchant\n\nWe will then have Word2Vec look for terms that are in the same (geometric) neighborhood as merchant.\nOK, create a new file step5_train_word2vec.py and run it:\nfrom pathlib import Path\nimport nltk\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import Word2Vec\n\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\n\n\n# Load and preprocess all texts (same logic as Step 4)\n\n\nTEXT_DIR = Path(\"texts\")\ntxt_paths = sorted(TEXT_DIR.glob(\"*.txt\"))\n\nTARGET_WORDS = 120\nMIN_WORDS = 5\nMAX_WORDS = 200\n\ndef chunk_text(text, target_words=120):\n    sentences = nltk.sent_tokenize(text)\n\n    chunks = []\n    current = []\n    current_len = 0\n\n    for sent in sentences:\n        words = sent.split()\n        if not words:\n            continue\n\n        if current_len + len(words) &gt; target_words and current:\n            chunks.append(\" \".join(current))\n            current = []\n            current_len = 0\n\n        current.append(sent)\n        current_len += len(words)\n\n    if current:\n        chunks.append(\" \".join(current))\n\n    return chunks\n\ntoken_lists = []\n\nprint(f\"Found {len(txt_paths)} text files.\")\nprint(\"Building token lists...\")\n\nfor path in txt_paths:\n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        text = f.read()\n\n    chunks = chunk_text(text, TARGET_WORDS)\n\n    for c in chunks:\n        tokens = simple_preprocess(c, deacc=True)\n        if MIN_WORDS &lt;= len(tokens) &lt;= MAX_WORDS:\n            token_lists.append(tokens)\n\nprint(\"\\nTotal tokenized chunks kept:\", len(token_lists))\n\n################\n# Train Word2Vec (same logic as what we did in week 07)\n################\n\nprint(\"\\nTraining Word2Vec...\")\n\nmodel = Word2Vec(\n    sentences=token_lists,\n    vector_size=200,   # dimensionality of word vectors\n    window=5,          # context window size\n    min_count=5,       # ignore very rare words\n    workers=4,         # adjust depending on your machine (see Week 07)\n    sg=1               # 1 = skip-gram; 0 = CBOW\n)\n\n# Save model\n\nPath(\"models\").mkdir(exist_ok=True)\nmodel_path = Path(\"models\") / \"w2v_full.bin\"\nmodel.save(str(model_path))\n\nprint(\"\\nModel saved to:\", model_path)\nThis will take a little while (or more than a little while, depending on your machine). Once it’s done, it will tell you: Model saved to: models\\w2v_full.bin.\n\n\nNow, we are going to load the model and ask for neighbors of our seed concept (that is, what words are near “merchant” in the vector space?).\nSo, create another file, step5b_query_word2vec.py, and run it:\nfrom pathlib import Path\nfrom gensim.models import Word2Vec\n\nmodel_path = Path(\"models\") / \"w2v_full.bin\"\nmodel = Word2Vec.load(str(model_path))\n\nseed = \"merchant\"\n\nif seed not in model.wv:\n    print(f\"'{seed}' not found in the model vocabulary.\")\n    print(\"This usually means min_count is too high or the corpus is too small.\")\nelse:\n    print(f\"Top 30 words similar to '{seed}':\")\n    for word, score in model.wv.similar_by_word(seed, topn=30):\n        print(f\"  {word:20s} {score:.3f}\")\nMy output is:\nTop 30 words similar to 'merchant':\nmarchant 0.736\nfactor 0.702\nmerchants 0.689\njeweller 0.667\ncustomer 0.665\npurser 0.648\ntailor 0.639\nwholesale 0.637\nclothier 0.635\nworshipful 0.632\nvintner 0.629\nclothyer 0.620\nadventurer 0.620\ntrade 0.620\nbrewer 0.619\nventurer 0.617\nchapman 0.616\nfactory 0.610\nseller 0.610\nstaple 0.606\ntradesman 0.606\nadventurers 0.606\nsailor 0.604\nhorner 0.603\nstaplers 0.599\neasterling 0.599\nbanker 0.598\nmarchants 0.595\ngoldsmith 0.594\napprentice 0.592\nWhat did you get? Anything surprising or simply unclear? The list here makes a lot of sense to me. A historical note: “Easterling merchants” would have been merchants from the Baltic regions, so that works in this list. What we have created is a corpus-specific semantic map for the concept of “merchant.”\nWe will now use this list as a resource for weak supervision in the next steps in our classification task.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#step-6-build-a-tiered-keyword-list-abc",
    "href": "week-08-09-word2vec.html#step-6-build-a-tiered-keyword-list-abc",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "Word2Vec gives us a list of words “near” our seed term (merchant). But we do not want to blindly treat Word2Vec neighbors as truth. Instead, we use them as a suggestions and then impose an interpretive structure.\nA useful strategy is to split candidate terms into tiers:\n\nTier A (high confidence): direct hits and spelling variants\ne.g., merchant, marchant, merchants, marchants\nTier B (strongly related roles): terms that usually indicate commercial activity\ne.g., factor, chapman, adventurer, venturer, staple\nTier C (maybe / adjacent): occupational neighborhood terms that might appear in commercial contexts but are often broader\ne.g., tailor, clothier, haberdasher\n\nThis tiering is a modeling decision:\n\nTier A+B will become our CORE triggers (high precision).\nTier C will become a MAYBE set (to inspect separately).\n\nCreate a new file step6_define_tiers.py and run it:\n# Tier A: direct spellings / variants of the seed concept\nTIER_A = {\n    \"merchant\", \"merchants\",\n    \"marchant\", \"marchants\"\n}\n\n# Tier B: closely related commercial roles / terms\nTIER_B = {\n    \"factor\", \"chapman\",\n    \"adventurer\", \"adventurers\",\n    \"venturer\", \"venturers\",\n    \"staple\", \"staplers\",\n    \"trade\",\n    \"purser\"\n}\n\n# Tier C: \"maybe\" occupational neighborhood (often adjacent, not always merchant-specific)\nTIER_C = {\n    \"clothier\", \"clothyer\",\n    \"tailor\", \"tayler\",\n    \"haberdasher\",\n    \"goldsmith\",\n    \"vintner\",\n    \"brewer\",\n    \"banker\",\n    \"grazier\",\n    \"jeweller\"\n}\n\nprint(\"Tier A size:\", len(TIER_A))\nprint(\"Tier B size:\", len(TIER_B))\nprint(\"Tier C size:\", len(TIER_C))\n\nprint(\"\\nTier A:\", sorted(TIER_A))\nprint(\"\\nTier B:\", sorted(TIER_B))\nprint(\"\\nTier C:\", sorted(TIER_C))\nLet’s walk through what each tier represents and why I set them up this way:\n\nTier A: terms are direct references to the concept we care about (spelling variants common in early modern texts);\nTier B: these terms describe occupations and roles that often involve trade; institutional or economic positions adjacent to merchants; vocabulary that appears frequently in commercial contexts. But they are not perfect synonyms for “merchant.”\nTier C: these terms are occupational nouns that are often related to production, craft, or commerce, and they frequently appear near merchant discourse. But they are much more ambiguous.\n\nThis is where interpretive judgment enters the pipeline:\n\nWe decide what our category means.\nWe decide which words count as strong evidence vs. weak evidence.\nWe acknowledge that “merchant discourse” is fuzzy at the edges.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#step-7-label-chunks-using-tiered-keywords-weak-supervision",
    "href": "week-08-09-word2vec.html#step-7-label-chunks-using-tiered-keywords-weak-supervision",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "We now have:\n\na large set of sentence-based chunks,\ntokenized into word lists,\nand a tiered list of keywords (A / B / C).\n\nThe next step is to assign labels to chunks. This is where we can translate our decision making about tiers into rule-based labeling of the chunks. This is called weak supervision.\n\n\nFor each chunk:\n\nCORE (label = 1) → contains any Tier A or Tier B word\n(high confidence merchant discourse)\nMAYBE (label = 2) → contains Tier C words only\n(ambiguous / adjacent cases)\nNEG (label = 0) → contains none of the above\n\nThese rules encode our interpretive decisions from Step 6. OK, now we can get started with the actual labelling.\n\n\n\nCreate a new file: step7_label_chunks.py and run it.\nfrom pathlib import Path\nimport nltk\nfrom gensim.utils import simple_preprocess\nimport json\n\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\n\n#\n# Tier definitions (from Step 6)\n\n\nTIER_A = {\n    \"merchant\", \"merchants\",\n    \"marchant\", \"marchants\"\n}\n\nTIER_B = {\n    \"factor\", \"chapman\",\n    \"adventurer\", \"adventurers\",\n    \"venturer\", \"venturers\",\n    \"staple\", \"staplers\",\n    \"trade\", \"purser\"\n}\n\nTIER_C = {\n    \"clothier\", \"clothyer\",\n    \"tailor\", \"tayler\",\n    \"haberdasher\",\n    \"goldsmith\",\n    \"vintner\",\n    \"brewer\",\n    \"banker\",\n    \"grazier\",\n    \"jeweller\"\n}\n\n\n# Load and process texts\n\n\nTEXT_DIR = Path(\"texts\")\ntxt_paths = sorted(TEXT_DIR.glob(\"*.txt\"))\n\nTARGET_WORDS = 120\nMIN_WORDS = 5\nMAX_WORDS = 200\n\ndef chunk_text(text, target_words=120):\n    sentences = nltk.sent_tokenize(text)\n\n    chunks = []\n    current = []\n    current_len = 0\n\n    for sent in sentences:\n        words = sent.split()\n        if not words:\n            continue\n\n        if current_len + len(words) &gt; target_words and current:\n            chunks.append(\" \".join(current))\n            current = []\n            current_len = 0\n\n        current.append(sent)\n        current_len += len(words)\n\n    if current:\n        chunks.append(\" \".join(current))\n\n    return chunks\n\nlabeled = []\n\nprint(f\"Processing {len(txt_paths)} files...\")\n\nfor path in txt_paths:\n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        text = f.read()\n\n    chunks = chunk_text(text, TARGET_WORDS)\n\n    for c in chunks:\n        tokens = simple_preprocess(c, deacc=True)\n        n_tokens = len(tokens)\n\n        if not (MIN_WORDS &lt;= n_tokens &lt;= MAX_WORDS):\n            continue\n\n        token_set = set(tokens)\n\n        if token_set & (TIER_A | TIER_B):\n            label = 1   # CORE\n        elif token_set & TIER_C:\n            label = 2   # MAYBE\n        else:\n            label = 0   # NEG\n\n        labeled.append((c, label))\n\nprint(\"Total chunks labeled:\", len(labeled))\n\n\n# Save labeled data\n\n\nPath(\"data\").mkdir(exist_ok=True)\n\nwith open(Path(\"data\") / \"merchant_labeled_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(labeled, f, ensure_ascii=False)\n\nprint(\"Saved labeled chunks to data/merchant_labeled_chunks.json\")\nWhat we did so far: we applied rules we designed, using the lexical triggers we chose, to create a labeled dataset. Obviously, these labels are imperfect and corpus-specific, but they are good enough to train a first classifier.\nAt this point, we have turned unstructured text into a supervised learning dataset. All the steps that we are doing next (train/test splits, classifiers, confusion matrices) depend on the choices made up to here. We can’t quite train a classifier, yet. First, we need to separate CORE, MAYBE, and NEG examples, inspect them, and think carefully about evaluation.\nThis is where questions of precision and recall enter the picture.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#step-8-sample-and-inspect",
    "href": "week-08-09-word2vec.html#step-8-sample-and-inspect",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "At the end of step7_label_chunks.py add the following code and run it.\nprint(\"\\nLabel distribution:\")\nprint(f\"  CORE (1): {sum(1 for _, y in labeled if y == 1)}\")\nprint(f\"  NEG  (0): {sum(1 for _, y in labeled if y == 0)}\")\nprint(f\"  MAYBE(2): {sum(1 for _, y in labeled if y == 2)}\")\n\n# Sample one from each category\nfor label_name, label_val in [(\"CORE\", 1), (\"MAYBE\", 2), (\"NEG\", 0)]:\n    example = next((text for text, y in labeled if y == label_val), None)\n    if example:\n        print(f\"\\n{label_name} example (first 200 chars):\")\n        print(example[:200])\nThis code will count how many chunks we got for each label and show examples of each type. When you get the results:\n\nDoes the CORE example actually contain merchant language?\n\n\n\nDoes the MAYBE example feel ambiguous?\n\n\n\nDoes the NEG example clearly not relate to merchants?",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#step-9-decide-what-to-train-on",
    "href": "week-08-09-word2vec.html#step-9-decide-what-to-train-on",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "Now we have three kinds of labeled chunks of text:\n\nCORE (label = 1): High-confidence merchant discourse\n\nMAYBE (label = 2): Ambiguous, adjacent, or borderline cases\n\nNEG (label = 0): Clearly not merchant discourse\n\nBefore we actually train a classifier, we must decide which of these labels should the model actually learn from. For this week, we will:\n\nTrain the classifier only on CORE vs. NEG\nExclude MAYBE from training\nSave MAYBE for later inspection and interpretation\n\nWhy? The honest answer is: I tried a couple of options and compared with my own readings of the texts, this gave the best results at this stage. But I also want to note the MAYBE category contains texts that are close to commerce but not clearly merchant discourse and so require a lot of contextual interpretation. They are also texts where other scholars may disagree with my reading. This means that including them for this training, would reduce interpretability and make evaluation harder to explain.\nInstead, we treat the MAYBE texts as a set of texts to which we apply the trained classifier later. This mirrors how supervised models are often used in practice: - trained on clear cases, - applied to ambiguous ones.\n\n\nCreate a new file step9_prepare_datasets.py . Before you run it, read the explanation about the NEG step after the code sample. Then run it.\nimport json\nfrom pathlib import Path\nimport random\n\nrandom.seed(42) # For reproducibility--same random sample each time [see (*) below]\n\nDATA_PATH = Path(\"data\") / \"merchant_labeled_chunks.json\"\n\nwith open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n    labeled = json.load(f)\n\ncore = [(t, 1) for (t, y) in labeled if y == 1]\nneg  = [(t, 0) for (t, y) in labeled if y == 0]\nmaybe = [t for (t, y) in labeled if y == 2]\n\nprint(\"Loaded:\")\nprint(\"  CORE:\", len(core))\nprint(\"  NEG :\", len(neg))\nprint(\"  MAYBE:\", len(maybe))\n\n###### =&gt; NEG step [see explanation below]\nneg_sample = random.sample(neg, len(core))\n\ntraining_data = core + neg_sample\nrandom.shuffle(training_data)\n\nprint(\"Training set size (CORE + NEG):\", len(training_data))\n\n### Split the data into train and test sets:\n\nsplit = int(0.8 * len(training_data))\ntrain_data = training_data[:split]\ntest_data  = training_data[split:]\n\nprint(\"Train size:\", len(train_data))\nprint(\"Test size :\", len(test_data))\nprint(\"MAYBE size:\", len(maybe))\n\n# Save the datasets:\n\nPath(\"data\").mkdir(exist_ok=True)\n\nwith open(Path(\"data\") / \"train_core_vs_neg.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(train_data, f, ensure_ascii=False)\n\nwith open(Path(\"data\") / \"test_core_vs_neg.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(test_data, f, ensure_ascii=False)\n\nwith open(Path(\"data\") / \"maybe_texts.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(maybe, f, ensure_ascii=False)\n\nprint(\"Saved training, test, and MAYBE datasets.\")\nWith this step in the code we are downsampling NEG to match the size of the CORE set. Why? [Think for a second about what most of the texts might look like…] Well, in most corpora, NEG examples vastly outnumber CORE examples: most of the text is not likely to be talking about the concept/topic you are targeting. Now, the texts that we have for class are a little skewed towards “merchant” and, as you will see after you run the code, we would still have an overwhelming number of NEG chunks, but I want you to learn best practices in general!\nTraining directly on all NEG data can cause the model to:\n\nalways predict “not merchant”\nachieve high accuracy but low usefulness\n\nOK, this is what I got back from this step:\nLoaded:\nCORE: 11373\nNEG : 527174 [!!!]\nMAYBE: 510\nTraining set size (CORE + NEG): 22746 [Important: can you see where this is coming from based on the above explanation? And, can you see why we needed to downsample?]\nTrain size: 18196\nTest size : 4550\nMAYBE size: 510\nOne more point (*): we use Python’s built-in pseudo-random number generator to, first, random.sample() from the full NEG list when we are downsampling the NEG to the same size as CORE [note that we are asking it to select as many NEG as len(core)]. Second, we use random.shuffle() to randomly reorder the combined CORE+NEG list because otherwise when we do our 80/20, all the CORE examples would be first and all the NEG examples would be after (not what we want!).",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-08-09-word2vec.html#week-9-part-iithe-classification-task",
    "href": "week-08-09-word2vec.html#week-9-part-iithe-classification-task",
    "title": "Weeks 08 & 9: Weak Supervision & Supervised Text Classification in Python",
    "section": "",
    "text": "Now we are ready to create an actual classifier. So far, our training data looks like this:\n\na chunk of text (a string),\npaired with a label (CORE = 1 or NEG = 0).\n\nWe can read these strings directly (and have done so to check things along the way), but obviously computers cannot. A classifier cannot operate on words, sentences, or meanings. It can only operate on numbers. So before we train a model, we must transform text into a numerical representation as we have been doing in previous weeks. In particular, we will revisit our old friend TF-IDF (remember our work in week 4).\nOur next steps:\n\nConvert text chunks into TF-IDF vectors. This helps emphasize distinctive vocabulary rather than common function words.\nTrain a simple binary classifier\nEvaluate its performance on held-out data\nApply it to the MAYBE set (review above if needed)\nEvaluate on what the model learned and what it did not, and see if it is useful for historical interpretation.\n\nAs with week 8, we are going to build everything step by step. Create a new file, step10_tfidf_representation.py. Don’t run it yet, I will give explanations below and I want you to read and understand them before running the code.\nimport json\nfrom pathlib import Path\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nDATA_DIR = Path(\"data\")\n\n#We now load the datasets we prepared at the end of last week (step 9)\nwith open(DATA_DIR / \"train_core_vs_neg.json\", \"r\", encoding=\"utf-8\") as f:\n    train_data = json.load(f)\n\nwith open(DATA_DIR / \"test_core_vs_neg.json\", \"r\", encoding=\"utf-8\") as f:\n    test_data = json.load(f)\n\n# Separate texts and labels\nX_train_texts = [t for (t, y) in train_data]\ny_train = [y for (t, y) in train_data]\n\nX_test_texts = [t for (t, y) in test_data]\ny_test = [y for (t, y) in test_data]\n\nprint(\"Train size:\", len(X_train_texts))\nprint(\"Test size :\", len(X_test_texts))\n\n##### ==&gt; See explanation [A] below\n\nvectorizer = TfidfVectorizer(\n    lowercase=True,\n    min_df=5,        # ignore very rare words\n    max_df=0.9       # ignore extremely common words; Explanation [B]\n)\nX_train = vectorizer.fit_transform(X_train_texts)\nX_test = vectorizer.transform(X_test_texts)\n\nprint(\"TF-IDF matrix shapes:\")\nprint(\"  Train:\", X_train.shape)\nprint(\"  Test :\", X_test.shape)\n\n#####  ==&gt; See explanation [C] below\n\nExplanation [A]: after the lines above, we still have raw text, so we have to define a TF-IDF vectorizer.\nExplanation [B]: we set the parameters as min_df=5 so that we remove words that appear in fewer than 5 documents; and max_df=0.9 so that we remove words that appear in more than 90% of the documents.\nExplanation [C]: we fit the vectorizer on the training data only, then apply it to the test data. Each text chunk is now a numeric vector.\n\nWe have:\n\n\n\nObject\nWhat it contains\nWhat it’s used for\n\n\n\n\nX_train\nTF-IDF vectors\nModel learns patterns\n\n\ny_train\nLabels\nSupervision signal\n\n\nX_test\nTF-IDF vectors\nModel makes predictions\n\n\ny_test\nLabels\nWe check how good predictions are\n\n\n\nI am going to belabor this more so that it’s fully clear, you can think of the X, Y part of this as:\nX = the input to the model → the text, represented numerically (TF-IDF vectors)\ny = the output we want the model to predict → the labels (CORE = 1, NEG = 0)\nSo:\nX_train = text chunks (as numbers)\ny_train = labels for those chunks\nWhere train = data the model is allowed to learn from; test = data the model has never seen before.\nWhen you run step 10, your output will look like:\nTrain size: 36874\nTest size : 9218\nTF-IDF matrix shapes:\nTrain: (36874, 27806)\nTest : (9218, 27806)\nThis is reassuring as it is what we want: we have the 80/20 split we created in step 9: 36,874 chunks were used to train the model; 9,218 chunks were held out for evaluation. So far so good.\nIn addition, the TF-IDF matrix has the shape (number of documents) by (number of features), that is (36874, 27806) translates to a matrix that has:\n\n36874 rows, where each row is for a training chunk. Each row is a TF-IDF vector representing a chunk.\n27,806 columns, where each column is for a word feature. Each column corresponds to a specific word in the vocabulary. Yes, we have 27,806 features! That’s not a problem: remember, TF-IDF matrices are sparse, linear models do well with this kind of setting (see the info below about the choice of logistic regression), and this is normal for text data.\n\n\n\nLater, when we compute a confusion matrix:\n\npredictions come from X_test\n“true labels” come from y_test\n\nThe confusion matrix will help us understand how often the model’s predictions on unseen data match the labels produced by our weak supervision rules. I will remind you below, but you want to start keeping in mind that the confusion matrix does not measure truth, it just measures agreement with a labeling scheme.\n\n\n\nNow we can finally train a classifier using a logistic regression model. We discussed how a logistic regression works in class (or will do so in Week 9, if you are reading ahead). If you want an advanced version of this, see chapter 4 in Jurafsky and Martin.\n\nNote: we are not going to go into the details covered in section 4.4 forward. For the simple reason that it would require more mathematical background than I assume for the class to discuss the loss function and gradients. (We will touch on the materials from section 4.9 on how to build a confusion matrix.)\n\nThere are many classifiers we could use. We choose logistic regression for three main reasons:\n\nIt is simple and well understood. It works well with a binary task such as ours: CORE (1) vs NEG (0). It models:\n\nLogistic regression is literally designed to model:\n\\[\nP(y =1 \\mid x)\n\\]\nSo it gives you: a probability of CORE for each chunk (via predict_proba ) and a clean decision rule: “predict CORE if probability ≥ 0.5”.\n\nIt works very well with TF-IDF features. TF-IDF produces very high-dimensional vectors (tens of thousands to 100k+ features) which are mostly zeros (sparse). Linear models such as logistic regression work well for these situations (they are fast to train and are stable).\nNote: the default threshold in scikit-learn is 0.5. As we saw in class, logistic regression models the log-odds of class membership and a probability of 0.5 corresponds to equal odds. This is reasonable when classes are roughly balanced in the training setup (recall that we decided to downsample the negatives above). We could change the threshold if we had good reasons to do so. I will bring this point up again at the end.\n\nThere are other options (linear support vector machine or naive Bayes), but we choose logistic regression because it is a strong, interpretable baseline for binary text classification with TF-IDF features. TF-IDF creates high-dimensional sparse vectors, and linear models are known to perform very well in this setting. Logistic regression also outputs probabilities, which lets us interpret model confidence and apply the model to ambiguous cases (our MAYBE set).\nCreate a new file, step11_train_classifier.py. We will build the script in chunks with explanations at each step and you will run it once it’s all put together.\nfrom pathlib import Path\nimport json\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    confusion_matrix,\n    classification_report,\n    roc_auc_score\n)\nNext, we load the data and use the TF-IDF representations we created in step 10 above.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nDATA_DIR = Path(\"data\")\n\nwith open(DATA_DIR / \"train_core_vs_neg.json\", \"r\", encoding=\"utf-8\") as f:\n    train_data = json.load(f)\n\nwith open(DATA_DIR / \"test_core_vs_neg.json\", \"r\", encoding=\"utf-8\") as f:\n    test_data = json.load(f)\n\nX_train_texts = [t for (t, y) in train_data]\ny_train = [y for (t, y) in train_data]\n\nX_test_texts = [t for (t, y) in test_data]\ny_test = [y for (t, y) in test_data]\n\nvectorizer = TfidfVectorizer(\n    lowercase=True,\n    min_df=5,\n    max_df=0.9\n)\n\nX_train = vectorizer.fit_transform(X_train_texts)\nX_test = vectorizer.transform(X_test_texts)\nNow we actually train the classifier (yes, I know, finally!). What will happen in this section of the script: the model examines the TF-IDF features and it learns the weights that separate CORE from NEG. We then make predictions on the test set.\nclf = LogisticRegression(\n    max_iter=1000,\n    n_jobs=1\n)\n\nclf.fit(X_train, y_train)\n\n#test set predictions\ny_pred = clf.predict(X_test)\ny_prob = clf.predict_proba(X_test)[:, 1]\nAt this point we evaluate our classifier with a confusion matrix that compares:\n\ntrue labels (y_test)\nmodel predictions (y_pred)\n\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion matrix:\")\nprint(cm)\nI will go over what this does in more detail below (step 12). But the next step is to include two more evaluation components. First, a classification report that summarizes how reliable positive predictions are (= precision); how many positives the model found (= recall); and the balance of precision and recall (F1-score). Second, ROC AUC, which measures how well the model separates CORE from NEG across all thresholds.\nAdd the next bit of code to your script and then run everything in step11_train_classifier.py.\nprint(\"\\nClassification report:\")\nprint(classification_report(y_test, y_pred))\n\n###ROC AUC\nauc = roc_auc_score(y_test, y_prob)\nprint(\"ROC AUC:\", round(auc, 3))\n\n\n\nNow that we have trained the classifier, computed the confusion matrix and ROC AUC, we can evaluate how closely do the model’s predictions agree with the labels in our test set, given a particular decision rule.\nLet’s start with the confusion matrix. When you ran step 11, you should have gotten something like this:\n[[6943 27]\n[ 367 1881]]\nWhat does this mean? I am going to present it as a table to make its entries legible:\n\n\n\n\nPredicted NEG (0)\nPredicted CORE (1)\n\n\n\n\nActual NEG (0)\n6943\n27\n\n\nActual CORE (1)\n367\n1881\n\n\n\nEach cell tells us something different about model behavior. By convention in scikit-learn:\n\nRows = actual (true) labels\nColumns = predicted labels\n\nSo we can rewrite it structurally as:\n\\[\\begin{bmatrix} \\text{TN} & \\text{FP} \\\\ \\text{FN} & \\text{TP} \\end{bmatrix}\\]\nTo make this clear:\n\nTrue negatives (TN) = 6943 → 6943 texts were labeled NEG in the test set, and the model correctly predicted NEG for them.\nFalse Positive (FP) = 27 → 27 texts were labeled NEG in the test set, and the model incorrectly predicted CORE for them.\nFalse Negatives (FN) = 367 → 367 texts were labeled CORE in the test set, and the model incorrectly predicted NEG for them.\nTrue Positives (TP) = 1881 → 1881 texts were labeled CORE in the test set, and the model correctly predicted CORE for them.\n\nInterpretation: our decision threshold in the logistic regression was 0.5. That is, if the predicted probability ≥ 0.5, label the text CORE; if the predicted probability &lt; 0.5, label the text NEG. The model is very good at recognizing texts that do not match our merchant criteria. It’s fairly good at avoiding predicting as positive texts that are labelled negative (27 false positives), but we do lose a fair amount of CORE labeled texts (367).\nThe confusion matrix is not the only validation tool we have. Let’s look at the classification report. You should have gotten something like this:\n\n\n\n\n\n\nClass\nPrecision\nRecall\nF1-Score\nSupport\n\n\n\n\n0\n0.95\n1.00\n0.97\n6,970\n\n\n1\n0.99\n0.84\n0.91\n2,248\n\n\n\nWhat do these numbers mean?\nFor CORE (class 1):\nPrecision (1) = 0.99\nFormula: \\(\\text{Precision} = \\frac{TP}{TP+FP}\\)\nIn our case, this tells us that when the model predicts CORE, it is correct 99% of the time. This matches your tiny number of false positives (27).\nRecall (1) = 084\nThe formula: \\(\\text{Recall} = \\frac{TP}{TP + FN}\\)\nThe model finds 84% of the CORE texts and it misses 16% (those 367 false negatives).\nF1-score (1) = 0.91\nF1 balances precision and recall.\nThe formula: \\(F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\)\n0.91 is a strong F1 score, but lower than precision because recall is lower.\nI will leave it up to you to think through the values for the NEG class.\nAs part of our report, we also had the following values:\n\n\n\nMetric\nPrecision\nRecall\nF1-Score\nSupport (total)\n\n\n\n\nAccuracy\n\n\n0.96*\n9,218\n\n\nMacro avg\n0.97\n0.92\n0.94\n9,218\n\n\nWeighted avg\n0.96\n0.96\n0.96\n9,218\n\n\n\n(*) I know, I know, this is weird: the placement of accuracy in the F1-score column is just a formatting convention from sklearn’s classification report. The 0.96 value had to go somewhere, and they chose that column.\nSo, what is actually going on besides formatting weirdness:\nAccuracy = 0.96\nFormula: \\(\\frac{TP + TN}{\\text{Total}}\\)\nInterpretation: the model agrees with the test labels 96% of the time. In general accuracy alone is not very informative in imbalanced tasks, which is why we look at precision and recall. But recall that we downsampled NEG to match CORE, so we had a balanced training set. Nonetheless, precision and recall are still very useful in understanding the kinds of errors the model is making.\n\n\n\nHere we take a simple average of class metrics to treat both classes equally:\n\\[\n\\frac{Metric_0 + Metric_1}{2}\n\\]\n\n\n\nWeights each class by its support (number of examples). Since NEG is larger (6970 vs 2248), the weights workout to:\nweight_neg = \\(\\frac{6970}{9218}\\) or about 0.756\nweight_core = \\(\\frac{2248}{9218}\\) or about 0.244\nThe weighted average multiplies each class’s score by the proportion of examples in that class. Since NEG makes up about 76% of the test set and performs very well, the weighted average is pulled toward the NEG score and ends up close to overall accuracy.\nFor example, the weighted F1:\nWeighted F1 is:\n\\((w_0 \\times F1_0) + (w_1 \\times F1_1)\\)\nSubstitute values:\n\\((0.756 \\times 0.97) + (0.244 \\times 0.91)\\)\nCompute each term:\n\\(0.756 \\times 0.97 ≈ 0.733\\)\n\\(0.244 \\times 0.91 ≈ 0.222\\)\nAdd them and round \\(≈0.96\\)\nThe weighted F1-score is close to overall accuracy because the model performs strongly on both classes and the larger class (NEG) dominates the weighting.\n\n\n\nNow the most important conceptual point in evaluating our model: ROC AUC does not use the 0.5 threshold. It evaluates how well the model separates the two classes across all possible thresholds.\nHow does ROC AUC work? Instead of using one specific cutoff, it changes the threshold from 0 to 1 and tests how well the model separates CORE from NEG at each threshold. Then it evaluates performance across all those possibilities.\nSplitting this up into its components:\nROC stands for Receiver Operating Characteristic.\nIt plots:\n\nTPR: True Positive Rate (same as Recall) on the y-axis\nFPR: False Positive Rate on the x-axis\n\nAs the threshold moves from 0 to 1, the curve traces out the model’s performance. If the model:\n\nperfectly separates classes → curve hugs the top-left corner (high TPR, low FPR at all thresholds)\nguesses randomly → curve follows a diagonal line (50/50 at every threshold)\n\nAUC stands for Area Under the Curve. It compresses all this information into a single number (for us, 0.996). AUC is literally the area under the ROC curve. It’s the integral of the True Positive Rate with respect to the False Positive Rate as we vary the threshold. This is why AUC ranges from 0.5 (diagonal line, area of triangle = 0.5) to 1.0 (perfect corner, full square = 1.0).\nMath aside: you don’t need to worry about computing this ourself, but here’s the integral:\n\\[\\text{AUC} = \\int_0^1 \\text{TPR}(t) \\, d(\\text{FPR}(t))\\]\nYou can think of it this way: if I randomly choose one CORE chunk and one NEG chunk, the AUC tells you the probability that the model assigns a higher probability to the CORE chunk.\nJust a reminder: you can see a full explanation here.\nWe have a value of 0.996, which means that the model almost perfectly ranks CORE texts above NEG texts. This is good! Unlike accuracy or F1-score which depend on choosing a threshold, ROC AUC tells us whether our weak supervision approach is fundamentally learning the right pattern. A high AUC (like 0.996) means our lexicon-based labels are capturing something real about “merchant” discourse.\n\n\n\nRemember that I mentioned that we could change the threshold for the classification task if we had a good reason to do so. We now want to go back and think through this issue.\nFirst of all, you might want to ask: Does high ROC-AUC mean that our 0.5 threshold performed well? We want to be careful with this question:\nOur high ROC-AUC (0.996) tells us the model is excellent at ranking texts. It consistently assigns higher probabilities to CORE than NEG. This means that if you sorted all chunks by their probability scores, true CORE chunks would appear near the top and true NEG chunks near the bottom 99.6% of the time, regardless of where you draw the threshold line.\n\nTrue CORE chunks get higher probability scores than true NEG chunks (ranking). Whether they cross the 0.5 threshold is a separate question (classification)\n\nAs an example, you could have:\nCORE chunk with prob = 0.48 (ranked above NEG chunks, but classified as NEG at 0.5) NEG chunk with prob = 0.02 (ranked below CORE chunks, classified correctly as NEG)\nThe model correctly ranked them (CORE &gt; NEG), which gives high AUC, even though the CORE chunk was misclassified at the 0.5 threshold.\nThis is important because we do not want to misread a high AUC as indicating that we picked (or were given by default) the optimal threshold for our specific research goals. As I mentioned earlier, the 0.5 threshold is a reasonable default when:\n\nClasses are balanced (we downsampled)\nFalse positives and false negatives are equally costly (In general, this depends on your research question)\n\nNow, we might want to explore other thresholds. Then we could pick one of the following approaches:\n\nPlot precision and recall at different thresholds\nOptimize for F1-score\nMake a domain-driven decision based on your research needs (if you ask me, this is always a question you should have in mind, but, at the end of the day, I am a humanist..)\n\nI am going to give you a couple of scenarios where this might be reasonable to do:\n\nI sometimes need a discovery tool to find texts (that talk about “merchant” concepts) that I might have missed. I would then lower the threshold (0.3-0.4) for high recall\nOn the other hand, if I am selecting passages for close reading and want only clear examples of the discourse, then I would pick a higher threshold (0.6-0.7) for high precision\nIf you want a balanced approach for further analysis (our case) → 0.5 is reasonable\n\nFor our purposes, we’ll stick with 0.5 since our evaluation metrics (precision, recall, F1) all look strong at this threshold, and it aligns with our balanced training set.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 08 and 09:Weak Supervision & Supervised Text Classification in Python"
    ]
  },
  {
    "objectID": "week-06-cooccurrence.html",
    "href": "week-06-cooccurrence.html",
    "title": "Week 06: Co-occurrence, PMI, and moving towards Word2Vec",
    "section": "",
    "text": "So far, we have been focusing on counting words and other features as a way to represent texts and to compare them to each other. What about modeling meaning? The basic intuition that we are going to work with is that we can learn a lot about the meaning of words by looking at their relational properties. That is:\n\na word’s meaning is constrained by its neighbors (“bank account” vs. “river bank”)\nmeaning emerges from patterns of use\n\nLater on in the semester, we will discuss the limitations of this approach (which some of you have already touched on in class by asking about sarcasm and irony, but there are objections derived from ordinary language philosophy that we need to address), but first we need to learn how to implement this intuition about meaning and to extract as much as we can from it!\nToday’s tutorial is preparing you for the next core concepts in the class:\n\nEmbeddings\nSemantics\nWord2Vec\nBERT-style models\n\nWe are going to do this with one of the foundational texts of modern economics, Adam Smith’s Wealth of Nations (1776). By the standards of this course, this is a positively modern text!\nYou will find the wealth.txt file under files on Canvas.\n\n\nStarting with out intuition that a word’s meaning is constrained by its neighbors, we have to define what we mean by “neighbors.” We are going to work with “windows” as the unit of analysis. The idea is that we place a window of fixed size across tokens and we record which words appear together.\nLet’s get started:\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(tibble)\nlibrary(readr) \nlibrary(tidyr) \nlibrary(tidyverse)\nlibrary(ggplot2)\n\nand let’s keep the same naming pattern we have developed in the previous tutorials:\n\nraw_text &lt;- read_file(\"texts/wealth.txt\")\n\ntexts &lt;- tibble(\n  doc_title = \"wealth.txt\",\n  text = raw_text\n)\n\ntexts\n\n# A tibble: 1 × 2\n  doc_title  text                                                               \n  &lt;chr&gt;      &lt;chr&gt;                                                              \n1 wealth.txt \"An Inquiry into the Nature and Causes of the Wealth of Nations\\r\\…\n\n## Bonus tip: another way to read a text that makes it easier to change later in the code:\n# file_path &lt;- \"wealth.txt\"\n# raw_text &lt;- read_file(file_path)\n\nGreat. Now we need to tokenize the text. Because we want to build windows that move through the text in order, we need to preserve token order (so we do not disregard word/token order the way we did in bag of words approaches).\n\ntokens &lt;- texts %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  group_by(doc_title) %&gt;%\n  mutate(token_id = row_number()) %&gt;%\n  ungroup()\n\n#Let's take a quick look \ntokens %&gt;% slice(1:20)\n\n# A tibble: 20 × 3\n   doc_title  word         token_id\n   &lt;chr&gt;      &lt;chr&gt;           &lt;int&gt;\n 1 wealth.txt an                  1\n 2 wealth.txt inquiry             2\n 3 wealth.txt into                3\n 4 wealth.txt the                 4\n 5 wealth.txt nature              5\n 6 wealth.txt and                 6\n 7 wealth.txt causes              7\n 8 wealth.txt of                  8\n 9 wealth.txt the                 9\n10 wealth.txt wealth             10\n11 wealth.txt of                 11\n12 wealth.txt nations            12\n13 wealth.txt by                 13\n14 wealth.txt adam               14\n15 wealth.txt smith              15\n16 wealth.txt contents           16\n17 wealth.txt introduction       17\n18 wealth.txt and                18\n19 wealth.txt plan               19\n20 wealth.txt of                 20\n\n\nFor this tutorial, we are going to try to get a sense for how Adam Smith discusses the term “labor” (side note: for the sake of simplicity, I normalized “labour” to “labor” before providing the file to you). This means that we are going to consider the token “labor” to be our anchor term and we will create windows around it. Important: a window size of 5 means that we are trying to capture 5 tokens on each side of our anchor.\nSelecting an anchor:\n\nanchor &lt;- \"labor\" #this allows you to change this as needed\nwindow_size &lt;- 5\n\n#check how many anchor words are in the text\ntokens %&gt;% filter(word == anchor) %&gt;% count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  1011\n\n\nOK, now we know that there are plenty of our anchor tokens, but we also need to know where they are so that we can find the windows around them.\n\nanchor_hits &lt;- tokens %&gt;%\n  filter(word == anchor) %&gt;%\n  select(doc_title, anchor_id = token_id) # organize by location of anchor position\n\n#Let's take a look at the tibble\nanchor_hits\n\n# A tibble: 1,011 × 2\n   doc_title  anchor_id\n   &lt;chr&gt;          &lt;int&gt;\n 1 wealth.txt        35\n 2 wealth.txt        61\n 3 wealth.txt        74\n 4 wealth.txt        81\n 5 wealth.txt       114\n 6 wealth.txt       147\n 7 wealth.txt       166\n 8 wealth.txt       230\n 9 wealth.txt       487\n10 wealth.txt       521\n# ℹ 1,001 more rows\n\n\nThe next step is the key to our analysis: we begin by pairing each anchor occurrence with every token in the document so that we can calculate distances between them. Once those distances are known, we filter down to the tokens that lie within a fix window around the anchor. So, let’s build the window table!\nPlease make sure that you understand the concepts behind the following block of code:\n\n# Each row in `windows` will be ONE token that appears near ONE anchor occurrence.\n\nwindows &lt;- anchor_hits %&gt;%\n  # Join each anchor occurrence to ALL tokens in the same document.\n  # This is a \"many-to-many\" join because one anchor hit matches many tokens [remember we had a similar set up last week].\n  left_join(tokens, by = \"doc_title\", relationship = \"many-to-many\") %&gt;%\n  \n  # Compute how far each token is from the anchor.\n  # Negative distance = to the LEFT of the anchor; positive = to the RIGHT.\n  mutate(distance = token_id - anchor_id) %&gt;%\n  \n  # Keep only tokens within the window size (±5 tokens).\n  filter(abs(distance) &lt;= window_size) %&gt;%\n  \n  # Remove the anchor word itself (distance 0) so we only keep context words, which is what we are interested in.\n  filter(distance != 0) %&gt;%\n  \n  \n  mutate(\n    window_id = paste0(doc_title, \"_\", anchor_id),\n    anchor_word = anchor\n  ) %&gt;%\n  \n  # Keep only the columns we need for the next steps.\n  select(\n    doc_title,\n    window_id,\n    anchor_word,\n    anchor_id,\n    token_id,\n    distance,\n    word\n  )\n\nwindows %&gt;%\n  arrange(anchor_id, distance) %&gt;%\n  slice(1:20)\n\n# A tibble: 20 × 7\n   doc_title  window_id     anchor_word anchor_id token_id distance word      \n   &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;           &lt;int&gt;    &lt;int&gt;    &lt;int&gt; &lt;chr&gt;     \n 1 wealth.txt wealth.txt_35 labor              35       30       -5 in        \n 2 wealth.txt wealth.txt_35 labor              35       31       -4 the       \n 3 wealth.txt wealth.txt_35 labor              35       32       -3 productive\n 4 wealth.txt wealth.txt_35 labor              35       33       -2 powers    \n 5 wealth.txt wealth.txt_35 labor              35       34       -1 of        \n 6 wealth.txt wealth.txt_35 labor              35       36        1 and       \n 7 wealth.txt wealth.txt_35 labor              35       37        2 of        \n 8 wealth.txt wealth.txt_35 labor              35       38        3 the       \n 9 wealth.txt wealth.txt_35 labor              35       39        4 order     \n10 wealth.txt wealth.txt_35 labor              35       40        5 according \n11 wealth.txt wealth.txt_61 labor              61       56       -5 i         \n12 wealth.txt wealth.txt_61 labor              61       57       -4 of        \n13 wealth.txt wealth.txt_61 labor              61       58       -3 the       \n14 wealth.txt wealth.txt_61 labor              61       59       -2 division  \n15 wealth.txt wealth.txt_61 labor              61       60       -1 of        \n16 wealth.txt wealth.txt_61 labor              61       62        1 chapter   \n17 wealth.txt wealth.txt_61 labor              61       63        2 ii        \n18 wealth.txt wealth.txt_61 labor              61       64        3 of        \n19 wealth.txt wealth.txt_61 labor              61       65        4 the       \n20 wealth.txt wealth.txt_61 labor              61       66        5 principle \n\n\nNow, each row in windows represents one word that appeared near one specific occurrence of the anchor. Next, we can collapse all the context window rows into a single ranked list by adding them up. This will measure how many times any given word appears within ±5 window_size tokens of the anchor across the whole document.\n\ncooc &lt;- windows %&gt;%\n  count(word, sort = TRUE, name = \"cooc_n\")\n\ncooc\n\n# A tibble: 1,085 × 2\n   word    cooc_n\n   &lt;chr&gt;    &lt;int&gt;\n 1 the       1146\n 2 of        1122\n 3 and        400\n 4 in         248\n 5 to         211\n 6 which      166\n 7 wages      158\n 8 produce    149\n 9 a          145\n10 is         142\n# ℹ 1,075 more rows\n\n\nSo you can view cooc_n as a tally of contextual proximity. Note: cooc_n does not tell you whether a word is important! It just tells you that it appears frequently (or not) near the anchor. At this point of the analysis, you can see that very common words (such as “is,” “the,” and “which”) rise to the top of the list. This is a problem as common words end up dominating even when they tell us little about the anchor–this is expected since we are using raw counts.\nYou may want to stop me and complain: well, why didn’t you remove stopwords? Wouldn’t that fix this? This is a reasonable complaint and removing stopwords would help but not solve the issue. Removing stopwords can make the co-occurrence list look more meaningful, but you will still have words that are very common in the document itself appear near any anchor word. We have a better way to solve the common word problem for this particular task…\n\n\n\nWith the raw co-occurrence count we are tracking the words that appear near the anchor most often. What we are going to compute using PMI is the words that appear near the anchor more often than we would expect given how frequently those words occur in the text overall. This makes PMI a better measure of association strength, rather than simple proximity.\nPointwise Mutual Information (PMI) is defined as:\n\\[\n\\mathrm{PMI}(w, a) = \\log_2 \\left( \\frac{P(w, a)}{P(w)\\,P(a)} \\right)\n\\]\nwhere:\n\n\\(P(w,a)\\) is the probability that the word \\(w\\) appears in the anchor’s context window\n\\(P(w)\\) is the overall probability of the word \\(w\\) in the document\n\\(P(a)\\) is the overall probability of the anchor word \\(a\\) in the document\n\nIf PMI is high, the word \\(w\\) appears near the anchor more often than we would expect by chance, given how frequent each word is overall. Reference if you want a deeper dive into PMI, see Appendix J in Jurafsky and Martin.\nFirst, we compute the overall word frequencies in the document, \\(P(w)\\) and \\(P(a)\\):\n\ntotal_tokens &lt;- nrow(tokens)\n\nword_freq &lt;- tokens %&gt;%\n  count(word, name = \"word_n\") %&gt;%\n  mutate(p_word = word_n / total_tokens)\n\nword_freq\n\n# A tibble: 10,228 × 3\n   word      word_n     p_word\n   &lt;chr&gt;      &lt;int&gt;      &lt;dbl&gt;\n 1 0            440 0.00115   \n 2 0d             1 0.00000261\n 3 1            192 0.000502  \n 4 1,000,000      2 0.00000523\n 5 1,001,171      1 0.00000261\n 6 1,101,107      1 0.00000261\n 7 1,200,000      1 0.00000261\n 8 1,214,583      1 0.00000261\n 9 1,243,120      1 0.00000261\n10 1,245,808      1 0.00000261\n# ℹ 10,218 more rows\n\n\nWe now get anchor frequency and probability:\n\nanchor_stats &lt;- word_freq %&gt;%\n  filter(word == anchor) %&gt;%\n  transmute(anchor_n = word_n, p_anchor = p_word)\n\nanchor_stats\n\n# A tibble: 1 × 2\n  anchor_n p_anchor\n     &lt;int&gt;    &lt;dbl&gt;\n1     1011  0.00264\n\n\nGut check: you might (or not) think that these tibbles look weird! At this stage, we are seeing the documents exactly as the computer sees it: a long sequence of tokens. Numbers appear here because Adam Smith uses a lot of numbers (which shouldn’t surprise us!). This gives us a baseline picture of overall frequency, which PMI will later use to decide whether a word’s appearance near the anchor is genuinely informative or merely a byproduct of being common everywhere.\nNow we need to compute \\(P(w,a)\\). We define the sample space as all tokens that appear inside any widow around the anchor word, excluding the anchor tokens themselves. You can think of this as the complete collection of all context words around the anchor in The Wealth of Nations. Then \\(P(w,a)\\) is the probability that, if we randomly select one token from all the tokens that occur inside anchor windows, that token is the word \\(w\\).\nIn practice, this means that we do the following:\n\nWe count how many times 𝑤 appears inside anchor windows.\nWe divide by the total number of tokens across all anchor windows.\nThe result is \\(P(w,a)\\).\n\nThis definition allows us to treat co-occurrence as a probabilistic event rather than just a raw count.\n\ntotal_window_tokens &lt;- nrow(windows)\n\np_w_given_windows &lt;- windows %&gt;%\n  count(word, name = \"cooc_n\") %&gt;%\n  mutate(p_word_in_windows = cooc_n / total_window_tokens)\n\np_w_given_windows\n\n# A tibble: 1,085 × 3\n   word     cooc_n p_word_in_windows\n   &lt;chr&gt;     &lt;int&gt;             &lt;dbl&gt;\n 1 1740          1         0.0000989\n 2 30            1         0.0000989\n 3 a           145         0.0143   \n 4 able          3         0.000297 \n 5 about         4         0.000396 \n 6 above         6         0.000593 \n 7 abridge       7         0.000692 \n 8 abridged      1         0.0000989\n 9 abridges      1         0.0000989\n10 absorbed      1         0.0000989\n# ℹ 1,075 more rows\n\n\nThis is effectively P(w| in anchor windows), which is proportional to \\(P(w,a)\\) for our purposes.\nNext, we join baseline frequencies and compute PMI\n\npmi_tbl &lt;- p_w_given_windows %&gt;%\n  left_join(word_freq, by = \"word\") %&gt;%\n  mutate(\n    pmi = log2(p_word_in_windows / (p_word * anchor_stats$p_anchor))\n  ) %&gt;%\n  arrange(desc(pmi))\n\npmi_tbl\n\n# A tibble: 1,085 × 6\n   word        cooc_n p_word_in_windows word_n     p_word   pmi\n   &lt;chr&gt;        &lt;int&gt;             &lt;dbl&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 tenfold          2         0.000198       1 0.00000261  14.8\n 2 abridges         1         0.0000989      1 0.00000261  13.8\n 3 ayrshire         1         0.0000989      1 0.00000261  13.8\n 4 carron           1         0.0000989      1 0.00000261  13.8\n 5 complexly        2         0.000198       2 0.00000523  13.8\n 6 conceives        1         0.0000989      1 0.00000261  13.8\n 7 declamation      1         0.0000989      1 0.00000261  13.8\n 8 differs          1         0.0000989      1 0.00000261  13.8\n 9 fashioning       1         0.0000989      1 0.00000261  13.8\n10 fattened         1         0.0000989      1 0.00000261  13.8\n# ℹ 1,075 more rows\n\n\nThis should start to give us a clearer picture! However, we do have to worry about rare words. PMI can become misleadingly large when cooc_n is very small (e.g., a word appears once in a window). A standard practice is to filter by minimum co-occurrence count.\n\npmi_tbl_filtered &lt;- pmi_tbl %&gt;%\n  filter(cooc_n &gt;= 3) %&gt;%        # adjust threshold as needed\n  arrange(desc(pmi))\n\npmi_tbl_filtered\n\n# A tibble: 423 × 6\n   word         cooc_n p_word_in_windows word_n    p_word   pmi\n   &lt;chr&gt;         &lt;int&gt;             &lt;dbl&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 abridge           7          0.000692      8 0.0000209  13.6\n 2 men’s             7          0.000692      8 0.0000209  13.6\n 3 hour’s            4          0.000396      5 0.0000131  13.5\n 4 division         51          0.00504      65 0.000170   13.5\n 5 divisions         3          0.000297      5 0.0000131  13.1\n 6 rewarded          6          0.000593     10 0.0000261  13.1\n 7 skilled           3          0.000297      5 0.0000131  13.1\n 8 subdivisions      3          0.000297      5 0.0000131  13.1\n 9 powers           19          0.00188      34 0.0000889  13.0\n10 productive       95          0.00940     175 0.000457   12.9\n# ℹ 413 more rows\n\n\nTake a minute to compare the first words in pmi_tbl_filtered with pmi_tbl. Which words rise to the top under PMI that did not dominate raw co-occurrence? What kinds of words does PMI “reward” and what kinds does it “punish”?\nFinally, we can visually compare raw co-occurrence:\n\ncooc %&gt;%\n  slice_max(cooc_n, n = 15) %&gt;%\n  mutate(word = reorder(word, cooc_n)) %&gt;%\n  ggplot(aes(x = cooc_n, y = word)) +\n  geom_col() +\n  labs(\n    title = str_glue(\"Top co-occurring words within ±{window_size} of '{anchor}'\"),\n    x = \"Co-occurrence count\",\n    y = NULL\n  )\n\n\n\n\n\n\n\n\nand top PMI (filtered by by co-occurrence threshold):\n\npmi_tbl_filtered %&gt;%\n  slice_max(pmi, n = 15) %&gt;%\n  mutate(word = reorder(word, pmi)) %&gt;%\n  ggplot(aes(x = pmi, y = word)) +\n  geom_col() +\n  labs(\n    title = str_glue(\"Top PMI-associated words near '{anchor}' (cooc_n ≥ 3)\"),\n    x = \"PMI (log2 scale)\",\n    y = NULL\n  )\n\n\n\n\n\n\n\n\n\n\nThe PMI analysis begins to reveal some interesting aspects how Adam Smith talks about labor. By comparing the raw co-occurrence counts with the PMI scores, we can distinguish between words that merely appear frequently throughout the text and words that have a genuinely strong association with the concept of labor.\nDivision and organization (“division” - PMI: 13.5, “divisions” - PMI: 13.1, “subdivisions” - PMI: 13.1): The extremely high PMI for “division” confirms something that you might already know about Smith: the concept of “division of labor” is central to his discussion. For Smith, this is a way to understand human cooperation. This fits in nicely with the results for skilled labor (“skilled” - PMI: 13.1): the appearance of “skilled” and phrases related to men’s labor suggests Smith distinguishes between different types of labor and their respective values or capabilities.\nProductivity and capacity (“productive” - PMI: 12.9, “powers” - PMI: 13.0): Smith consistently discusses labor in terms of its productive capacity and the powers or capabilities it possesses.\nCompensation (“rewarded” - PMI: 13.1, “wages” - appears in raw counts): not a surprising result by any means, but Smith does discuss rewards and wages alongside labor.\nIf you have read Smith’s Wealth of Nations, this isn’t particularly surprising… but that’s the point of this exercise. PMI is doing quite well at picking up features of interest in a complex, 18th century economic text.\n\n\n\n\nWord2Vec will keep the same core intuition (meaning from context), but it will learn dense vectors by predicting words from windows rather than just counting them. While PMI gives us valuable insights into which words are strongly associated with our anchor term, it still relies on explicitly counting co-occurrences within fixed windows. Word2Vec takes this same fundamental intuition, but approaches it from a predictive rather than counting framework. Instead of tallying how often words appear together, Word2Vec trains a neural network to predict context words from anchor words (or vice versa), learning dense vector representations in the process. These vectors capture semantic relationships in a continuous space where similar words cluster together, allowing for operations like analogy reasoning (e.g., “king” - “man” + “woman” ≈ “queen”). This approach compresses the kind of distributional information we’ve been exploring with PMI into compact, efficient representations that can capture nuanced semantic relationships across your entire corpus.\nThe table below summarizes how co-occurrence and PMI connect conceptually to Word2Vec. Keep this in mind for Week 7.\n\n\n\n\n\n\n\n\nAspect\nCo-occurrence / PMI\nWord2Vec\n\n\n\n\nUnit of analysis\nWords within fixed context windows\nWords within fixed context windows\n\n\nCore operation\nCounting occurrences and associations\nPredicting words from context (or context from words)\n\n\nRepresentation\nSparse, count-based tables\nDense, learned vectors\n\n\nRole of frequency\nExplicitly corrected (via PMI)\nImplicitly downweighted during training\n\n\nWhat is learned\nAssociation strength between words\nVector representations encoding semantic similarity\n\n\nSemantic intuition\nWords are meaningful if they co-occur unexpectedly\nWords are meaningful if they help predict similar contexts",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 06: Co-occurrence & PMI"
    ]
  },
  {
    "objectID": "week-06-cooccurrence.html#windows-co-occurrence-and-pmi",
    "href": "week-06-cooccurrence.html#windows-co-occurrence-and-pmi",
    "title": "Week 06: Co-occurrence, PMI, and moving towards Word2Vec",
    "section": "",
    "text": "So far, we have been focusing on counting words and other features as a way to represent texts and to compare them to each other. What about modeling meaning? The basic intuition that we are going to work with is that we can learn a lot about the meaning of words by looking at their relational properties. That is:\n\na word’s meaning is constrained by its neighbors (“bank account” vs. “river bank”)\nmeaning emerges from patterns of use\n\nLater on in the semester, we will discuss the limitations of this approach (which some of you have already touched on in class by asking about sarcasm and irony, but there are objections derived from ordinary language philosophy that we need to address), but first we need to learn how to implement this intuition about meaning and to extract as much as we can from it!\nToday’s tutorial is preparing you for the next core concepts in the class:\n\nEmbeddings\nSemantics\nWord2Vec\nBERT-style models\n\nWe are going to do this with one of the foundational texts of modern economics, Adam Smith’s Wealth of Nations (1776). By the standards of this course, this is a positively modern text!\nYou will find the wealth.txt file under files on Canvas.\n\n\nStarting with out intuition that a word’s meaning is constrained by its neighbors, we have to define what we mean by “neighbors.” We are going to work with “windows” as the unit of analysis. The idea is that we place a window of fixed size across tokens and we record which words appear together.\nLet’s get started:\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(tibble)\nlibrary(readr) \nlibrary(tidyr) \nlibrary(tidyverse)\nlibrary(ggplot2)\n\nand let’s keep the same naming pattern we have developed in the previous tutorials:\n\nraw_text &lt;- read_file(\"texts/wealth.txt\")\n\ntexts &lt;- tibble(\n  doc_title = \"wealth.txt\",\n  text = raw_text\n)\n\ntexts\n\n# A tibble: 1 × 2\n  doc_title  text                                                               \n  &lt;chr&gt;      &lt;chr&gt;                                                              \n1 wealth.txt \"An Inquiry into the Nature and Causes of the Wealth of Nations\\r\\…\n\n## Bonus tip: another way to read a text that makes it easier to change later in the code:\n# file_path &lt;- \"wealth.txt\"\n# raw_text &lt;- read_file(file_path)\n\nGreat. Now we need to tokenize the text. Because we want to build windows that move through the text in order, we need to preserve token order (so we do not disregard word/token order the way we did in bag of words approaches).\n\ntokens &lt;- texts %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  group_by(doc_title) %&gt;%\n  mutate(token_id = row_number()) %&gt;%\n  ungroup()\n\n#Let's take a quick look \ntokens %&gt;% slice(1:20)\n\n# A tibble: 20 × 3\n   doc_title  word         token_id\n   &lt;chr&gt;      &lt;chr&gt;           &lt;int&gt;\n 1 wealth.txt an                  1\n 2 wealth.txt inquiry             2\n 3 wealth.txt into                3\n 4 wealth.txt the                 4\n 5 wealth.txt nature              5\n 6 wealth.txt and                 6\n 7 wealth.txt causes              7\n 8 wealth.txt of                  8\n 9 wealth.txt the                 9\n10 wealth.txt wealth             10\n11 wealth.txt of                 11\n12 wealth.txt nations            12\n13 wealth.txt by                 13\n14 wealth.txt adam               14\n15 wealth.txt smith              15\n16 wealth.txt contents           16\n17 wealth.txt introduction       17\n18 wealth.txt and                18\n19 wealth.txt plan               19\n20 wealth.txt of                 20\n\n\nFor this tutorial, we are going to try to get a sense for how Adam Smith discusses the term “labor” (side note: for the sake of simplicity, I normalized “labour” to “labor” before providing the file to you). This means that we are going to consider the token “labor” to be our anchor term and we will create windows around it. Important: a window size of 5 means that we are trying to capture 5 tokens on each side of our anchor.\nSelecting an anchor:\n\nanchor &lt;- \"labor\" #this allows you to change this as needed\nwindow_size &lt;- 5\n\n#check how many anchor words are in the text\ntokens %&gt;% filter(word == anchor) %&gt;% count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  1011\n\n\nOK, now we know that there are plenty of our anchor tokens, but we also need to know where they are so that we can find the windows around them.\n\nanchor_hits &lt;- tokens %&gt;%\n  filter(word == anchor) %&gt;%\n  select(doc_title, anchor_id = token_id) # organize by location of anchor position\n\n#Let's take a look at the tibble\nanchor_hits\n\n# A tibble: 1,011 × 2\n   doc_title  anchor_id\n   &lt;chr&gt;          &lt;int&gt;\n 1 wealth.txt        35\n 2 wealth.txt        61\n 3 wealth.txt        74\n 4 wealth.txt        81\n 5 wealth.txt       114\n 6 wealth.txt       147\n 7 wealth.txt       166\n 8 wealth.txt       230\n 9 wealth.txt       487\n10 wealth.txt       521\n# ℹ 1,001 more rows\n\n\nThe next step is the key to our analysis: we begin by pairing each anchor occurrence with every token in the document so that we can calculate distances between them. Once those distances are known, we filter down to the tokens that lie within a fix window around the anchor. So, let’s build the window table!\nPlease make sure that you understand the concepts behind the following block of code:\n\n# Each row in `windows` will be ONE token that appears near ONE anchor occurrence.\n\nwindows &lt;- anchor_hits %&gt;%\n  # Join each anchor occurrence to ALL tokens in the same document.\n  # This is a \"many-to-many\" join because one anchor hit matches many tokens [remember we had a similar set up last week].\n  left_join(tokens, by = \"doc_title\", relationship = \"many-to-many\") %&gt;%\n  \n  # Compute how far each token is from the anchor.\n  # Negative distance = to the LEFT of the anchor; positive = to the RIGHT.\n  mutate(distance = token_id - anchor_id) %&gt;%\n  \n  # Keep only tokens within the window size (±5 tokens).\n  filter(abs(distance) &lt;= window_size) %&gt;%\n  \n  # Remove the anchor word itself (distance 0) so we only keep context words, which is what we are interested in.\n  filter(distance != 0) %&gt;%\n  \n  \n  mutate(\n    window_id = paste0(doc_title, \"_\", anchor_id),\n    anchor_word = anchor\n  ) %&gt;%\n  \n  # Keep only the columns we need for the next steps.\n  select(\n    doc_title,\n    window_id,\n    anchor_word,\n    anchor_id,\n    token_id,\n    distance,\n    word\n  )\n\nwindows %&gt;%\n  arrange(anchor_id, distance) %&gt;%\n  slice(1:20)\n\n# A tibble: 20 × 7\n   doc_title  window_id     anchor_word anchor_id token_id distance word      \n   &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;           &lt;int&gt;    &lt;int&gt;    &lt;int&gt; &lt;chr&gt;     \n 1 wealth.txt wealth.txt_35 labor              35       30       -5 in        \n 2 wealth.txt wealth.txt_35 labor              35       31       -4 the       \n 3 wealth.txt wealth.txt_35 labor              35       32       -3 productive\n 4 wealth.txt wealth.txt_35 labor              35       33       -2 powers    \n 5 wealth.txt wealth.txt_35 labor              35       34       -1 of        \n 6 wealth.txt wealth.txt_35 labor              35       36        1 and       \n 7 wealth.txt wealth.txt_35 labor              35       37        2 of        \n 8 wealth.txt wealth.txt_35 labor              35       38        3 the       \n 9 wealth.txt wealth.txt_35 labor              35       39        4 order     \n10 wealth.txt wealth.txt_35 labor              35       40        5 according \n11 wealth.txt wealth.txt_61 labor              61       56       -5 i         \n12 wealth.txt wealth.txt_61 labor              61       57       -4 of        \n13 wealth.txt wealth.txt_61 labor              61       58       -3 the       \n14 wealth.txt wealth.txt_61 labor              61       59       -2 division  \n15 wealth.txt wealth.txt_61 labor              61       60       -1 of        \n16 wealth.txt wealth.txt_61 labor              61       62        1 chapter   \n17 wealth.txt wealth.txt_61 labor              61       63        2 ii        \n18 wealth.txt wealth.txt_61 labor              61       64        3 of        \n19 wealth.txt wealth.txt_61 labor              61       65        4 the       \n20 wealth.txt wealth.txt_61 labor              61       66        5 principle \n\n\nNow, each row in windows represents one word that appeared near one specific occurrence of the anchor. Next, we can collapse all the context window rows into a single ranked list by adding them up. This will measure how many times any given word appears within ±5 window_size tokens of the anchor across the whole document.\n\ncooc &lt;- windows %&gt;%\n  count(word, sort = TRUE, name = \"cooc_n\")\n\ncooc\n\n# A tibble: 1,085 × 2\n   word    cooc_n\n   &lt;chr&gt;    &lt;int&gt;\n 1 the       1146\n 2 of        1122\n 3 and        400\n 4 in         248\n 5 to         211\n 6 which      166\n 7 wages      158\n 8 produce    149\n 9 a          145\n10 is         142\n# ℹ 1,075 more rows\n\n\nSo you can view cooc_n as a tally of contextual proximity. Note: cooc_n does not tell you whether a word is important! It just tells you that it appears frequently (or not) near the anchor. At this point of the analysis, you can see that very common words (such as “is,” “the,” and “which”) rise to the top of the list. This is a problem as common words end up dominating even when they tell us little about the anchor–this is expected since we are using raw counts.\nYou may want to stop me and complain: well, why didn’t you remove stopwords? Wouldn’t that fix this? This is a reasonable complaint and removing stopwords would help but not solve the issue. Removing stopwords can make the co-occurrence list look more meaningful, but you will still have words that are very common in the document itself appear near any anchor word. We have a better way to solve the common word problem for this particular task…\n\n\n\nWith the raw co-occurrence count we are tracking the words that appear near the anchor most often. What we are going to compute using PMI is the words that appear near the anchor more often than we would expect given how frequently those words occur in the text overall. This makes PMI a better measure of association strength, rather than simple proximity.\nPointwise Mutual Information (PMI) is defined as:\n\\[\n\\mathrm{PMI}(w, a) = \\log_2 \\left( \\frac{P(w, a)}{P(w)\\,P(a)} \\right)\n\\]\nwhere:\n\n\\(P(w,a)\\) is the probability that the word \\(w\\) appears in the anchor’s context window\n\\(P(w)\\) is the overall probability of the word \\(w\\) in the document\n\\(P(a)\\) is the overall probability of the anchor word \\(a\\) in the document\n\nIf PMI is high, the word \\(w\\) appears near the anchor more often than we would expect by chance, given how frequent each word is overall. Reference if you want a deeper dive into PMI, see Appendix J in Jurafsky and Martin.\nFirst, we compute the overall word frequencies in the document, \\(P(w)\\) and \\(P(a)\\):\n\ntotal_tokens &lt;- nrow(tokens)\n\nword_freq &lt;- tokens %&gt;%\n  count(word, name = \"word_n\") %&gt;%\n  mutate(p_word = word_n / total_tokens)\n\nword_freq\n\n# A tibble: 10,228 × 3\n   word      word_n     p_word\n   &lt;chr&gt;      &lt;int&gt;      &lt;dbl&gt;\n 1 0            440 0.00115   \n 2 0d             1 0.00000261\n 3 1            192 0.000502  \n 4 1,000,000      2 0.00000523\n 5 1,001,171      1 0.00000261\n 6 1,101,107      1 0.00000261\n 7 1,200,000      1 0.00000261\n 8 1,214,583      1 0.00000261\n 9 1,243,120      1 0.00000261\n10 1,245,808      1 0.00000261\n# ℹ 10,218 more rows\n\n\nWe now get anchor frequency and probability:\n\nanchor_stats &lt;- word_freq %&gt;%\n  filter(word == anchor) %&gt;%\n  transmute(anchor_n = word_n, p_anchor = p_word)\n\nanchor_stats\n\n# A tibble: 1 × 2\n  anchor_n p_anchor\n     &lt;int&gt;    &lt;dbl&gt;\n1     1011  0.00264\n\n\nGut check: you might (or not) think that these tibbles look weird! At this stage, we are seeing the documents exactly as the computer sees it: a long sequence of tokens. Numbers appear here because Adam Smith uses a lot of numbers (which shouldn’t surprise us!). This gives us a baseline picture of overall frequency, which PMI will later use to decide whether a word’s appearance near the anchor is genuinely informative or merely a byproduct of being common everywhere.\nNow we need to compute \\(P(w,a)\\). We define the sample space as all tokens that appear inside any widow around the anchor word, excluding the anchor tokens themselves. You can think of this as the complete collection of all context words around the anchor in The Wealth of Nations. Then \\(P(w,a)\\) is the probability that, if we randomly select one token from all the tokens that occur inside anchor windows, that token is the word \\(w\\).\nIn practice, this means that we do the following:\n\nWe count how many times 𝑤 appears inside anchor windows.\nWe divide by the total number of tokens across all anchor windows.\nThe result is \\(P(w,a)\\).\n\nThis definition allows us to treat co-occurrence as a probabilistic event rather than just a raw count.\n\ntotal_window_tokens &lt;- nrow(windows)\n\np_w_given_windows &lt;- windows %&gt;%\n  count(word, name = \"cooc_n\") %&gt;%\n  mutate(p_word_in_windows = cooc_n / total_window_tokens)\n\np_w_given_windows\n\n# A tibble: 1,085 × 3\n   word     cooc_n p_word_in_windows\n   &lt;chr&gt;     &lt;int&gt;             &lt;dbl&gt;\n 1 1740          1         0.0000989\n 2 30            1         0.0000989\n 3 a           145         0.0143   \n 4 able          3         0.000297 \n 5 about         4         0.000396 \n 6 above         6         0.000593 \n 7 abridge       7         0.000692 \n 8 abridged      1         0.0000989\n 9 abridges      1         0.0000989\n10 absorbed      1         0.0000989\n# ℹ 1,075 more rows\n\n\nThis is effectively P(w| in anchor windows), which is proportional to \\(P(w,a)\\) for our purposes.\nNext, we join baseline frequencies and compute PMI\n\npmi_tbl &lt;- p_w_given_windows %&gt;%\n  left_join(word_freq, by = \"word\") %&gt;%\n  mutate(\n    pmi = log2(p_word_in_windows / (p_word * anchor_stats$p_anchor))\n  ) %&gt;%\n  arrange(desc(pmi))\n\npmi_tbl\n\n# A tibble: 1,085 × 6\n   word        cooc_n p_word_in_windows word_n     p_word   pmi\n   &lt;chr&gt;        &lt;int&gt;             &lt;dbl&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 tenfold          2         0.000198       1 0.00000261  14.8\n 2 abridges         1         0.0000989      1 0.00000261  13.8\n 3 ayrshire         1         0.0000989      1 0.00000261  13.8\n 4 carron           1         0.0000989      1 0.00000261  13.8\n 5 complexly        2         0.000198       2 0.00000523  13.8\n 6 conceives        1         0.0000989      1 0.00000261  13.8\n 7 declamation      1         0.0000989      1 0.00000261  13.8\n 8 differs          1         0.0000989      1 0.00000261  13.8\n 9 fashioning       1         0.0000989      1 0.00000261  13.8\n10 fattened         1         0.0000989      1 0.00000261  13.8\n# ℹ 1,075 more rows\n\n\nThis should start to give us a clearer picture! However, we do have to worry about rare words. PMI can become misleadingly large when cooc_n is very small (e.g., a word appears once in a window). A standard practice is to filter by minimum co-occurrence count.\n\npmi_tbl_filtered &lt;- pmi_tbl %&gt;%\n  filter(cooc_n &gt;= 3) %&gt;%        # adjust threshold as needed\n  arrange(desc(pmi))\n\npmi_tbl_filtered\n\n# A tibble: 423 × 6\n   word         cooc_n p_word_in_windows word_n    p_word   pmi\n   &lt;chr&gt;         &lt;int&gt;             &lt;dbl&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 abridge           7          0.000692      8 0.0000209  13.6\n 2 men’s             7          0.000692      8 0.0000209  13.6\n 3 hour’s            4          0.000396      5 0.0000131  13.5\n 4 division         51          0.00504      65 0.000170   13.5\n 5 divisions         3          0.000297      5 0.0000131  13.1\n 6 rewarded          6          0.000593     10 0.0000261  13.1\n 7 skilled           3          0.000297      5 0.0000131  13.1\n 8 subdivisions      3          0.000297      5 0.0000131  13.1\n 9 powers           19          0.00188      34 0.0000889  13.0\n10 productive       95          0.00940     175 0.000457   12.9\n# ℹ 413 more rows\n\n\nTake a minute to compare the first words in pmi_tbl_filtered with pmi_tbl. Which words rise to the top under PMI that did not dominate raw co-occurrence? What kinds of words does PMI “reward” and what kinds does it “punish”?\nFinally, we can visually compare raw co-occurrence:\n\ncooc %&gt;%\n  slice_max(cooc_n, n = 15) %&gt;%\n  mutate(word = reorder(word, cooc_n)) %&gt;%\n  ggplot(aes(x = cooc_n, y = word)) +\n  geom_col() +\n  labs(\n    title = str_glue(\"Top co-occurring words within ±{window_size} of '{anchor}'\"),\n    x = \"Co-occurrence count\",\n    y = NULL\n  )\n\n\n\n\n\n\n\n\nand top PMI (filtered by by co-occurrence threshold):\n\npmi_tbl_filtered %&gt;%\n  slice_max(pmi, n = 15) %&gt;%\n  mutate(word = reorder(word, pmi)) %&gt;%\n  ggplot(aes(x = pmi, y = word)) +\n  geom_col() +\n  labs(\n    title = str_glue(\"Top PMI-associated words near '{anchor}' (cooc_n ≥ 3)\"),\n    x = \"PMI (log2 scale)\",\n    y = NULL\n  )\n\n\n\n\n\n\n\n\n\n\nThe PMI analysis begins to reveal some interesting aspects how Adam Smith talks about labor. By comparing the raw co-occurrence counts with the PMI scores, we can distinguish between words that merely appear frequently throughout the text and words that have a genuinely strong association with the concept of labor.\nDivision and organization (“division” - PMI: 13.5, “divisions” - PMI: 13.1, “subdivisions” - PMI: 13.1): The extremely high PMI for “division” confirms something that you might already know about Smith: the concept of “division of labor” is central to his discussion. For Smith, this is a way to understand human cooperation. This fits in nicely with the results for skilled labor (“skilled” - PMI: 13.1): the appearance of “skilled” and phrases related to men’s labor suggests Smith distinguishes between different types of labor and their respective values or capabilities.\nProductivity and capacity (“productive” - PMI: 12.9, “powers” - PMI: 13.0): Smith consistently discusses labor in terms of its productive capacity and the powers or capabilities it possesses.\nCompensation (“rewarded” - PMI: 13.1, “wages” - appears in raw counts): not a surprising result by any means, but Smith does discuss rewards and wages alongside labor.\nIf you have read Smith’s Wealth of Nations, this isn’t particularly surprising… but that’s the point of this exercise. PMI is doing quite well at picking up features of interest in a complex, 18th century economic text.\n\n\n\n\nWord2Vec will keep the same core intuition (meaning from context), but it will learn dense vectors by predicting words from windows rather than just counting them. While PMI gives us valuable insights into which words are strongly associated with our anchor term, it still relies on explicitly counting co-occurrences within fixed windows. Word2Vec takes this same fundamental intuition, but approaches it from a predictive rather than counting framework. Instead of tallying how often words appear together, Word2Vec trains a neural network to predict context words from anchor words (or vice versa), learning dense vector representations in the process. These vectors capture semantic relationships in a continuous space where similar words cluster together, allowing for operations like analogy reasoning (e.g., “king” - “man” + “woman” ≈ “queen”). This approach compresses the kind of distributional information we’ve been exploring with PMI into compact, efficient representations that can capture nuanced semantic relationships across your entire corpus.\nThe table below summarizes how co-occurrence and PMI connect conceptually to Word2Vec. Keep this in mind for Week 7.\n\n\n\n\n\n\n\n\nAspect\nCo-occurrence / PMI\nWord2Vec\n\n\n\n\nUnit of analysis\nWords within fixed context windows\nWords within fixed context windows\n\n\nCore operation\nCounting occurrences and associations\nPredicting words from context (or context from words)\n\n\nRepresentation\nSparse, count-based tables\nDense, learned vectors\n\n\nRole of frequency\nExplicitly corrected (via PMI)\nImplicitly downweighted during training\n\n\nWhat is learned\nAssociation strength between words\nVector representations encoding semantic similarity\n\n\nSemantic intuition\nWords are meaningful if they co-occur unexpectedly\nWords are meaningful if they help predict similar contexts",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 06: Co-occurrence & PMI"
    ]
  },
  {
    "objectID": "week-04-representation.html",
    "href": "week-04-representation.html",
    "title": "Week 04: Text Representation (1)",
    "section": "",
    "text": "Document-Feature Matrices\nIn Week 3, we began to compare two of Misselden’s documents through sentiment analysis around trade terminology. In class on Wednesday, we discussed measures of similarity between texts and what a Document-Feature Matrix (DFM) looks like. Let’s implement what we learned in class by comparing the two Misselden’s texts to “mystery” text A06785.txt.\nTo create a DFM of the three texts, we will use the quanteda package. We start by loading the packages, reading the texts, and combining into a quanteda corpus named corp.\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\n\ntxt_circle    &lt;- read_file(\"texts/A07594__Circle_of_Commerce.txt\")\ntxt_free      &lt;- read_file(\"texts/B14801__Free_Trade.txt\")\ntxt_third     &lt;- read_file(\"texts/A06785.txt\")\n\ntexts &lt;- c(\n  \"Circle_of_Commerce\" = txt_circle,\n  \"Free_Trade\"         = txt_free,\n  \"Third_Text_A06785\"  = txt_third\n)\n\ncorp &lt;- corpus(texts)\n\nWe then tokenize and do some basic cleaning (most of this should be very familiar by now):\n\ntoks &lt;- tokens(\n  corp,\n  remove_punct   = TRUE,\n  remove_numbers = TRUE,\n  remove_symbols = TRUE\n)\n\ntoks &lt;- tokens_tolower(toks)\n\ncustom_stop &lt;- c(\n  \"vnto\",\"haue\",\"doo\",\"hath\",\"bee\",\"ye\",\"thee\",\"hee\",\"shall\",\"hast\",\"doe\",\n  \"beene\",\"thereof\",\"thus\" \n)\n\ntoks &lt;- tokens_remove(toks, pattern = c(stopwords(\"en\"), custom_stop))\n\nThe choices that we made when tokenizing do matter! Note that I doing the following:\n\nI am removing punctuation. With this decision, I am asserting that for this analysis, punctuation doesn’t matter (so I want to treat “commerce.” = “commerce!” = “commerce”). You may disagree! If were were doing an analysis of style, then punctuation would matter a lot.\nI am removing numbers. Since these are mercantile texts (including the “mystery” one), it may seems weird to remove potential economic data. My choice is based on a concern for distracting or inaccurate numbers (such as pagination or rounded guesses about population and trade estimates). Again, you may disagree and want to keep them (that’s an easy fix in the code above).\nI am removing symbols. This goes hand-in-hand with the choice about numbers. Most symbols are going to be transcription/OCR errors. The only (likely) valuable ones are currency markers. Since I opted to remove numbers, I decided to remove symbols.\n\nAs I have taken pains to demonstrate, these are all decisions that you can change. But you have to have a principled reason to do so!\nNow we can build the DFM using quanteda’s dfm() and inspect it to make sure that it looks reasonable:\n\n# Document-feature matrix (DFM)\ndfm_mat &lt;- dfm(toks)\n\n# Inspect by raw count (in our corpus) the top 25 features \ndfm_mat\n\nDocument-feature matrix of: 3 documents, 9,083 features (56.93% sparse) and 0 docvars.\n                    features\ndocs                 circle commerce prooeme herodotvs clio reportes croesvs\n  Circle_of_Commerce     14       25       1         1    2        1       1\n  Free_Trade              1       22       0         0    0        0       0\n  Third_Text_A06785      41       31       0         0    0        0       0\n                    features\ndocs                 king lydia ſonne\n  Circle_of_Commerce   35     1     5\n  Free_Trade           53     0     0\n  Third_Text_A06785    36     0     0\n[ reached max_nfeat ... 9,073 more features ]\n\ntopfeatures(dfm_mat, 25)\n\n      trade    exchange commodities       money   merchants         may \n        602         478         293         290         282         272 \n        one      moneys       great    kingdome         now        made \n        268         267         214         188         187         186 \n       said          ſo        much           p     malynes       cloth \n        186         184         183         170         162         162 \n      value     without        time       price        upon         man \n        150         148         147         146         145         142 \n    whereby \n        133 \n\n\nOK, so far so good. This is a sparse matrix and the top features seem quite plausible. We could also use this as a way to (potentially) remove some more stop words (I am not sure that “one” or “now” are giving us much information). I am going to keep things as they are. In the aggregate, stopwords won’t be a problem for what we are doing today (this is an empirical statement based on experience; I present no evidence for this here), and we are going to have a better way to get rid of overly common terms in the next section.\n\nFor your entertainment: try playing around with stopwords and difference choices in the tokenization step.\n\nTo compare the documents, we are going to use the second new library we introduced today: quanteda.textstats. It will gives us a number of ways to compare our three texts. We are going to try two of them and compare the results.\n\nFirst:\nLet’s start with a correlation measure. We are going to treat each document as a word frequency vector. So, if we have N words in our corpus, each document is represented by an N-dimensional vector based on the frequency of each word within that text. The vectors are simply the rows of the DFM (where each row is a document).\nWhat we are going to measure in this step is the pairwise Pearson correlation between the vectors across all the word features (the columns of the DFM).\n\n# correlation similarity\nsim_cor &lt;- textstat_simil(\n  dfm_mat,\n  method = \"correlation\",\n  margin = \"documents\"\n)\nsim_cor\n\ntextstat_simil object; method = \"correlation\"\n                   Circle_of_Commerce Free_Trade Third_Text_A06785\nCircle_of_Commerce              1.000      0.569             0.505\nFree_Trade                      0.569      1.000             0.441\nThird_Text_A06785               0.505      0.441             1.000\n\n\n\n\nSecond:\nNow that we understand how each document is represented at a vector, we can measure the distance between them as vectors by measuring the angle between them. We do this using cosine similarity (as discussed in class). We just have to change the method in textstat_simil().\n\nsim_cos &lt;- textstat_simil(\n  dfm_mat,\n  method = \"cosine\",\n  margin = \"documents\"\n)\n\nsim_cos\n\ntextstat_simil object; method = \"cosine\"\n                   Circle_of_Commerce Free_Trade Third_Text_A06785\nCircle_of_Commerce              1.000      0.596             0.533\nFree_Trade                      0.596      1.000             0.471\nThird_Text_A06785               0.533      0.471             1.000\n\n\nIn both cases, we (reassuringly) got 1.000 down the diagonal (each text is perfectly similar to itself) and the matrices are symmetric (can you see why this should be the case?). We also notice that the two texts by Misselden are more similar to each other than to the “mystery” text in both measures. This is also good news since the third text is by a different author.\nThe third text is Gerard Malynes’s The Center of the Circle of Commerce (1623). Malynes and Misselden disagree over governmental intervention in trade, and their works address each other’s arguments.\n\nQuestion for you: given that Malynes’s work is closely related to Misselden’s what do you think of the measures of similarity we just computed? As you think about this, remind yourself of how the DFM is set up.\n\n\n\n\nTF-IDF:\nNot all words are created equal in a document. We know this because we remove stopwords and we even create custom stopwords to account for idiosyncratically common words. This can be due to historical context or to technical, but non-characteristic language. For example, my research is on 17th century Puritan sermons and the use of the word “Scripture” is not a particularly distinguishing feature of these texts. But, as I say of many things in this class, this is a value judgement and dependent on technique. It may turn out that keeping a common word such as “Scripture” in my corpus would actually produce better results. By making these decisions explicit in a stopwords list, we can always retrace our steps.\nThe downside of relying on stopwords is that, in a sense, we have to take educated guesses at which words don’t carry enough discriminating meaning between texts for our corpus. This is where TF-IDF comes in.\nThere are two components to TF-IDF:\n\nTerm frequency (TF), which captures how often a word appears in a document (what we have been counting when setting up the DFM)\nInverse Document Frequency (IDF), which measures how rare a word is across the corpus. A word that appears in only a few documents will have high IDF. This means that a word with high IDF can be a distinguishing feature for a document. \\(\\text{IDF} = \\log\\left(\\frac{\\text{number of total docs}}{\\text{number of docs with term}}\\right)\\)\n\nNote: quanteda gives you the option to use either natural or base 10 log. The default is base 10. See the full documentation here. In some other applications, you will see a preference for natural logarithm. The difference between the two is really irrelevant (it’s easy to apply a change of base) and what matters is the compression. What the log does: it keeps the monotonic property of IDF (rarer words get higher IDF values), but it won’t let something like a typo in one document dominate.\n\n\nTF-IDF is just the product of TF with IDF. [Note: we should really say TF-IDF(t,d), that is, for term t in document d, but the short-hand skips that parenthetical.] Quanteda will do all the work for us.\nLet’s go ahead and see how TF-IDF changes our DFM matrix:\n\ndfm_tfidf &lt;- dfm_tfidf(dfm_mat)\n\ndfm_tfidf\n\nDocument-feature matrix of: 3 documents, 9,083 features (56.93% sparse) and 0 docvars.\n                    features\ndocs                 circle commerce   prooeme herodotvs      clio  reportes\n  Circle_of_Commerce      0        0 0.4771213 0.4771213 0.9542425 0.4771213\n  Free_Trade              0        0 0         0         0         0        \n  Third_Text_A06785       0        0 0         0         0         0        \n                    features\ndocs                   croesvs king     lydia    ſonne\n  Circle_of_Commerce 0.4771213    0 0.4771213 2.385606\n  Free_Trade         0            0 0         0       \n  Third_Text_A06785  0            0 0         0       \n[ reached max_nfeat ... 9,073 more features ]\n\ntopfeatures(dfm_tfidf, 20)\n\n      ſo     upon   moneys  foreign     alſo       us     unto kingdome \n87.79031 69.18258 47.01637 41.50955 37.21546 34.82985 33.39849 33.10516 \n    ſuch     said  becauſe    realm    theſe ballance forraine    thoſe \n32.92137 32.75297 31.96712 30.53576 29.58152 29.10440 29.10440 28.62728 \n    muſt  malynes     loss  balance \n28.62728 28.52678 28.15015 24.81031 \n\n#Note: so far, dfm_tfidf is a quanteda object that has a compact storage format. If we want to use it with base R, we need to change it into a general R matrix by doing the following: \n\ntfidf_mat &lt;- as.matrix(dfm_tfidf)\n\nYou can think of tfidf_mat as a matrix where the rows are the documents, the columns are the terms in the corpus, and the values are the TF-IDF weights (instead of the raw counts). Now we can extract the words with the top TF_IDF values for each text:\n\n# Circle of Commerce\ncircle_tfidf &lt;- tfidf_mat[\"Circle_of_Commerce\", ]\n\n# Sort and get top 20\ntop_circle &lt;- sort(circle_tfidf, decreasing = TRUE)[1:20]\ntop_circle\n\n      ſo     alſo     ſuch  becauſe    theſe ballance forraine    thoſe \n87.79031 37.21546 32.92137 31.96712 29.58152 29.10440 29.10440 28.62728 \n    muſt  malynes     ſame    ſhall     ſome    firſt    leſſe     mony \n28.62728 27.82242 24.33318 22.42470 21.94758 20.99334 20.99334 20.42659 \n     vſe kingdome   ſhould himſelfe \n20.03909 19.89831 19.08485 19.08485 \n\n# Free Trade\nfree_tfidf &lt;- tfidf_mat[\"Free_Trade\", ]\n\n# Sort and get top 20\ntop_free &lt;- sort(free_tfidf, decreasing = TRUE)[1:20]\ntop_free\n\n    maiesties  christendome      kingdome           vse          also \n    17.653486     14.313638     13.206844     12.882274     12.150297 \n        selfe          vpon      maiestie         seeme common-wealth \n    10.496668     10.037202     10.019546      8.588183      8.452380 \n       causes      forreine    gouernment    domestique      encrease \n     7.924107      7.748015      7.395833      7.156819      6.679698 \n        vsury      speciall       iustice     non-latin      alphabet \n     6.679698      6.202576      6.202576      5.987103      5.987103 \n\n# A06786\nA06785_tfidf &lt;- tfidf_mat[\"Third_Text_A06785\", ]\n\n# Sort and get top 20\ntop_A06785 &lt;- sort(A06785_tfidf, decreasing = TRUE)[1:20]\ntop_A06785\n\n          upon         moneys        foreign             us           unto \n      69.18258       46.48809       41.50955       34.82985       33.39849 \n         realm           said           loss        balance     merchant's \n      30.53576       30.28770       28.15015       24.81031       24.81031 \n             s         silver         native        stivers      misselden \n      20.25049       19.08485       19.08485       17.65349       17.17637 \n          coin undervaluation  overbalancing       enhanced         weight \n      17.17637       17.17637       17.17637       15.74500       15.74500 \n\n\n\nA technical R note: here tfidf_mat[\"Circle_of_Commerce\", ] were are subsetting the matrix tfidf_mat using [row, column] notation. I am asking R to go to the row named “Circle_of_Commerce” from the matrix and grab all of the columns (the blank space after the comma). For a reference on how to subset data in R, go here. To test yourself, what would you expect to see if you were to print the result of the following? (You have all the information you need on this page!)\nsubset_test &lt;- tfidf_mat[\"Circle_of_Commerce\", c(\"malynes\", \"forraine\", \"ballance\")]\n\nWe can now visualize the most characteristic terms for each document:\n\ntfidf_top_tbl &lt;- bind_rows(\n  tibble(document = \"Circle of Commerce\", term = names(top_circle), tfidf = unname(top_circle)),\n  tibble(document = \"Free Trade\",         term = names(top_free),   tfidf = unname(top_free)),\n  tibble(document = \"Third Text\",         term = names(top_A06785),  tfidf = unname(top_A06785))\n)\n\nggplot(tfidf_top_tbl, aes(x = tfidf, y = reorder(term, tfidf))) +\n  geom_col() +\n  facet_wrap(~ document, scales = \"free_y\") +\n  labs(\n    title = \"Most Characteristic Terms by Document (TF–IDF)\",\n    x = \"TF–IDF score\",\n    y = NULL\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWARNING and a task for you:\nThe visualization above isn’t quite right. I don’t mean that it’s technically wrong: the code is correct and the visualization is displaying the correct terms for each document. However, I made some choices along the way that resulted in some less than illuminating results.\n\nWhat would you change? Why? Hint: think through the discussion about how TF-IDF is computed and what it captures.\nHow would you change it? Hint: you may want to review the lesson from week 3.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 04: Text Representation (1)"
    ]
  },
  {
    "objectID": "week-02-basics.html#weeks-goals",
    "href": "week-02-basics.html#weeks-goals",
    "title": "Week 02: Basics",
    "section": "Week’s Goals:",
    "text": "Week’s Goals:\nThis week, we compare two early modern economic texts by examining word frequencies and bigrams. Our two texts are by Edward Misselden, one of the major Early Modern mercantilists whose work we will be analyzing. The goal is for you to become familiar with how to start implementing NLP workflows in RStudio. The two texts are available on Canvas, under files in the folder named Text Files (note: there are other files in there which we will use later on in the term).\nNote: this will be the standard set up going forward. Sample code will be here and files will be on Canvas, unless otherwise specified.\n\nOur guiding questions will be: how does Misselden’s language about trade change between 1622 and 1623? This is obviously an artificially simplistic question at this stage, but it will help us explore some useful NLP methods. To begin answering this question, we will compare word frequency between two of his works: Free Trade (London, 1622) and The Circle of Commerce (London, 1623). We will also look at bigram frequencies as a preview of the longer discussion of N-grams next week.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#what-you-need",
    "href": "week-02-basics.html#what-you-need",
    "title": "Week 02: Basics",
    "section": "What you need:",
    "text": "What you need:\nI am assuming that you will be working inside an RStudio Project (recommended). We will read two plain-text files, tidy the tokens/bigrams, and compare frequency patterns. You will need:\n\nA project folder that contains all of your files (try to have a consistent folder organization throughout the semester).\nThe plain-text files on Canvas.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#a-word-of-warning-about-training-wheels",
    "href": "week-02-basics.html#a-word-of-warning-about-training-wheels",
    "title": "Week 02: Basics",
    "section": "A word of warning about training wheels:",
    "text": "A word of warning about training wheels:\nBecause this class doesn’t assume familiarity with R, I am giving extended explanations of the code during this first session (including tips on how to set up your directories and extensive links to R documentation). As the semester proceeds, I will assume that you are gaining confidence and familiarity with R and Rstudio, and the training wheels will slowly come off. Take advantage of the slower pace at the start of the semester to set yourself up for the harder weeks to come!\nThis week I will also show the results of each step of code by printing after each step. This will make (a bit more) explicit how each chunk of code modifies our data. You will be expected to do this more and more on your own starting next week.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#step-by-step-from-raw-texts-to-comparison-plots",
    "href": "week-02-basics.html#step-by-step-from-raw-texts-to-comparison-plots",
    "title": "Week 02: Basics",
    "section": "Step-by-step: from raw texts to comparison plots",
    "text": "Step-by-step: from raw texts to comparison plots",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#setup",
    "href": "week-02-basics.html#setup",
    "title": "Week 02: Basics",
    "section": "Setup",
    "text": "Setup\nAt the beginning of each R file, you will want to call all the packages needed. This will look like:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(tibble)\nlibrary(scales)\n\nI am storing my text files in a directory named texts/ inside my project: I strongly suggest doing the same to make your life easier.\nThe next section of code demonstrates how to read the files and to organize them into a type of dataframe that is well suited for Tidy work: a tibble. For this week, I am using generic “file_a” and “file_b” names to simplify the rather complex original file names. We will discuss the naming conventions of these files as well as how to deal with XML files (the original format of Misselden’s texts) in the next few weeks, but for now, I want you to focus on getting them into the appropriate format for analysis.\n\n# You will need the correct file paths if you don't follow my naming conventions:\nfile_a &lt;- \"texts/A07594__Circle_of_Commerce.txt\"\nfile_b &lt;- \"texts/B14801__Free_Trade.txt\"\n\n# Read the raw text files into R\ntext_a &lt;- read_file(file_a)\ntext_b &lt;- read_file(file_b)\n\n# Combine into a tibble for tidytext workflows\ntexts &lt;- tibble(\n  doc_title = c(\"Text A\", \"Text B\"),\n  text = c(text_a, text_b)\n)\n\ntexts\n\n# A tibble: 2 × 2\n  doc_title text                                                                \n  &lt;chr&gt;     &lt;chr&gt;                                                               \n1 Text A    THE CIRCLE OF COMMERCE. The Prooeme. HERODOTVS in his CLIO, reporte…\n2 Text B    CAP. I. The Causes of the want of Money in England. IT hauing pleas…\n\n\nAs you can see from the .txt files, these are Early Modern texts with (at least some) of the original orthography. In reality, these have already been partially cleaned as I have accessed them from EarlyPrint (we will discuss different sources of texts, historical and otherwise) as you start thinking about your term project.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#tokenization-stopwords-and-counting-words",
    "href": "week-02-basics.html#tokenization-stopwords-and-counting-words",
    "title": "Week 02: Basics",
    "section": "Tokenization, stopwords, and counting words:",
    "text": "Tokenization, stopwords, and counting words:\nWe will turn each text into a table of one word per row, this is the tokenization process in Tidy. Tokens are the basic unit of text that we are working on and the process of tokenization “breaks down” the text into these units for computational analysis. For right now, we are going to consider each word to be a token, but we could break up the text into characters or parts of words if needed. We will then we remove stopwords (words like “the” or “and” that are not useful for this analysis) so that the remaining words are more meaningful for comparison.\nWe will do this in two steps:\n\nStopwords: we will create an “all stopwords” list by combining tidytext’s built-in list with our own, corpus-specific, list. Our list is going to be a one-column tibble to match the format of the built-in stopwords:\n\n\n# Start with tidytext's built-in stopword list\ndata(\"stop_words\")\n\n# Add our own project-specific stopwords (you can, and will, expand this list later)\ncustom_stopwords &lt;- tibble(\n  word = c(\n    \"vnto\", \"haue\", \"doo\", \"hath\", \"bee\", \"ye\", \"thee\"\n  )\n)\n\nall_stopwords &lt;- bind_rows(stop_words, custom_stopwords) %&gt;%\n  distinct(word)\n\nall_stopwords %&gt;% slice(1:10)\n\n# A tibble: 10 × 1\n   word       \n   &lt;chr&gt;      \n 1 a          \n 2 a's        \n 3 able       \n 4 about      \n 5 above      \n 6 according  \n 7 accordingly\n 8 across     \n 9 actually   \n10 after      \n\n\nNext we are going to create a function that tokenizes the texts, removes both standard and custom stopwords, and then counts which words appear most frequently in each document. As you look over this section, you will want to become familiar with the pipe in R (%&gt;%).\n\ntexts is a tibble where each row corresponds to a document;\nunnest_tokens(word, text): splits the text column into individual word tokens, creates a new column called word, and expands the tibble so each row is now one word occurrence\nmutate: here we are normalizing all words to lower case. This is a research decision: I want to count words like “Money,” “MONEY,” and “money” as the same token, so that I can ask how often the texts talk about “money” regardless of capitalization. Note: mutate is part of the dplyr package.\nanti_join: this is where we remove the stopwords we defined earlier and keeps only the tokens that should (we think!) give us the information we want. Look over the information on joins here and make sure that you understand what it’s doing and understand why the tibble format is important! We will be using anti_join frequently.\nFinally we count.\n\n\nword_counts &lt;- texts %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  mutate(word = str_to_lower(word)) %&gt;%\n  anti_join(all_stopwords, by = \"word\") %&gt;%\n  count(doc_title, word, sort = TRUE)\n\nword_counts\n\n# A tibble: 7,438 × 3\n   doc_title word            n\n   &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;\n 1 Text A    trade         233\n 2 Text A    exchange      186\n 3 Text B    trade         185\n 4 Text A    ſo            184\n 5 Text A    malynes       158\n 6 Text A    merchants     126\n 7 Text A    mony          118\n 8 Text A    hee           115\n 9 Text A    kingdome      113\n10 Text A    commodities    96\n# ℹ 7,428 more rows\n\n\nNote: unnest_tokes(word, text) removes punctuation and lowercases automatically. You can a try couple of options to solve this issue. The first is to change a couple of parameters in the function: unnest_tokens(word, text, strip_punct = FALSE) to keep the punctuation and unnest_tokens(word, text, to_lower = FALSE) to keep upper cases. What if you want to keep only certain symbols and types of punctuation? We will discuss this at the beginning of Week 3.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#comparing-word-frequencies-across-texts",
    "href": "week-02-basics.html#comparing-word-frequencies-across-texts",
    "title": "Week 02: Basics",
    "section": "Comparing word frequencies across texts",
    "text": "Comparing word frequencies across texts\nLet’s compare how individual words differ across texts after stopword removal through a first visualization. To keep the visualization readable, we focus on the top 20 most frequent words overall. For now, we won’t get into all the details of the visualization code; we will focus on the main components.\nThe goal of this visualization is a side-by-side comparison plot showing the top 20 most frequent words across the texts with one facet per text. Note: these are the same words for both texts. This means that we are creating a direct comparison of how Misselden uses the most frequent terms.\n\nWe start by putting word_count in the correct format with pivot_wider: each word will have a column for each document’s count.\nvalues_fill = 0: if a word never appears in a text, it gets a 0 instead of NA.\nNext, we rank words based on this criterion: for each word, look at how often it appears in Text A and how often it appears in Text B, and keep the largest count.\n\nThis is what is achieved by computing the row-wise maximum with pmax and then defining the vector: max_n = pmax(TextA, TextB).\nmutate() adds a column to word_comparison_tbl (the wide version of word_count we are defining in this block) based on how we just defined max_n (that is: it takes the vector and attaches it as a column to the tibble).\narrange(desc(max_n) sorts the words from most to least frequent based on max_n.\n\n\n\nStop and regroup:\nI have just implicitly (sneakily) introduced some new things and you might find it helpful to take a look at this introduction to data structures. We will reinforce this topic as we go so don’t panic if this is new to you!\n\n\nBack to the code:\nWe have produced a wide, ranked comparison table. We want to turn into a tidy table that can easily be plotted. To do this we need three main steps:\n\nselect the top 20 words to plot (based on our ranking criterion above). We will do this with slice_head(n = plot_n_words), which takes the first plot_n_words of word_comparison_tbl.\nreshape from wide to long to please ggplot: we will talk about ggplot more in the future, but, for now, we can treat it as a blackbox that produces nice visualizations.\norder the words for the plot with mutate(word = fct_reorder(word, n, .fun = max)). Note: here too, I am asking you to take this as a blackbox for now. What this step is trying to solve is the problem (for us) that a character vector has no inherent order.\n\nFinally we plot!\n\nplot_n_words &lt;- 20  # you can change this as needed\n\n# Select the most frequent words overall\nword_comparison_tbl &lt;- word_counts %&gt;%\n  pivot_wider(\n    names_from = doc_title,\n    values_from = n,\n    values_fill = 0\n  ) %&gt;%\n  mutate(max_n = pmax(`Text A`, `Text B`)) %&gt;%\n  arrange(desc(max_n))\n\nword_plot_data &lt;- word_comparison_tbl %&gt;%\n  slice_head(n = plot_n_words) %&gt;%\n  pivot_longer(\n    cols = c(`Text A`, `Text B`),\n    names_to = \"doc_title\",\n    values_to = \"n\"\n  ) %&gt;%\n  mutate(word = fct_reorder(word, n, .fun = max))\n\nggplot(word_plot_data, aes(x = n, y = word)) + #black magic happens thanks to ggplot\n  geom_col() +\n  facet_wrap(~ doc_title, scales = \"free_x\") +\n  labs(\n    title = \"Most frequent words (stopwords removed)\",\n    subtitle = paste0(\n      \"Top \", plot_n_words,\n      \" words by maximum frequency across both texts\"\n    ),\n    x = \"Word frequency\",\n    y = NULL\n  ) +\n  theme_minimal()",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#bigrams-starting-to-think-about-context",
    "href": "week-02-basics.html#bigrams-starting-to-think-about-context",
    "title": "Week 02: Basics",
    "section": "Bigrams: starting to think about context",
    "text": "Bigrams: starting to think about context\nSingle-word frequencies tell us which terms are common, but they don’t tell us about the context of these words. Our next step will be to try to get a first, basic understanding of how words in our texts fit together. Bigrams allow us to see which words appear together, capturing short phrases and recurring ideas. This is also our first step towards exploring formulaic language,and discursive patterns, rather than just isolated vocabulary.\n\nYou should be able to understand the syntax of this first step (make sure that you do!):\n\n\nbigrams &lt;- texts %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\nbigrams\n\n# A tibble: 53,816 × 2\n   doc_title bigram           \n   &lt;chr&gt;     &lt;chr&gt;            \n 1 Text A    the circle       \n 2 Text A    circle of        \n 3 Text A    of commerce      \n 4 Text A    commerce the     \n 5 Text A    the prooeme      \n 6 Text A    prooeme herodotvs\n 7 Text A    herodotvs in     \n 8 Text A    in his           \n 9 Text A    his clio         \n10 Text A    clio reportes    \n# ℹ 53,806 more rows\n\n\nCurrently, each bigram is stored as a single string. We want to remove stopwords (using the custom list we created earlier). In order to do that, we need to be able to inspect each word separately in the bigram. separate() does just that! It takes the bigram column and splits each string at the space character. It then creates two new columns: “word1” and “word2.”\n\nbigrams_separated &lt;- bigrams %&gt;%\n  separate(bigram, into = c(\"word1\", \"word2\"), sep = \" \")\n\nbigrams_separated\n\n# A tibble: 53,816 × 3\n   doc_title word1     word2    \n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    \n 1 Text A    the       circle   \n 2 Text A    circle    of       \n 3 Text A    of        commerce \n 4 Text A    commerce  the      \n 5 Text A    the       prooeme  \n 6 Text A    prooeme   herodotvs\n 7 Text A    herodotvs in       \n 8 Text A    in        his      \n 9 Text A    his       clio     \n10 Text A    clio      reportes \n# ℹ 53,806 more rows\n\n\n\nNow we can remove all_stopwords\n\n\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(\n    !word1 %in% all_stopwords$word,\n    !word2 %in% all_stopwords$word\n  )\n\nbigrams_filtered\n\n# A tibble: 7,625 × 3\n   doc_title word1    word2    \n   &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;    \n 1 Text A    prooeme  herodotvs\n 2 Text A    clio     reportes \n 3 Text A    croesvs  king     \n 4 Text A    ſonne    borne    \n 5 Text A    borne    dumbe    \n 6 Text A    king     himſelf  \n 7 Text A    imminent danger   \n 8 Text A    certaine perſian  \n 9 Text A    perſian  ready    \n10 Text A    lay      violent  \n# ℹ 7,615 more rows\n\n\nWe remove bigrams where either word is a stopword, since phrases like “of the” or “and the” are rarely meaningful analytically.\n\nbigram_counts &lt;- bigrams_filtered %&gt;%\n  count(doc_title, word1, word2, sort = TRUE)\n\nbigram_counts\n\n# A tibble: 6,447 × 4\n   doc_title word1     word2           n\n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;\n 1 Text B    common    wealth         48\n 2 Text A    merchants adventurers    35\n 3 Text B    latin     alphabet       34\n 4 Text A    low       countries      21\n 5 Text A    free      trade          20\n 6 Text B    east      india          20\n 7 Text A    latin     alphabet       17\n 8 Text A    natiue    commodities    17\n 9 Text A    forraine  commodities    16\n10 Text A    common    wealth         14\n# ℹ 6,437 more rows\n\n\nNow that we removed the stopwords, we can use unite to put the bigrams back together:\n\nbigram_counts &lt;- bigram_counts %&gt;%\n  unite(bigram, word1, word2, sep = \" \")\n\nbigram_counts\n\n# A tibble: 6,447 × 3\n   doc_title bigram                    n\n   &lt;chr&gt;     &lt;chr&gt;                 &lt;int&gt;\n 1 Text B    common wealth            48\n 2 Text A    merchants adventurers    35\n 3 Text B    latin alphabet           34\n 4 Text A    low countries            21\n 5 Text A    free trade               20\n 6 Text B    east india               20\n 7 Text A    latin alphabet           17\n 8 Text A    natiue commodities       17\n 9 Text A    forraine commodities     16\n10 Text A    common wealth            14\n# ℹ 6,437 more rows\n\n\nImportant conceptual point: why are we creating bigrams first, cleaning them from stopwords, and then gluing them back together? Shouldn’t we just tokenize the text into individual words, clean the stopwords, and then find the bigrams?\nLet’s test this out with a phrase such as: “strong economy of international trade.” If we do bigrams first, we get: “strong economy”, “economy of”, “of international”, “international trade.” Removing stopwords will give us: “strong economy”, “international trade” (make sure this makes sense to you).\nIf we cleaned the stopwords first, the phrase would turn into: “strong economy international trade.” The bigrams would then be: “strong economy”, “economy international”, “international trade.” We now have an “extra” bigram, “economy international” (can you see why?).\nNB: there will be situations where the second method (clean first, bigrams second) may be the better option! Think of the phrase: “the state of the art.” The clean first method will give us “state art,” while our method (bigrams first, clean second) won’t capture this at all. The bigram “state art” is a distortion of the concept “the state of the art”–which method is better depends on the question you are asking, but you need to be aware that the choices you make have downstream effects.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#comparing-bigrams",
    "href": "week-02-basics.html#comparing-bigrams",
    "title": "Week 02: Basics",
    "section": "Comparing bigrams:",
    "text": "Comparing bigrams:\nSo far, we have looked at the most frequent bigrams within each text. But frequency alone does not tell us what is distinctive about a text. Remember: we started by (somewhat artificially) asking how the language of the two texts differ. If we compare how often the same bigrams appear across texts, we should be able to start seeing some differences between them.\n\nSince the two texts by Misselden differ in length and in number of bigrams, we want to normalize by the number of bigrams within each document to compare the two.\nWe then reshape the data for comparison: in tidy, we want one row per bigram so that we can easily compare across the two text.\n\n\nbigram_relative &lt;- bigram_counts %&gt;%\n  group_by(doc_title) %&gt;%\n  mutate(\n    total_bigrams = sum(n),\n    proportion = n / total_bigrams\n  ) %&gt;%\n  ungroup()\n\nbigram_wide &lt;- bigram_relative %&gt;%\n  select(doc_title, bigram, proportion) %&gt;%\n  pivot_wider(\n    names_from = doc_title,\n    values_from = proportion,\n    values_fill = 0\n  )\n\nbigram_wide\n\n# A tibble: 6,377 × 3\n   bigram                `Text B` `Text A`\n   &lt;chr&gt;                    &lt;dbl&gt;    &lt;dbl&gt;\n 1 common wealth         0.0187    0.00277\n 2 merchants adventurers 0         0.00692\n 3 latin alphabet        0.0133    0.00336\n 4 low countries         0.00156   0.00415\n 5 free trade            0         0.00395\n 6 east india            0.00779   0      \n 7 natiue commodities    0.00273   0.00336\n 8 forraine commodities  0         0.00316\n 9 letters patents       0.000390  0.00277\n10 thouſand pounds       0         0.00277\n# ℹ 6,367 more rows\n\n\nNext, we are going to contrast the bigrams in the two texts by identifying which biagrams are most likely to distinguish one text from the other. Note: this is not a statistical test, we are just exploring the differences in the bigrams between the two texts. To recap, up to this point, we have:\n\nextracted bigrams\nfiltered them\nnormalized them within each document\n\nWe now want to know which bigram is most characteristic of “Text A” (The Circle of Commerce) relative to “Text B” (Free Trade).\n\nbigram_diff &lt;- bigram_wide %&gt;%\n  mutate(\n    diff = `Text A` - `Text B`\n  ) %&gt;%\n  arrange(desc(abs(diff)))\n\nbigram_diff %&gt;% slice(1:20)\n\n# A tibble: 20 × 4\n   bigram                `Text B` `Text A`     diff\n   &lt;chr&gt;                    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 common wealth         0.0187    0.00277 -0.0159 \n 2 latin alphabet        0.0133    0.00336 -0.00989\n 3 east india            0.00779   0       -0.00779\n 4 merchants adventurers 0         0.00692  0.00692\n 5 free trade            0         0.00395  0.00395\n 6 merchants aduenturers 0.00390   0       -0.00390\n 7 forraine commodities  0         0.00316  0.00316\n 8 thouſand pounds       0         0.00277  0.00277\n 9 low countries         0.00156   0.00415  0.00259\n10 letters patents       0.000390  0.00277  0.00238\n11 publique vtility      0.00234   0       -0.00234\n12 cloth trade           0.00429   0.00198 -0.00231\n13 33 ſh                 0         0.00217  0.00217\n14 20 ſhillings          0         0.00198  0.00198\n15 ſh 4                  0         0.00198  0.00198\n16 disorderly trade      0.00195   0       -0.00195\n17 fishing vpon          0.00195   0       -0.00195\n18 india stocke          0.00195   0       -0.00195\n19 kings honour          0.00195   0       -0.00195\n20 maiesties subiects    0.00195   0       -0.00195\n\n\nWhat does diff tell us? If diff &gt; 0, then the bigram is more prominent in Text A; if diff &lt;0, in Text B. If diff is approximately 0, then it is used more or less in similar proportion in the two texts.\n\nQuestions:\n\nHow do you interpret the results above? What do they tell you?\nThere is a problem with the list above due to spelling inconsistencies in the period (as well as the fact that we ignored numbers). How do you think this is affecting our results? Next week, you will learn a way to fix this.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”",
    "section": "",
    "text": "With the recent explosion in availability of digitized historical and literary texts and the availability of powerful language models, researchers are increasingly turning to computational tools for the analysis of text as data. But not all text is equally amenable to computational approaches. Historical texts often require specialized approaches to bridge the gap between the books as originally produced and analysis-ready data. In this course, students will learn to prepare and analyze historical and literary texts for natural language processing. We will also consider questions of interpretation and the ethics of corpus construction.Our corpora will derive from economic documents and travel narratives from the 17th century. These texts are challenging and require careful curation and cleaning, and they will force you to learn meticulous practices and work-flows in text analysis. Whether working with contemporary or historical texts, most textual data is messy and requires careful preprocessing. By focusing on these Early Modern documents, you will learn how to approach quantitative text analysis with qualitative study of the cultural, economic, and political context that produced the data you are analyzing.",
    "crumbs": [
      "Home",
      "Course",
      "IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”",
    "section": "",
    "text": "With the recent explosion in availability of digitized historical and literary texts and the availability of powerful language models, researchers are increasingly turning to computational tools for the analysis of text as data. But not all text is equally amenable to computational approaches. Historical texts often require specialized approaches to bridge the gap between the books as originally produced and analysis-ready data. In this course, students will learn to prepare and analyze historical and literary texts for natural language processing. We will also consider questions of interpretation and the ethics of corpus construction.Our corpora will derive from economic documents and travel narratives from the 17th century. These texts are challenging and require careful curation and cleaning, and they will force you to learn meticulous practices and work-flows in text analysis. Whether working with contemporary or historical texts, most textual data is messy and requires careful preprocessing. By focusing on these Early Modern documents, you will learn how to approach quantitative text analysis with qualitative study of the cultural, economic, and political context that produced the data you are analyzing.",
    "crumbs": [
      "Home",
      "Course",
      "IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”"
    ]
  },
  {
    "objectID": "week-01-introduction.html",
    "href": "week-01-introduction.html",
    "title": "Week 01: Introduction",
    "section": "",
    "text": "Welcome to IDS 570, “Text as Data.” For each week of the course, you will find code and notes to match the in-class lecture. For the first few weeks of the semester, we will be working in R. As the term goes on, we will introduce Python concepts and methodologies.\nThis repository is a work in progress: I will update it (and the pull-down menu will grow) as the semester progresses and as I add or subtract materials for the course.\n\n\nThis site is generated from Quarto source files and updated regularly–expect things to change and if you notice a typo, please let me or the TA’s know. Code examples are illustrative and meant to be adapted.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 01: Introduction"
    ]
  },
  {
    "objectID": "week-01-introduction.html#welcome",
    "href": "week-01-introduction.html#welcome",
    "title": "Week 01: Introduction",
    "section": "",
    "text": "Welcome to IDS 570, “Text as Data.” For each week of the course, you will find code and notes to match the in-class lecture. For the first few weeks of the semester, we will be working in R. As the term goes on, we will introduce Python concepts and methodologies.\nThis repository is a work in progress: I will update it (and the pull-down menu will grow) as the semester progresses and as I add or subtract materials for the course.\n\n\nThis site is generated from Quarto source files and updated regularly–expect things to change and if you notice a typo, please let me or the TA’s know. Code examples are illustrative and meant to be adapted.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 01: Introduction"
    ]
  },
  {
    "objectID": "week-01-introduction.html#instruction-for-this-week",
    "href": "week-01-introduction.html#instruction-for-this-week",
    "title": "Week 01: Introduction",
    "section": "Instruction for this week:",
    "text": "Instruction for this week:\nFor each week of the course, you will find code and notes to match the in-class lecture. For this week: you need to become familiar with RStudio and learn how to organize directories and projects. You can find a friendly introduction to R at https://rladiessydney.org/courses/01-basicbasics-0. Complete the three BasicBasics lessons. The goal is to feel comfortable with RStudio, installing packages, and reading data into RStudio.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 01: Introduction"
    ]
  },
  {
    "objectID": "week-03-dictionaries.html",
    "href": "week-03-dictionaries.html",
    "title": "Week 03: Dictionaries",
    "section": "",
    "text": "A Quick Aside: Fixing Spelling with Regex\nWhen looking at the data from week 2, you may have noticed some unusual characters (see, for example, the print out of the tibble “word_counts”). We need to make some decisions on how to handle them. This is where regular expressions come in. Regular expressions, abbreviated to regex, are a way to define patterns within strings. It will be helpful to get familiar with the package stringr (which we have used before without comment) by looking at the documentation here.\n\nlibrary(readr) \nlibrary(dplyr) \nlibrary(stringr) \nlibrary(tibble)  \n\ncircle_raw &lt;- read_file(\"texts/A07594__Circle_of_Commerce.txt\") \n\ntext_tbl &lt;- tibble(   \n  doc_title = \"The Circle of Commerce\",   \n  text = circle_raw \n  )  \n\n# Taking a look at text_tbl (or at least part of it since it's very long):\ntext_tbl %&gt;% select(doc_title)\n\n# A tibble: 1 × 1\n  doc_title             \n  &lt;chr&gt;                 \n1 The Circle of Commerce\n\nnchar(text_tbl$text)\n\n[1] 191605\n\nstr_sub(text_tbl$text, 1, 400)\n\n[1] \"THE CIRCLE OF COMMERCE. The Prooeme. HERODOTVS in his CLIO, reportes that CROESVS King of LYDIA had a ſonne borne dumbe: and his Countrey being invaded, and the King himſelf in imminent danger of death by a certaine Perſian ready to lay violent hands on him; the Kings ſonne affected with the preſent danger, then ſpake that neuer ſpake before, and cryed alowd, O homo ne perimas Patrem! O man kill n\"\n\n\nBefore we change any parts of the text, we are going to preserve the original text in case we change our mind later on. So we are creating a new tibble named text_tbl that contains the original text in a column named text_original:\n\ntext_tbl &lt;- text_tbl %&gt;%\n  mutate(text_original = text)\n\nIn early modern print, the long s (ſ) represents the same letter as the modern s (both were used in print). When it appears explicitly in transcription or OCR, we can safely normalize it to a modern s. In historical texts, regex is often used for normalization, not “correction.” the long s isn’t an error, it’s just a variant (as the British “colour” vs. the American “color” are variants of the same word).\nRegular expressions allow us to:\n\nidentify patterns in text, and\nreplace them systematically.\n\n\n text_tbl &lt;- text_tbl %&gt;%   \n  mutate(     \n    text_clean = str_replace_all(text_original, \"ſ\", \"s\")   \n    )  \n\n\n# Let's count how many \"ſ\" we had before and after to check that the substitution worked:\n\ntibble(\n  long_s_before = str_count(text_tbl$text_original, \"ſ\"),\n  long_s_after  = str_count(text_tbl$text_clean, \"ſ\")\n)\n\n# A tibble: 1 × 2\n  long_s_before long_s_after\n          &lt;int&gt;        &lt;int&gt;\n1          4517            0\n\n\nA bit of warning:\n\nWe replace only the explicit character ſ\nSometimes, the long S is represented as an “f” in transcribed/OCR’ed texts, but I do not recommending trying to guess when an f “should be” an s. Make sure that you know why! If not, ask!\n\nFinally, to pick up another loose thread from Week 2, we can use regex to selectively keep some punctuation and symbols. Remember that we used unnest_tokens to tokenize our text, but we learnt that it automatically removes punctuation. We can set that parameter to FALSE and keep all punctuation. But we can also be more selective. For example, if I only want to keep currency markers (British pounds for our texts), apostrophes, and hyphens, I can do the following:\ntext_tbl &lt;- text_tbl %&gt;%\n  mutate(\n    text_clean = str_replace_all(text_clean, regex(\"[[:punct:]&&[^£'-]]\"), \" \") # note I am writing over \"text_clean\", we could also create a new                                                                                       column instead\n  )\nNB: “^ at the start of a character class in regex means”NOT”. I am removing all punctuation except the list I am defining.\n\nStandardizing Name Spellings:\nEarly Modern spelling was not standardized, meaning that the same word may have been spelled in a number of different ways by the same author within the same document. Name spelling was also not standardized and this fact presents some serious problems for us. While it’s pretty easy to decide that “friend” = “freind” (a peculiar spelling by the poet John Milton) for normalization processes, standardizing names in historical texts is not a neutral preprocessing step. It requires biographical and historical research.\nIf we decide that, for example, Smythe, Smyth, and Smith all refer to the same name, we are making a scholarly claim about identity, authorship, and equivalence across spelling variation. Different research questions may require different choices. Therefore, name standardization rules should always be documented and justified as part of your research.\nHaving said that, how would we go about standardizing the variant of Smith?\nThe first step is to define an explicit standardization map. This step will seem excessive right now, but if you are standardizing a large number of words, this vector will keep track of your decisions and it can be expanded as you go.\n\nname_map &lt;- c(   \n\"Smythe\" = \"Smith\",   \"Smyth\"  = \"Smith\",   \"Smithe\" = \"Smith\" \n)\n\nNow, we do not want to simply “search and replace” based on this map. The name-place “Smythfield” (which I am making up right now) should not be replaced by “Smithfield.” What we want to replace is the specific string “Smyth”. To do this, we have to think about word boundaries and typesetting (we want to catch all versions of the name regardless of case):\n\ntext_standard &lt;- text_tbl %&gt;%\n  mutate(\n    text_norm = text_clean %&gt;%\n      str_replace_all(regex(\"\\\\bSmythe\\\\b\", ignore_case = TRUE), \"Smith\") %&gt;%\n      str_replace_all(regex(\"\\\\bSmyth\\\\b\",  ignore_case = TRUE), \"Smith\") %&gt;%\n      str_replace_all(regex(\"\\\\bSmithe\\\\b\", ignore_case = TRUE), \"Smith\")\n  )\n\nThe code above uses \\\\b to denote the word boundary. This syntax is peculiar to R (in Python is would be \\b instead). The boundary matches the position between: a word character ([A–Z a–z 0–9 _]) AND a non-word character (space, punctuation, start/end of string). So, \\\\bSmithe\\\\b will catch: “Smithe”, “Smithe.”, “Smithe)”, etc. but not Smithefield.\nNote also that I added a new column with mutate, called text_norm. This way, the original column text_clean is not changed.\n\n\n\nDescribing the Text: N-grams and Trade\nLet’s start to explore how The Circle of Commerce talks about “trade.” Last week we focused on single words and on all bigrams. But there are limitations to what word counts and context-free bigrams tell us about a text. What if we wanted to know more than just how often the word “trade” is used? An obvious, first question is to ask how Misselden describes trade in The Circle: is he focusing on “domestic trade” or “foreign trade”? Does the think that “trade is growing” or that “trade is declining”? We can start exploring these kinds of relationship by using n-grams containing the word trade. The first steps of this analysis will be a bit of review.\nI am going to focus on bigrams as it will keep the lesson clean and easy to follow. Let’s start by setting up the needed packages (in addition to the ones we already loaded):\n\nlibrary(tidyr) \nlibrary(tidytext) \nlibrary(ggplot2) \nlibrary(forcats)\n\nBigrams tell us which words tend to appear next to each other in our text. But their raw frequencies can be dominated by turns of phrase (such as “the trade” or “this trade”) that may or may not be helpful in our analysis. So we will have to make some decisions on how to clean the texts and how to interpret our results.\nWe are going to start by tokenizing into bigrams. To do so, we are going to take two words at the time rather than one at the time when we use unnest_tokens.\n\nbigrams_raw &lt;- text_standard %&gt;%\n select(doc_title, text_norm) %&gt;%\n unnest_tokens(output = \"bigram\", input = text_norm, token = \"ngrams\", n = 2)\n\n#let's take a look\n\nbigrams_raw %&gt;% count(bigram, sort = TRUE) %&gt;% slice_head(n = 10)\n\n# A tibble: 10 × 2\n   bigram       n\n   &lt;chr&gt;    &lt;int&gt;\n 1 of the     386\n 2 in the     224\n 3 to the     144\n 4 and the    113\n 5 it is      102\n 6 of trade    91\n 7 for the     90\n 8 to be       80\n 9 of his      77\n10 that the    73\n\n\n\nCleaning the bigrams:\nAs you can see from the list above, we never removed stopwords from text_standard. We are going to do this now. We will have to split up the bigrams into single words and remove stopwords for each “half” of the bigram (see week 2 for more details on cleaning):\n\ndata(\"stop_words\") # the standard list from tidytext, but you can adapt the process from week 2 to include custom stop words\n\nbigrams_clean &lt;- bigrams_raw %&gt;%\nseparate(bigram, into = c(\"word1\", \"word2\"), sep = \" \") %&gt;%\nfilter(!word1 %in% stop_words$word) %&gt;%\nfilter(!word2 %in% stop_words$word) %&gt;%\nfilter(str_detect(word1, \"^[a-z]+$\")) %&gt;% # note: here I am removing ALL punctuation (earlier we kept specific symbols)\nfilter(str_detect(word2, \"^[a-z]+$\"))\n\nbigrams_clean %&gt;% count(word1, word2, sort = TRUE) %&gt;% slice_head(n = 10)\n\n# A tibble: 10 × 3\n   word1     word2           n\n   &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;\n 1 merchants adventurers    35\n 2 low       countries      21\n 3 free      trade          20\n 4 latin     alphabet       17\n 5 natiue    commodities    17\n 6 forraine  commodities    16\n 7 common    wealth         14\n 8 letters   patents        14\n 9 thousand  pounds         14\n10 cloth     trade          10\n\n\nNow we glue the bigrams back together and visualize them using ggplot:\n\nbigram_counts &lt;- bigrams_clean %&gt;%\ncount(word1, word2, sort = TRUE) %&gt;%\nunite(\"bigram\", word1, word2, sep = \" \")\n\nbigram_counts %&gt;%\nslice_head(n = 20) %&gt;%\nmutate(bigram = fct_reorder(bigram, n)) %&gt;%\nggplot(aes(x = n, y = bigram)) +\ngeom_col() +\nlabs(\ntitle = \"Most frequent bigrams (after stopword filtering)\",\nx = \"Count\",\ny = NULL\n)\n\n\n\n\n\n\n\n\n\n\nHoming in on the concept of trade:\nWe are going to work on two straightforward “trade-focused” strategies that will give us a preliminary way to capture the concept of trade through token analysis:\n\nFilter bigrams that literally contain the token trade\nUse a small “trade lexicon” to capture near-synonyms (e.g., traffick, commerce, merchant, exchange)\n\nLet’s start with the first strategy: which bigrams contain the token trade?\n\ntrade_bigrams &lt;- bigram_counts %&gt;%\nfilter(str_detect(bigram, \"\\\\btrade\\\\b\")) # this line should look familiar\n\ntrade_bigrams %&gt;% slice_head(n = 25)\n\n# A tibble: 25 × 2\n   bigram                n\n   &lt;chr&gt;             &lt;int&gt;\n 1 free trade           20\n 2 cloth trade          10\n 3 trade cap             5\n 4 adventurers trade     4\n 5 fishing trade         3\n 6 kingdomes trade       3\n 7 forraine trade        2\n 8 india trade           2\n 9 persia trade          2\n10 trade free            2\n# ℹ 15 more rows\n\ntrade_bigrams %&gt;%\nslice_head(n = 20) %&gt;%\nmutate(bigram = fct_reorder(bigram, n)) %&gt;%\nggplot(aes(x = n, y = bigram)) +\ngeom_col() +\nlabs(\ntitle = \"Bigrams that include the word 'trade'\",\nx = \"Count\",\ny = NULL\n)\n\n\n\n\n\n\n\n\nThis is helpful: it lets us get a first sense of what types of trade might be most important conceptually to Misselden (“free trade” is clearly something that he is concerned with). But what about trade related terms, such as “commodities” or “exchange”? How do their bigrams fit in?\nLet’s create a trade lexicon and check:\n\ntrade_lexicon &lt;- c(\n  \"trade\", \"traffick\", \"traffic\", \"commerce\", \"merchant\", \"merchants\",\n  \"exchange\", \"export\", \"import\", \"commodity\", \"commodities\",\n  \"navigation\", \"shipping\", \"market\", \"markets\"\n)\n\ntrade_theme_bigrams &lt;- bigrams_clean %&gt;%\n  filter(word1 %in% trade_lexicon | word2 %in% trade_lexicon) %&gt;%\n  count(word1, word2, sort = TRUE) %&gt;%\n  unite(\"bigram\", word1, word2, sep = \" \")\n\ntrade_theme_bigrams %&gt;% slice_head(n = 25)\n\n# A tibble: 25 × 2\n   bigram                    n\n   &lt;chr&gt;                 &lt;int&gt;\n 1 merchants adventurers    35\n 2 free trade               20\n 3 natiue commodities       17\n 4 forraine commodities     16\n 5 cloth trade              10\n 6 exchange betweene         8\n 7 commodities exported      5\n 8 commodities imported      5\n 9 low exchange              5\n10 politique exchange        5\n# ℹ 15 more rows\n\ntrade_theme_bigrams %&gt;%\nslice_head(n = 20) %&gt;%\nmutate(bigram = fct_reorder(bigram, n)) %&gt;%\nggplot(aes(x = n, y = bigram)) +\ngeom_col() +\nlabs(\ntitle = \"Trade-theme bigrams (lexicon-based)\",\nx = \"Count\",\ny = NULL\n)\n\n\n\n\n\n\n\n\nWe now see that “natiue commodities” and “forraine commodities” are almost as important as “free trade” in The Circle of Commerce, and “merchants adventurers” is even more prominent.\nWarning: while my lexicon tracks the alternate spelling for “traffic” (= “traffick”), it doesn’t account for other possible spelling variations (such as “native” = “natiue”). In addition, it treats the variations as distinct words, so that “native plant” is being counted as a different bigram from “natiue plant.” This is fine (in my opinion) at this exploratory stage, but it’s something that will need addressing in our analysis down the line.\n\nFor your own entertainment: test your understanding of regex by standardizing “traffick” to “traffic” in the text upstream of the bigram analysis.\n\n\n\n\nSentiment Analysis:\nSo far, we have explore the language around trade (and trade-related words), but what about the sentiment of this language? Sentiment analysis is a pretty standard technique in NLP projects, but, as we discussed in class, it doesn’t translate smoothly to specialized and historical discourse. For our purposes, we will want to create a targeted sentiment analysis where we measure sentiment only in passages near “trade” (and related terms), instead of across the whole document. This will give us more granular insight over the documents’ tone around the concept of trade.\nLet’s compare The Circle of Commerce (1623) to the earlier text, Free Trade (1622). We will do this in steps:\n\nDefine a trade keyword set (e.g., trade/commerce/merchant).\nExtract token windows around each keyword occurrence (±30 words). There is no hard and fast rule on the size of the window that you want to select. I settled on 30 as a good estimate/guess based on my experience in reading Early Modern texts.\nCompute sentiment inside those windows only.\nCompare two texts.\n\nLet’s start the usual way:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(ggplot2)\n\ncircle_raw &lt;- read_file(\"texts/A07594__Circle_of_Commerce.txt\")\nfree_raw   &lt;- read_file(\"texts/B14801__Free_Trade.txt\")\n\ntexts_miss &lt;- tibble(\n  doc_title = c(\"Circle of Commerce\", \"Free Trade\"),\n  text = c(circle_raw, free_raw)\n)\n\nWe are going to do some basic normalization:\n\ntexts_miss &lt;- texts_miss %&gt;%\n  mutate(\n    text_norm = text %&gt;%\n      str_replace_all(\"ſ\", \"s\") %&gt;%   # long s as above\n      str_replace_all(\"\\\\s+\", \" \") %&gt;% # collapse whitespace\n      str_to_lower()\n  )\n\nNote: I am adding a line to collapse whitespace because historical texts can have irregular spaces due to archaic typography. We want to remove extraneous whitespaces and replace them with a single whitespace, such as “The circle of commerce” –&gt; “The circle of commerce.” This problem is not unique to historical texts: as you are aware, social media posts don’t always adhere to the best typesetting (or spelling or grammar…) rules!\nImportant: since we are going to want to create a window of 30 words around our target trade terms, we are going to need to keep track of the position of each word in the document. We do this with the index (token_id).\n\ntokens &lt;- texts_miss %&gt;%\n  unnest_tokens(word, text_norm, token = \"words\") %&gt;%\n  group_by(doc_title) %&gt;%\n  mutate(token_id = row_number()) %&gt;%\n  ungroup()\n\nTo make sure that you understand what we are doing above, start by reviewing unnest_tokens from week 2. That should give you a sense of why we want token_id = row_number. Finally, inspect tokens in RStudio–does it look the way you expect it to?\nNext, we are going to identify where our target terms (in this case, I am going to look at “trade”, “commerce”, “merchant”, and “merchants”, but you can play around with these choices) are in the texts:\n\ntrade_terms &lt;- c(\"trade\", \"commerce\", \"merchant\", \"merchants\")\n\ntrade_hits &lt;- tokens %&gt;%\n  filter(word %in% trade_terms) %&gt;%\n  select(doc_title, hit_word = word, hit_token_id = token_id)\n\nNow that we have located the position of the trade terms (trade_hits), we can create the ±30 token window around them:\n\nwindow_size &lt;- 30\n\ntrade_windows &lt;- tokens %&gt;%\n  inner_join(trade_hits, by = \"doc_title\") %&gt;%\n  filter(token_id &gt;= hit_token_id - window_size,\n         token_id &lt;= hit_token_id + window_size) %&gt;%\n  mutate(window_id = paste(doc_title, hit_token_id, sep = \"_\"))\n\nA couple of notes about the code above:\n\nI gave you a link to information about joins last week, go back to it to understand inner_join.\nWhen you run this code, you will get a warning about “an unexpected many-to-many relationship”: this is fine and expected. Our target words appear frequently in the documents (they are, after all, mercantile documents about trade) and so the windows we are creating are overlapping, leading R to warn us about this issue. You can silence the warning if you want.\n\nLet’s take a look at what one window looks like:\n\ntrade_windows %&gt;%\n  filter(window_id == nth(unique(window_id), 10)) %&gt;%\n  summarise(window_text = str_c(word, collapse = \" \")) %&gt;%\n  pull(window_text) %&gt;%\n  cat()\n\nvnfitnesse of my pen to represent such pieces so also had i not the happines to attend those then or these since in any of their assemblies as did other merchants whereby my discourse might haue receiued some life and force from their worth and influence their good acceptation of my poore endeauours together with the approbation of many other noble\n\n\nJust as you expected (right?), this is a 61-token window centered around one of our target words (“merchants” in this case).\nNow we are reading to actually compute sentiments. Tidytext gives us access to a number of sentiment lexicons. We are going to use a binary one, bing (by Bing Liu and his collaborators), that categorizes words into positive or negative. As we discussed at the onset, this is not going to be historically accurate, but it will give us a reasonable baseline.\nWe are going to get into the code details in a second, but let’s start with the reasoning behind this approach:\n\nWe are going to look at all the 61-token windows around “trade”, “merchant”, etc.\nMatch words to the sentiment dictionary\nCount: how many positive words vs negative words in each window?\nCalculate a net score: positive count minus negative count [so, a window with 5 positive, 2 negative → net = +3 (overall positive)].\n\nThe code looks like this:\n\nbing &lt;- get_sentiments(\"bing\")\n\nwindow_sentiment &lt;- trade_windows %&gt;%\n  inner_join(bing, by = \"word\") %&gt;%  # keeps only sentiment-bearing words\n  count(doc_title, window_id, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(net_sentiment = positive - negative)\n\nNext, we create an overall summary of sentiments for each text. First:\n\nWe are going to find all occurrences of trade-related tokens and create a fixed size “window” around each term.\nWe then inner_join those tokens to the Bing sentiment lexicon so that each token can be labeled as either positive or negative. In this step, if a token doesn’t appear in the Bing lexicon, it gets dropped (a window that contains no Bing words will have no rows in window_sentiment). This means that n() is counting rows of window_sentiment. That is, n() counts the number of word-tokens matched in Bing, aggregated by doc_title.\n\nThen, from window_sentiment, we compute:\n\nThe number of positive and negative Bing-matched token occurrences (so, if “good” appears five times, it contributes five positive occurrences) in our trade-related window.\nThe total net sentiment (= positive - negative)\nThe average net sentiment per sentiment-bearing window = net_sentiment / n_distinct(window_id) . That is, the denominator is the number of windows with at least one Bing match.\n\n\ntext_sentiment_summary &lt;- window_sentiment %&gt;%\n  group_by(doc_title) %&gt;%\n  summarise(\n    windows = n(),\n    total_positive = sum(positive),\n    total_negative = sum(negative),\n    total_net_sentiment = sum(net_sentiment),      \n    avg_net_per_window = mean(net_sentiment),\n    .groups = \"drop\"\n  )\ntext_sentiment_summary\n\n# A tibble: 2 × 6\n  doc_title          windows total_positive total_negative total_net_sentiment\n  &lt;chr&gt;                &lt;int&gt;          &lt;int&gt;          &lt;int&gt;               &lt;int&gt;\n1 Circle of Commerce     338            556            327                 229\n2 Free Trade             249            404            310                  94\n# ℹ 1 more variable: avg_net_per_window &lt;dbl&gt;\n\n\nBased on this summary, we can start to see that The Circle of Commerce uses more positive language around the trade-related terms that we selected. To get a better handle of this, we are going to plot the distribution of sentiment scores across all the trade windows for each document as a bar graph:\n\nggplot(window_sentiment, aes(x = net_sentiment)) +\n  geom_histogram(binwidth = 1) +\n  facet_wrap(~ doc_title, ncol = 1) +\n  labs(\n    title = \"Sentiment in Trade-Centered Windows (±30 words)\",\n    x = \"Net sentiment (positive - negative) per window\",\n    y = \"Number of trade windows\"\n  )\n\n\n\n\n\n\n\n\n\n\nAddendum about Sentiments:\nWe want to be careful in how we interpret sentiment analysis. What we are computing above is a score based on the common (standard?) usage of terms such as “wonderful” or “happy” or “depressing”, where we label certain terms as expressing either positive or negative sentiments. These are of course deeply cultural questions. We all know the stereotypes of Italians being effusive while the British are reserved (as an Italian I do not in the least approve of this absolutely terrible and unfair statement)–does a “great!” from two different speakers mean the same level of positive approval?\nThe past is culturally different to the present and we have to be very careful with sentiment analysis with historical texts. The correct solution would be to create our own, historically based lexicon by becoming familiar with the writing style and linguistic nuances of the 17th century and then scoring terminology (or better yet passages and phrases) for positive or negative sentiments. This is obviously labor intensive. But competent text analysis on any specialized language (historical, medical, legal, etc) requires expertise and care.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 03: Dictionaries"
    ]
  },
  {
    "objectID": "week-05-representation.html",
    "href": "week-05-representation.html",
    "title": "Week 05: Text Representation (2)",
    "section": "",
    "text": "In this tutorial, we’ll explore three measures of lexical complexity (also called lexical diversity or lexical richness) using two historical texts; one you know well by now, the other one is a new mystery text that you can find on Canvas:\n\n“The Circle of Commerce” by Edward Misselden (1623)\n“A69858.txt”\n\nWe want to see if the two texts differ in terms of vocabulary diversity in a meaningful way.\nWhat is Lexical Complexity? Lexical complexity measures how varied a writer’s vocabulary is. A text with high lexical diversity uses many different words, while a text with low diversity repeats the same words frequently. We’ll learn three different, but well-validated, measures:\n\nType-Token Ratio (TTR) - The simplest measure\nGuiraud Index - Corrects for text length\nMeasure of Textual Lexical Diversity (MTLD) - The most sophisticated measure\n\n\n\nWe are going to introduce a new library for MTLD (koRpus). This is a great library to be familiar with because it includes functions for automatics language detection, hyphenation, several indices of lexical diversity (including MTLD), and indices of readability. It’s also well documented and a standard package in academic work.\n\nlibrary(readr) \nlibrary(tidyverse)\nlibrary(tidyr) \nlibrary(tidytext) \nlibrary(ggplot2) \nlibrary(udpipe)\nlibrary(koRpus)\n\n#install.packages(\"koRpus.lang.en\") # You will need to do this once\nlibrary(koRpus.lang.en)\n\ntheme_set(theme_minimal(base_size = 14))\n\n\n# Read the text files\ncircle &lt;- read_file(\"texts/A07594__Circle_of_Commerce.txt\")\nmystery &lt;- read_file(\"texts/A69858.txt\")\n\n# Create a tidy data frame with both texts\ntexts_df &lt;- tibble(\n  document = c(\"Circle of Commerce\", \"A69858\"),\n  author = c(\"Misselden\", \"Unknown\"),\n  text = c(circle, mystery)\n)\n\n# Display basic information\ntexts_df %&gt;%\n  select(document, author) %&gt;%\n  knitr::kable(caption = \"Our Two Texts\")\n\n\nOur Two Texts\n\n\ndocument\nauthor\n\n\n\n\nCircle of Commerce\nMisselden\n\n\nA69858\nUnknown\n\n\n\n\n\nNow that we have our two texts loaded, let start with Type-Token Ratio (TTR). TTR is the simplest measure of lexical diversity. It’s calculated as:\n\\[\\text{TTR} = \\frac{\\text{Types (unique words)}}{\\text{Tokens (total words)}}\\]\n\n# Tokenize the texts and do some basic cleaning (review week 2 if you are usure!)\ntokens &lt;- texts_df %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  mutate(word = str_to_lower(word))\n\n# Calculate TTR for each text\nttr_results &lt;- tokens %&gt;%\n  group_by(document, author) %&gt;%\n  summarise(\n    tokens = n(),                    # Count total words\n    types = n_distinct(word),        # Count unique words\n    ttr = types / tokens,            # Calculate TTR\n    .groups = \"drop\"\n  )\n\n# Display results\nttr_results %&gt;%\n  knitr::kable(\n    digits = 3,\n    caption = \"Type-Token Ratio Results\",\n    col.names = c(\"Document\", \"Author\", \"Total Words\", \"Unique Words\", \"TTR\")\n  )\n\n\nType-Token Ratio Results\n\n\nDocument\nAuthor\nTotal Words\nUnique Words\nTTR\n\n\n\n\nA69858\nUnknown\n5942\n1317\n0.222\n\n\nCircle of Commerce\nMisselden\n33894\n4769\n0.141\n\n\n\n\n\nNote: I am using a new function, knitr::kable(x), to display the results as a simple table. It takes whatever is the x (=data frame or matrix) and makes it into a table.\nLet’s visualize the results that we just got:\n\n# Create a bar plot comparing TTR\nggplot(ttr_results, aes(x = author, y = ttr, fill = author)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(ttr, 3)), \n            vjust = -0.5, size = 5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Type-Token Ratio Comparison\",\n    subtitle = \"Higher values = more diverse vocabulary\",\n    x = NULL,\n    y = \"Type-Token Ratio (TTR)\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(0, max(ttr_results$ttr) * 1.15)\n\n\n\n\n\n\n\n\nFrom this first measure, we see that Misselden’s text has a lower TTR than the unknown one. What does this mean, exactly? Well, we can see that in The Circle of Commerce only 14.1% of the words are new word forms (that is, a word string–after normalization–that has not appeared before in exactly that form in the text so far), while for the “mystery” text, 22.2% of the words are new word forms. That is, TTR is a measure of “non-repetition” of word forms. Another way to think about it is that for The Circle of Commerce, if you encounter any random word in the text, that’s a 14.1% chance that it’s a word type you haven’t seen before. So, The Circle of Commerce has lower lexical variety than A69858: it repeats itself more than A69858.\nWarning: longer texts will almost always have a lower TTR score than shorter texts, even if they have a richer vocabulary (can you see why?). Earlier, we saw that A69858 has, in total, 5942 words, while Circle of Commerce has, in total, 33894. The two texts are substantially different in length, so we need to double check with a length-corrected measure (such as Guiraud and MTLD).\nWhy you might still want to compute TTR: it’s a standard across disciplines and you will encounter it in publications, so you need to know what you are looking at. Anyone reading a corpus analysis will recognize it and be able to understand what it represents. TTR is a standard measure of lexical variety. It’s easy to audit and easy to reproduce. It’s also a good first diagnostic check.\n\n\n\nThe Guiraud Index (also called Root TTR) solves the text length problem by using a mathematical correction:\n\\[\\text{Guiraud} = \\frac{\\text{Types}}{\\sqrt{\\text{Tokens}}}\\]\nBy dividing by the square root of tokens instead of the raw count, the Guiraud Index is more stable across different text lengths. This adjustment is a practical approximation that assumes that as the length of a text doubles, its vocabulary does not double. The square root in the Guiraud index is there because vocabulary growth in texts is empirically sublinear: as a text gets longer, new words appear more slowly. The Guiraud Index essentially asks: how many distinct word forms does a text introduce once we partially correct for the fact that longer texts repeat terms more often?\nCalculate Guiraud Index:\n\n# Calculate Guiraud Index for each text\nguiraud_results &lt;- tokens %&gt;%\n  group_by(document, author) %&gt;%\n  summarise(\n    tokens = n(),\n    types = n_distinct(word),\n    guiraud = types / sqrt(tokens),   # Guiraud formula\n    .groups = \"drop\"\n  )\n\n# Display results\nguiraud_results %&gt;%\n  knitr::kable(\n    digits = 3,\n    caption = \"Guiraud Index Results\",\n    col.names = c(\"Document\", \"Author\", \"Total Words\", \"Unique Words\", \"Guiraud Index\")\n  )\n\n\nGuiraud Index Results\n\n\n\n\n\n\n\n\n\nDocument\nAuthor\nTotal Words\nUnique Words\nGuiraud Index\n\n\n\n\nA69858\nUnknown\n5942\n1317\n17.085\n\n\nCircle of Commerce\nMisselden\n33894\n4769\n25.904\n\n\n\n\n\nAnd let’s visualize it:\n\n# Create a bar plot comparing Guiraud Index\nggplot(guiraud_results, aes(x = author, y = guiraud, fill = author)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(guiraud, 2)), \n            vjust = -0.5, size = 5) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(\n    title = \"Guiraud Index Comparison\",\n    subtitle = \"Length-corrected measure of lexical diversity\",\n    x = NULL,\n    y = \"Guiraud Index\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(0, max(guiraud_results$guiraud) * 1.15)\n\n\n\n\n\n\n\n\nThis result gives a very different picture from what we noticed with TTR: The Circle has a more varied vocabulary than the mystery text when we partially account for the length of the text.\nHowever, you want to make sure that when you compare texts using the Guiraud Index, you are looking at texts that are of similar genre and (though less so) similar length. Different types of texts are associated with different demands on vocabulary and rhetorical strategies (for example, a legal argument will require different vocabulary choice than a news report of the same court proceeding). Because the index still depends on text length and genre, there is no universal ‘benchmark’ scale. Instead, values should be interpreted relatively, as we are doing here.\nThis leads us to:\n\n\n\nWhat is MTLD?\nMTLD is the most sophisticated measure we’ll use. It was designed to account for the problems of the measures above. TTR and Guiraud essentially ask: given a text of length N, how many distinct word forms does it contain? MTLD instead asks: how long can a text continue before its lexical diversity drops below a fixed threshold? It works by:\n\nReading through the text sequentially\nCalculating a running TTR as it goes\nCounting how many times the TTR drops below 0.72 (this has been determined empirically across many text types and genres). Once you detect that the document’s TTR drops below 0.72, you count one factor. Then you reset and restart computing TTR on the next chunk of text.\nThe result is the average “length” needed before vocabulary starts repeating and it’s computed by:\n\n\n\n\nThe Measure of Textual Lexical Diversity (MTLD) is computed as:\n\\[\n\\text{MTLD} = \\frac{N}{F}\n\\]\nwhere:\n\nN = total number of word tokens in the text\nF = total number of factors, including any partial factor at the end of the text\n\nIn practice, MTLD is usually computed in both directions (forward and backward through the text), and the final value is the mean of the two:\n\\[\n\\text{MTLD}_{\\text{final}} =\n\\frac{\\text{MTLD}_{\\text{forward}} + \\text{MTLD}_{\\text{backward}}}{2}\n\\]\nHigher MTLD = more diverse vocabulary (the author can go longer before repeating words).\nLet’s compute this second version (both forward and backward through the text using the koRpus package (and use the formatting that it requires). Note: koRpus gives you a number of options which you can find here.\n\n# Function to calculate MTLD for a text\ncalculate_mtld &lt;- function(text_string, doc_name) {\n  # Create a temporary file (koRpus requirement)\n  temp_file &lt;- tempfile(fileext = \".txt\")\n  writeLines(text_string, temp_file)\n  \n  # Tokenize with koRpus\n  tokenized &lt;- tokenize(temp_file, lang = \"en\")\n  \n  # Calculate MTLD\n  mtld_result &lt;- MTLD(tokenized)\n  \n  # Extract the MTLD value\n  mtld_value &lt;- mtld_result@MTLD$MTLD\n  \n  # Clean up\n  unlink(temp_file)\n  \n  return(mtld_value)\n}\n\n# Calculate MTLD for both texts\nmtld_results &lt;- texts_df %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    mtld = calculate_mtld(text, document)\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(document, author, mtld)\n\nLanguage: \"en\"\nLanguage: \"en\"\n\n# Display results\nmtld_results %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"MTLD Results\",\n    col.names = c(\"Document\", \"Author\", \"MTLD\")\n  )\n\n\nMTLD Results\n\n\nDocument\nAuthor\nMTLD\n\n\n\n\nCircle of Commerce\nMisselden\n63.09\n\n\nA69858\nUnknown\n84.17\n\n\n\n\n\nAn R coding note for those of you who are new to coding. The MTLD computation using koRpus is being put together in the curly brackets that define all the steps that the new function we are defining, calculate_mtld, will perform. NB: calculate_mtlddoes not actually compute MTLD, it just takes a text string (our Tidy data), converts it to a file for koRpus, calls in koRpus to compute MTLD, and then returns everything to Tidy.\nLet’s visualize the results:\n\n# Create a bar plot comparing MTLD\nggplot(mtld_results, aes(x = author, y = mtld, fill = author)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(mtld, 1)), \n            vjust = -0.5, size = 5) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"MTLD Comparison\",\n    subtitle = \"Mean Length of Sequential Word Strings (Higher = More Diverse)\",\n    x = NULL,\n    y = \"MTLD Score\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(0, max(mtld_results$mtld) * 1.15)\n\n\n\n\n\n\n\n\nWell, this is interesting and important in understanding how these measures work. Unlike Guiraud, but like TTR, MTLD assigns a greater lexical variety to our mystery text. What MTLD is picking up is a difference in how lexical variety is distributed across the text.\nTTR and Guiraud only care about how many distinct word forms there are overall in the texts. They are both global measures of variety.\nMTLD is sensitive to how repetition is patterned through text “time” (think of reading the text as your measure of text “time”). A higher MTLD in the mystery text means that it sustains lexical variety more consistently across its length while Misselden’s lower MTLD means that The Circle includes repetition-heavy chunks earlier on and more often (even though it globally, when adjusted for length, ends up with more unique token types).\nGive this kind of result, we expect that in The Circle, Misselden includes lots of repeated terms and phrases within sections (local repetition as he perhaps defines and explains a given concept/idea), but overall introduces more terms per unit of text length. The mystery text, instead, distributes its vocabulary more evenly. A69858 uses fewer unique words in total, but sustain more lexical variety within any given passage.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 05: Text Representation (2)"
    ]
  },
  {
    "objectID": "week-05-representation.html#measures-of-lexical-complexity",
    "href": "week-05-representation.html#measures-of-lexical-complexity",
    "title": "Week 05: Text Representation (2)",
    "section": "",
    "text": "In this tutorial, we’ll explore three measures of lexical complexity (also called lexical diversity or lexical richness) using two historical texts; one you know well by now, the other one is a new mystery text that you can find on Canvas:\n\n“The Circle of Commerce” by Edward Misselden (1623)\n“A69858.txt”\n\nWe want to see if the two texts differ in terms of vocabulary diversity in a meaningful way.\nWhat is Lexical Complexity? Lexical complexity measures how varied a writer’s vocabulary is. A text with high lexical diversity uses many different words, while a text with low diversity repeats the same words frequently. We’ll learn three different, but well-validated, measures:\n\nType-Token Ratio (TTR) - The simplest measure\nGuiraud Index - Corrects for text length\nMeasure of Textual Lexical Diversity (MTLD) - The most sophisticated measure\n\n\n\nWe are going to introduce a new library for MTLD (koRpus). This is a great library to be familiar with because it includes functions for automatics language detection, hyphenation, several indices of lexical diversity (including MTLD), and indices of readability. It’s also well documented and a standard package in academic work.\n\nlibrary(readr) \nlibrary(tidyverse)\nlibrary(tidyr) \nlibrary(tidytext) \nlibrary(ggplot2) \nlibrary(udpipe)\nlibrary(koRpus)\n\n#install.packages(\"koRpus.lang.en\") # You will need to do this once\nlibrary(koRpus.lang.en)\n\ntheme_set(theme_minimal(base_size = 14))\n\n\n# Read the text files\ncircle &lt;- read_file(\"texts/A07594__Circle_of_Commerce.txt\")\nmystery &lt;- read_file(\"texts/A69858.txt\")\n\n# Create a tidy data frame with both texts\ntexts_df &lt;- tibble(\n  document = c(\"Circle of Commerce\", \"A69858\"),\n  author = c(\"Misselden\", \"Unknown\"),\n  text = c(circle, mystery)\n)\n\n# Display basic information\ntexts_df %&gt;%\n  select(document, author) %&gt;%\n  knitr::kable(caption = \"Our Two Texts\")\n\n\nOur Two Texts\n\n\ndocument\nauthor\n\n\n\n\nCircle of Commerce\nMisselden\n\n\nA69858\nUnknown\n\n\n\n\n\nNow that we have our two texts loaded, let start with Type-Token Ratio (TTR). TTR is the simplest measure of lexical diversity. It’s calculated as:\n\\[\\text{TTR} = \\frac{\\text{Types (unique words)}}{\\text{Tokens (total words)}}\\]\n\n# Tokenize the texts and do some basic cleaning (review week 2 if you are usure!)\ntokens &lt;- texts_df %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  mutate(word = str_to_lower(word))\n\n# Calculate TTR for each text\nttr_results &lt;- tokens %&gt;%\n  group_by(document, author) %&gt;%\n  summarise(\n    tokens = n(),                    # Count total words\n    types = n_distinct(word),        # Count unique words\n    ttr = types / tokens,            # Calculate TTR\n    .groups = \"drop\"\n  )\n\n# Display results\nttr_results %&gt;%\n  knitr::kable(\n    digits = 3,\n    caption = \"Type-Token Ratio Results\",\n    col.names = c(\"Document\", \"Author\", \"Total Words\", \"Unique Words\", \"TTR\")\n  )\n\n\nType-Token Ratio Results\n\n\nDocument\nAuthor\nTotal Words\nUnique Words\nTTR\n\n\n\n\nA69858\nUnknown\n5942\n1317\n0.222\n\n\nCircle of Commerce\nMisselden\n33894\n4769\n0.141\n\n\n\n\n\nNote: I am using a new function, knitr::kable(x), to display the results as a simple table. It takes whatever is the x (=data frame or matrix) and makes it into a table.\nLet’s visualize the results that we just got:\n\n# Create a bar plot comparing TTR\nggplot(ttr_results, aes(x = author, y = ttr, fill = author)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(ttr, 3)), \n            vjust = -0.5, size = 5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Type-Token Ratio Comparison\",\n    subtitle = \"Higher values = more diverse vocabulary\",\n    x = NULL,\n    y = \"Type-Token Ratio (TTR)\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(0, max(ttr_results$ttr) * 1.15)\n\n\n\n\n\n\n\n\nFrom this first measure, we see that Misselden’s text has a lower TTR than the unknown one. What does this mean, exactly? Well, we can see that in The Circle of Commerce only 14.1% of the words are new word forms (that is, a word string–after normalization–that has not appeared before in exactly that form in the text so far), while for the “mystery” text, 22.2% of the words are new word forms. That is, TTR is a measure of “non-repetition” of word forms. Another way to think about it is that for The Circle of Commerce, if you encounter any random word in the text, that’s a 14.1% chance that it’s a word type you haven’t seen before. So, The Circle of Commerce has lower lexical variety than A69858: it repeats itself more than A69858.\nWarning: longer texts will almost always have a lower TTR score than shorter texts, even if they have a richer vocabulary (can you see why?). Earlier, we saw that A69858 has, in total, 5942 words, while Circle of Commerce has, in total, 33894. The two texts are substantially different in length, so we need to double check with a length-corrected measure (such as Guiraud and MTLD).\nWhy you might still want to compute TTR: it’s a standard across disciplines and you will encounter it in publications, so you need to know what you are looking at. Anyone reading a corpus analysis will recognize it and be able to understand what it represents. TTR is a standard measure of lexical variety. It’s easy to audit and easy to reproduce. It’s also a good first diagnostic check.\n\n\n\nThe Guiraud Index (also called Root TTR) solves the text length problem by using a mathematical correction:\n\\[\\text{Guiraud} = \\frac{\\text{Types}}{\\sqrt{\\text{Tokens}}}\\]\nBy dividing by the square root of tokens instead of the raw count, the Guiraud Index is more stable across different text lengths. This adjustment is a practical approximation that assumes that as the length of a text doubles, its vocabulary does not double. The square root in the Guiraud index is there because vocabulary growth in texts is empirically sublinear: as a text gets longer, new words appear more slowly. The Guiraud Index essentially asks: how many distinct word forms does a text introduce once we partially correct for the fact that longer texts repeat terms more often?\nCalculate Guiraud Index:\n\n# Calculate Guiraud Index for each text\nguiraud_results &lt;- tokens %&gt;%\n  group_by(document, author) %&gt;%\n  summarise(\n    tokens = n(),\n    types = n_distinct(word),\n    guiraud = types / sqrt(tokens),   # Guiraud formula\n    .groups = \"drop\"\n  )\n\n# Display results\nguiraud_results %&gt;%\n  knitr::kable(\n    digits = 3,\n    caption = \"Guiraud Index Results\",\n    col.names = c(\"Document\", \"Author\", \"Total Words\", \"Unique Words\", \"Guiraud Index\")\n  )\n\n\nGuiraud Index Results\n\n\n\n\n\n\n\n\n\nDocument\nAuthor\nTotal Words\nUnique Words\nGuiraud Index\n\n\n\n\nA69858\nUnknown\n5942\n1317\n17.085\n\n\nCircle of Commerce\nMisselden\n33894\n4769\n25.904\n\n\n\n\n\nAnd let’s visualize it:\n\n# Create a bar plot comparing Guiraud Index\nggplot(guiraud_results, aes(x = author, y = guiraud, fill = author)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(guiraud, 2)), \n            vjust = -0.5, size = 5) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(\n    title = \"Guiraud Index Comparison\",\n    subtitle = \"Length-corrected measure of lexical diversity\",\n    x = NULL,\n    y = \"Guiraud Index\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(0, max(guiraud_results$guiraud) * 1.15)\n\n\n\n\n\n\n\n\nThis result gives a very different picture from what we noticed with TTR: The Circle has a more varied vocabulary than the mystery text when we partially account for the length of the text.\nHowever, you want to make sure that when you compare texts using the Guiraud Index, you are looking at texts that are of similar genre and (though less so) similar length. Different types of texts are associated with different demands on vocabulary and rhetorical strategies (for example, a legal argument will require different vocabulary choice than a news report of the same court proceeding). Because the index still depends on text length and genre, there is no universal ‘benchmark’ scale. Instead, values should be interpreted relatively, as we are doing here.\nThis leads us to:\n\n\n\nWhat is MTLD?\nMTLD is the most sophisticated measure we’ll use. It was designed to account for the problems of the measures above. TTR and Guiraud essentially ask: given a text of length N, how many distinct word forms does it contain? MTLD instead asks: how long can a text continue before its lexical diversity drops below a fixed threshold? It works by:\n\nReading through the text sequentially\nCalculating a running TTR as it goes\nCounting how many times the TTR drops below 0.72 (this has been determined empirically across many text types and genres). Once you detect that the document’s TTR drops below 0.72, you count one factor. Then you reset and restart computing TTR on the next chunk of text.\nThe result is the average “length” needed before vocabulary starts repeating and it’s computed by:\n\n\n\n\nThe Measure of Textual Lexical Diversity (MTLD) is computed as:\n\\[\n\\text{MTLD} = \\frac{N}{F}\n\\]\nwhere:\n\nN = total number of word tokens in the text\nF = total number of factors, including any partial factor at the end of the text\n\nIn practice, MTLD is usually computed in both directions (forward and backward through the text), and the final value is the mean of the two:\n\\[\n\\text{MTLD}_{\\text{final}} =\n\\frac{\\text{MTLD}_{\\text{forward}} + \\text{MTLD}_{\\text{backward}}}{2}\n\\]\nHigher MTLD = more diverse vocabulary (the author can go longer before repeating words).\nLet’s compute this second version (both forward and backward through the text using the koRpus package (and use the formatting that it requires). Note: koRpus gives you a number of options which you can find here.\n\n# Function to calculate MTLD for a text\ncalculate_mtld &lt;- function(text_string, doc_name) {\n  # Create a temporary file (koRpus requirement)\n  temp_file &lt;- tempfile(fileext = \".txt\")\n  writeLines(text_string, temp_file)\n  \n  # Tokenize with koRpus\n  tokenized &lt;- tokenize(temp_file, lang = \"en\")\n  \n  # Calculate MTLD\n  mtld_result &lt;- MTLD(tokenized)\n  \n  # Extract the MTLD value\n  mtld_value &lt;- mtld_result@MTLD$MTLD\n  \n  # Clean up\n  unlink(temp_file)\n  \n  return(mtld_value)\n}\n\n# Calculate MTLD for both texts\nmtld_results &lt;- texts_df %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    mtld = calculate_mtld(text, document)\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(document, author, mtld)\n\nLanguage: \"en\"\nLanguage: \"en\"\n\n# Display results\nmtld_results %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"MTLD Results\",\n    col.names = c(\"Document\", \"Author\", \"MTLD\")\n  )\n\n\nMTLD Results\n\n\nDocument\nAuthor\nMTLD\n\n\n\n\nCircle of Commerce\nMisselden\n63.09\n\n\nA69858\nUnknown\n84.17\n\n\n\n\n\nAn R coding note for those of you who are new to coding. The MTLD computation using koRpus is being put together in the curly brackets that define all the steps that the new function we are defining, calculate_mtld, will perform. NB: calculate_mtlddoes not actually compute MTLD, it just takes a text string (our Tidy data), converts it to a file for koRpus, calls in koRpus to compute MTLD, and then returns everything to Tidy.\nLet’s visualize the results:\n\n# Create a bar plot comparing MTLD\nggplot(mtld_results, aes(x = author, y = mtld, fill = author)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(mtld, 1)), \n            vjust = -0.5, size = 5) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"MTLD Comparison\",\n    subtitle = \"Mean Length of Sequential Word Strings (Higher = More Diverse)\",\n    x = NULL,\n    y = \"MTLD Score\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(0, max(mtld_results$mtld) * 1.15)\n\n\n\n\n\n\n\n\nWell, this is interesting and important in understanding how these measures work. Unlike Guiraud, but like TTR, MTLD assigns a greater lexical variety to our mystery text. What MTLD is picking up is a difference in how lexical variety is distributed across the text.\nTTR and Guiraud only care about how many distinct word forms there are overall in the texts. They are both global measures of variety.\nMTLD is sensitive to how repetition is patterned through text “time” (think of reading the text as your measure of text “time”). A higher MTLD in the mystery text means that it sustains lexical variety more consistently across its length while Misselden’s lower MTLD means that The Circle includes repetition-heavy chunks earlier on and more often (even though it globally, when adjusted for length, ends up with more unique token types).\nGive this kind of result, we expect that in The Circle, Misselden includes lots of repeated terms and phrases within sections (local repetition as he perhaps defines and explains a given concept/idea), but overall introduces more terms per unit of text length. The mystery text, instead, distributes its vocabulary more evenly. A69858 uses fewer unique words in total, but sustain more lexical variety within any given passage.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 05: Text Representation (2)"
    ]
  },
  {
    "objectID": "week-05-representation.html#measures-of-syntactic-complexity",
    "href": "week-05-representation.html#measures-of-syntactic-complexity",
    "title": "Week 05: Text Representation (2)",
    "section": "Measures of Syntactic Complexity",
    "text": "Measures of Syntactic Complexity\nSo much for vocabulary measures! What if we want to understand the complexity (or lack thereof) of the syntax of a text?\nThe core idea that we will use for this is to measure structure via frequency counts (e.g., number of clauses, number of dependent clauses), ratios (e.g., clauses per sentence; dependent clauses per clause), or length indices (e.g., mean length of clause in words).\nNote: research in syntax complexity is particularly well-represented in second language acquisition research (see, for example here for a strong discussion).\nWe are still working with the same two texts as above!\n\ntexts_df &lt;- tibble(document = c(\"Circle of Commerce\", \"A69858\"),\n                   author = c(\"Misselden\", \"Unknown\"),\n                   text = c(circle, mystery)\n                   )\n\ntexts_df\n\n# A tibble: 2 × 3\n  document           author    text                                             \n  &lt;chr&gt;              &lt;chr&gt;     &lt;chr&gt;                                            \n1 Circle of Commerce Misselden \"THE CIRCLE OF COMMERCE. The Prooeme. HERODOTVS …\n2 A69858             Unknown   \"THe Author craves leave in the first place, to …\n\n\nWe are going to use something new UDPipe to: tokenize, do parts of speech tagging, lemmatize, and do dependency parsing. Note: for Early Modern texts there is a package out of Northwestern (MorphAdorner) that performs better. Since I don’t have any Early Modernists this semester, I am going to stick with a more generalizable workflow.\n\n#Load an English UD (= Univesal Dependencies) model ONCE\n\nmodel_info &lt;- udpipe_download_model(language = \"english-ewt\")\nud_model &lt;- udpipe_load_model(model_info$file_model)\n\nI chose english-ewt because it’s widely adopted and well-documented for English prose. It captures the core syntactic relations and it does a decent job on pre-Modern texts. I would not use this for my own research, but that’s a niche issue.\nNow, let’s annotate both texts using UDPipe:\n\nanno_df &lt;- texts_df %&gt;%\n  mutate(\n    # Parse each text with the UD parser; set doc_id to our document name\n    anno = map2(text, document, ~ udpipe_annotate(ud_model, x = .x, doc_id = .y) %&gt;%\n      as.data.frame())\n  ) %&gt;%\n  # Keep only parsed annotations, then unnest into rows\n  select(anno) %&gt;%\n  unnest(anno) %&gt;%\n  # Use the UD doc_id as our document label (and drop any duplicates cleanly)\n  rename(document = doc_id) %&gt;%\n  # Select columns for syntactic analysis\n  select(\n    document,\n    paragraph_id,\n    sentence_id,\n    token_id,\n    token,\n    lemma,\n    upos,          # part of speech\n    feats,         # grammatical features (e.g., verb form)\n    head_token_id, # head of dependency relation\n    dep_rel        # dependency relation type\n  )\n\nanno_df %&gt;% glimpse()\n\nRows: 47,121\nColumns: 10\n$ document      &lt;chr&gt; \"Circle of Commerce\", \"Circle of Commerce\", \"Circle of C…\n$ paragraph_id  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ sentence_id   &lt;int&gt; 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ token_id      &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"…\n$ token         &lt;chr&gt; \"THE\", \"CIRCLE\", \"OF\", \"COMMERCE\", \".\", \"The\", \"Prooeme\"…\n$ lemma         &lt;chr&gt; \"the\", \"circle\", \"of\", \"Commerce\", \".\", \"the\", \"Prooeme\"…\n$ upos          &lt;chr&gt; \"DET\", \"NOUN\", \"ADP\", \"PROPN\", \"PUNCT\", \"DET\", \"NOUN\", \"…\n$ feats         &lt;chr&gt; \"Definite=Def|PronType=Art\", \"Number=Sing\", NA, \"Number=…\n$ head_token_id &lt;chr&gt; \"2\", \"0\", \"4\", \"2\", \"2\", \"2\", \"0\", \"2\", \"6\", \"4\", \"4\", \"…\n$ dep_rel       &lt;chr&gt; \"det\", \"root\", \"case\", \"nmod\", \"punct\", \"det\", \"root\", \"…\n\n\nFor head_token_id, what we are doing at (*) is important in creating a dependency tree. Here’s a toy example:\n\n# Create an example parsed sentence\nexample_sentence &lt;- tibble(\n  token = c(\"The\", \"big\", \"dog\", \"barks\"),\n  token_id = c(1, 2, 3, 4),\n  head_token_id = c(3, 3, 4, 0),\n  Relationship = c(\n    '\"The\" depends on word #3 (dog)',\n    '\"big\" depends on word #3 (dog)',\n    '\"dog\" depends on word #4 (barks)',\n    '\"barks\" is the ROOT (doesn\\'t depend on anything)'\n  )\n)\n\nexample_sentence %&gt;%\n  knitr::kable(\n    caption = 'Example: Dependency structure of \"The big dog barks\"',\n    align = c(\"l\", \"c\", \"c\", \"l\")\n  )\n\n\nExample: Dependency structure of “The big dog barks”\n\n\n\n\n\n\n\n\ntoken\ntoken_id\nhead_token_id\nRelationship\n\n\n\n\nThe\n1\n3\n“The” depends on word #3 (dog)\n\n\nbig\n2\n3\n“big” depends on word #3 (dog)\n\n\ndog\n3\n4\n“dog” depends on word #4 (barks)\n\n\nbarks\n4\n0\n“barks” is the ROOT (doesn’t depend on anything)\n\n\n\n\n\nThere will be grammar: we are going to define some syntactic complexity features by using dependency relations. In the code below, we are going to create binary flags for different syntactic structures.\n\nsyntax_df &lt;- anno_df %&gt;%\nmutate(\nis_word = upos != \"PUNCT\", #&lt;--is it a word (and not punctuation?)\n\n\n# Is this an independent clause? finite verbs are proxy for indipendent clauses\nis_clause = (upos %in% c(\"VERB\", \"AUX\")) &\n            str_detect(coalesce(feats, \"\"), \"VerbForm=Fin\"),\n\n# Dependent clause? \nis_dep_clause = dep_rel %in% c(\n  \"advcl\", #adverbial clause \n  \"ccomp\", # clausal complement\n  \"xcomp\", #open clausal complement\n  \"acl\", #adnomial clause\n  \"acl:relcl\" #relative clause\n),\n\n# Is this coordination? That is, does it use \"and\" \"or\" etc.?\nis_coord = dep_rel %in% c(\"conj\", \"cc\"),\n\n# Nominal complexity: these relations make noun phrases more complex\nis_complex_nominal = dep_rel %in% c(\n  \"amod\", # adjective modifier (\"big cup\")\n  \"nmod\", #nominal modifier (\"cup of tea\")\n  \"compound\", # compound (\"lemon tea\")\n  \"appos\" #apposition (\"tea, my favorite!\")\n)\n\n)\n\nA quick note about punctuation: we filter out punctuation because punctuation itself doesn’t add to a sentence complexity. While someone typing ‘I “like” lemon tea’ changes the meaning from ‘I like lemon tea,’ it doesn’t add to syntactic complexity.\nIf you want to know a bit more about clauses, you can find the full list here. But a quick summary for our needs is below:\n\n\n\n\nTable 1\n\n\n\n\n\n\nRelation\nFull Name\nWhat It Modifies\nExample\nUD Documentation\n\n\n\n\nadvcl\nAdverbial clause\nVerb/predicate\nI left because I was tired\nadvcl\n\n\nccomp\nClausal complement\nVerb (as object)\nShe said that he arrived\nccomp\n\n\nxcomp\nOpen clausal complement\nVerb (shares subject)\nI want to leave\nxcomp\n\n\nacl\nAdnominal clause\nNoun\nThe dog sleeping there\nacl\n\n\nrelcl\nRelative clause\nNoun\nThe book that I read\nacl:relcl\n\n\n\n\n\n\n\n\nLet’s check what we did:\n\n# looking at the first 20 rows in the data frame\nsyntax_df %&gt;% \n  select(document, token, upos, is_clause, is_dep_clause) %&gt;%\n  head(20)\n\n# A tibble: 20 × 5\n   document           token     upos  is_clause is_dep_clause\n   &lt;chr&gt;              &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;     &lt;lgl&gt;        \n 1 Circle of Commerce THE       DET   FALSE     FALSE        \n 2 Circle of Commerce CIRCLE    NOUN  FALSE     FALSE        \n 3 Circle of Commerce OF        ADP   FALSE     FALSE        \n 4 Circle of Commerce COMMERCE  PROPN FALSE     FALSE        \n 5 Circle of Commerce .         PUNCT FALSE     FALSE        \n 6 Circle of Commerce The       DET   FALSE     FALSE        \n 7 Circle of Commerce Prooeme   NOUN  FALSE     FALSE        \n 8 Circle of Commerce .         PUNCT FALSE     FALSE        \n 9 Circle of Commerce HERODOTVS NOUN  FALSE     FALSE        \n10 Circle of Commerce in        ADP   FALSE     FALSE        \n11 Circle of Commerce his       PRON  FALSE     FALSE        \n12 Circle of Commerce CLIO      NOUN  FALSE     FALSE        \n13 Circle of Commerce ,         PUNCT FALSE     FALSE        \n14 Circle of Commerce reportes  VERB  TRUE      FALSE        \n15 Circle of Commerce that      SCONJ FALSE     FALSE        \n16 Circle of Commerce CROESVS   PROPN FALSE     FALSE        \n17 Circle of Commerce King      PROPN FALSE     FALSE        \n18 Circle of Commerce of        ADP   FALSE     FALSE        \n19 Circle of Commerce LYDIA     PROPN FALSE     FALSE        \n20 Circle of Commerce had       VERB  TRUE      TRUE         \n\n\nNow that we have parsed our texts and identified syntactic features, we can measure complexity. Let’s do this through five measures. Important: these measures are meant for relative comparison between documents!\n\nCalculate Mean Length of Sentence (MLS): how long are sentences on average?\nCalculate Clausal Density (C/S): how many clauses per sentence?\nSubordination (DC/C, DC/S): how much embedding?\nCoordination (Coord/C, Coord/S): how much coordination?\nPhrasal Complexity (CN/C, CN/S): how complex are noun phrases?\n\nBefore we jump into the details of these measures, we need to aggregate at the sentence level and count syntactic features for each individual sentence:\n\nsentence_df &lt;- syntax_df %&gt;%\n  filter(is_word) %&gt;%           #count words (not punctuation)\n  group_by(document, sentence_id) %&gt;%   #group by document and sentence\n  summarise(\n    words          = n(),   #number of words per sentence\n    clauses        = sum(is_clause), # number of clauses per sentence\n    dep_clauses    = sum(is_dep_clause), #number of dependent clauses per sentence\n    .groups = \"drop\"\n  )\n\nsentence_df\n\n# A tibble: 1,872 × 5\n   document sentence_id words clauses dep_clauses\n   &lt;chr&gt;          &lt;int&gt; &lt;int&gt;   &lt;int&gt;       &lt;int&gt;\n 1 A69858             1    29       1           2\n 2 A69858             2    44       3           2\n 3 A69858             3    45       6           5\n 4 A69858             4    28       1           3\n 5 A69858             5     6       0           0\n 6 A69858             6    39       1           0\n 7 A69858             7    23       0           2\n 8 A69858             8    64       7           5\n 9 A69858             9    35       3           3\n10 A69858            10    27       1           2\n# ℹ 1,862 more rows\n\n\n1. Mean length of sentence: now we measure the average sentence length by number of words. The assumption behind this step is that longer sentences tend to be more syntactically complex.\n\nmls_df &lt;- sentence_df %&gt;%\ngroup_by(document) %&gt;%\nsummarise(\nMLS = mean(words), # Average words per sentence\n.groups = \"drop\"\n)\n\n# Let's take a look\nmls_df \n\n# A tibble: 2 × 2\n  document             MLS\n  &lt;chr&gt;              &lt;dbl&gt;\n1 A69858              22.9\n2 Circle of Commerce  21.2\n\n\n2. Overall Sentence Complexity (C/S = Clauses per sentence): while longer sentences tend to be more complex, that’s not the only way complexity works. A short sentence in English, such as “The dog I saw bit my cousin,” can be structurally complex. The next step is to count clauses. More clauses allow for the possibility of more syntactic complexity via coordination and subordination.\n\n#Calculate clauses per sentence\nclausal_density_df &lt;- sentence_df %&gt;%\ngroup_by(document) %&gt;%\nsummarise(\nsentences = n(),\nclauses   = sum(clauses),\nC_per_S   = clauses / sentences,\n.groups = \"drop\"\n)\n\n# Let's check and how do they compare?\nclausal_density_df\n\n# A tibble: 2 × 4\n  document           sentences clauses C_per_S\n  &lt;chr&gt;                  &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt;\n1 A69858                   262     451    1.72\n2 Circle of Commerce      1610    2581    1.60\n\n\n3. Subordination (DC/C and DC/S): this measures how much the author of the text uses dependent clauses (subordination). Subordination creates hierarchical, embedded structures. For example, think of this (monster) sentence: “Although the merchants insisted that the shortage was temporary, and because the city council feared that the unrest would spread unless action were taken immediately, the King decided that he would issue new regulations after he had consulted his advisors.”\nIt contains multiple embedded dependent clauses (“Although…”, “that the shortage…”, “because…”, “unless…”, “that he would…”, “after he had…”), demonstrating how subordination creates a layered, hierarchical structure that is characteristic of complex syntax.”\n\nsubordination_df &lt;- sentence_df %&gt;%\ngroup_by(document) %&gt;%\nsummarise(\nclauses = sum(clauses),\ndep_clauses = sum(dep_clauses),\nsentences = n(),\nDC_per_C = dep_clauses / pmax(clauses, 1), #avoid division by 0 (shouldn't happen, but if no clauses were detected in the sentence, we still want it to run)\nDC_per_S = dep_clauses / sentences,\n.groups = \"drop\"\n)\n\nsubordination_df\n\n# A tibble: 2 × 6\n  document           clauses dep_clauses sentences DC_per_C DC_per_S\n  &lt;chr&gt;                &lt;int&gt;       &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 A69858                 451         306       262    0.678     1.17\n2 Circle of Commerce    2581        1795      1610    0.695     1.11\n\n\nWhat do these measures mean?\nDC/C (Dependent Clauses per Clause): What proportion of clauses are dependent? Higher value means that there is more subordination relative to total clauses.\nDC/S (Dependent Clauses per Sentence): How many dependent clauses in each sentence on average? The higher the measure, the more embeddings/subordination per sentence.\n4. Coordination (Coord/C and Coord/S): we are next going to measure how much the author use **coordination** (linking clauses with “and”, “but”, “or”). This is a measure of how “horizontal,” rather than hierarchical subordination.\n\ncoordination_df &lt;- syntax_df %&gt;%\ngroup_by(document) %&gt;%\nsummarise(\ncoord_relations = sum(is_coord),\nclauses         = sum(is_clause),\nsentences       = n_distinct(sentence_id),\nCoord_per_C     = coord_relations / pmax(clauses, 1),\nCoord_per_S     = coord_relations / sentences,\n.groups = \"drop\"\n)\n\ncoordination_df\n\n# A tibble: 2 × 6\n  document           coord_relations clauses sentences Coord_per_C Coord_per_S\n  &lt;chr&gt;                        &lt;int&gt;   &lt;int&gt;     &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 A69858                         691     451       262        1.53        2.64\n2 Circle of Commerce            4071    2581      1611        1.58        2.53\n\n\nThese are “densities” of coordination:\nCoord/C (Coordination per Clause): Total number of coordination (“and”, “or”, “but”…) markers divided by number of clauses.\nCoord/S (Coordination per Sentence): Total number of coordination markers divided by number of sentences.\nNote: our coordination measure looks high (&gt;2). I bolded “markers” in “coordination markers” because what we are counting is any coordination marker, including in, for example, noun phrases (“trade and commerce”) or in a list of coordinated adjectives (“lawful, honest, and profitable”). So you are not just counting the number of coordinated clauses, but rather the frequency of parallel syntactic structures.\n5. Phrasal Complexity (CN/C and CN/S): writing increases in complexity not just through clauses, but through elaborated phrases. This is especially common in specialized or technical writing. Noun phrases are phrases built around a noun (or pronoun) that act as a unit in a sentence (if you are rusty on noun phrases aka nominal phrases, check this wikipedia entry).\n\nnominal_df &lt;- syntax_df %&gt;%\ngroup_by(document) %&gt;%\nsummarise(\ncomplex_nominals = sum(is_complex_nominal),\nclauses          = sum(is_clause),\nsentences        = n_distinct(sentence_id),\nCN_per_C         = complex_nominals / pmax(clauses, 1),\nCN_per_S         = complex_nominals / sentences,\n.groups = \"drop\"\n)\n\nnominal_df\n\n# A tibble: 2 × 6\n  document           complex_nominals clauses sentences CN_per_C CN_per_S\n  &lt;chr&gt;                         &lt;int&gt;   &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 A69858                          994     451       262     2.20     3.79\n2 Circle of Commerce             5828    2581      1611     2.26     3.62\n\n\nAs a way to understand the difference between clausal and phrasal complexity, look over this example:\n\nMore clauses: The ship was large and fast and was in port. (Not great construction, but plenty of phrases).\nMore complex phrase: The large fast ship was in port.\n\nLet’s bring everything together:\n\n# Combine all measures\nall_measures &lt;- mls_df %&gt;%  # ← Added mls_df %&gt;%\n  left_join(clausal_density_df %&gt;% select(document, C_per_S), by = \"document\") %&gt;%\n  left_join(subordination_df %&gt;% select(document, DC_per_C, DC_per_S), by = \"document\") %&gt;%\n  left_join(coordination_df %&gt;% select(document, Coord_per_C, Coord_per_S), by = \"document\") %&gt;%\n  left_join(nominal_df %&gt;% select(document, CN_per_C, CN_per_S), by = \"document\")\n\nall_measures %&gt;%\n  knitr::kable(\n    digits = 2,\n    col.names = c(\"Document\", \"MLS\", \"C/S\", \"DC/C\", \"DC/S\", \n                  \"Coord/C\", \"Coord/S\", \"CN/C\", \"CN/S\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocument\nMLS\nC/S\nDC/C\nDC/S\nCoord/C\nCoord/S\nCN/C\nCN/S\n\n\n\n\nA69858\n22.85\n1.72\n0.68\n1.17\n1.53\n2.64\n2.20\n3.79\n\n\nCircle of Commerce\n21.23\n1.60\n0.70\n1.11\n1.58\n2.53\n2.26\n3.62\n\n\n\n\n\nAnd let’s visualize it\n\n# Reshape for plotting\nsyntax_long &lt;- all_measures %&gt;%  # ← Added %&gt;%\n  pivot_longer(\n    cols = -document,\n    names_to = \"Measure\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(\n    Category = case_when(\n      Measure == \"MLS\" ~ \"Sentence Length\",\n      Measure == \"C_per_S\" ~ \"Clausal Density\",\n      Measure %in% c(\"DC_per_C\", \"DC_per_S\") ~ \"Subordination\",\n      Measure %in% c(\"Coord_per_C\", \"Coord_per_S\") ~ \"Coordination\",\n      Measure %in% c(\"CN_per_C\", \"CN_per_S\") ~ \"Phrasal Complexity\"\n    )\n  )\n\n# Plot\nggplot(syntax_long, aes(x = Measure, y = Value, fill = document)) +\n  geom_col(position = \"dodge\", width = 0.7) +\n  facet_wrap(~Category, scales = \"free\", ncol = 2) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Syntactic Complexity: Complete Profile\",\n    subtitle = \"Comparing multiple dimensions of syntactic complexity\",\n    x = NULL,\n    y = \"Value\",\n    fill = \"Document\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\nOK, we have numbers and pretty pictures, but what does this all mean? Looking at these numbers, my take away is that the two texts are fairly similar in syntactic complexity. The mystery text has marginally longer sentences with more clauses, while Misselden’s text shows slightly more elaborate noun phrases. But nothing remarkable. Both texts employ comparable levels of subordination and coordination, suggesting similar levels of structural sophistication in their writing.\nGiven that the results for both texts are nearly identical (differences of 0.1-0.2 across most measures), I (and most researchers) would conclude these texts show no meaningful difference in syntactic complexity. Of course, this is a rather limited example, where I am comparing just two texts. If I had a large corpus, then I would want to take a more rigorous approach. We will come back to this over the next couple of weeks.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 05: Text Representation (2)"
    ]
  },
  {
    "objectID": "week-07-word2vec.html",
    "href": "week-07-word2vec.html",
    "title": "Week 07: Word2Vec and LDA introduction",
    "section": "",
    "text": "Before working with text in today’s tutorial, we are going to have an introduction to getting started in Python. This is for those of you who haven’t worked in Python before. Note: this doesn’t mean that you want to forget everything that you learned in R up to now! The analyses that we conducted in R allowed us to develop a granular understanding of how to represent and measure textual features. The tasks we will do in Python will be less “transparent”, but you don’t want to give up the intuition you developed so far.\nIf you have never worked with Python, you can find a beginner friendly, step by step guide on how to download Python and set up VS Code (the environment you will use for Python–think of it as the Python version of R studio): here.\nIf you have worked with Python before, you can use whichever environment you want.\nQuick note: you want Python 3.11.x. The bottleneck is scikit-learn, which doesn’t play well with the most recent version of Python.\nA second note about learning Python: If you are new to Python, the first thing that you will see as you look at tutorials is that Python is an object-oriented language. I have added a link that takes you to realpython.com, which is a great resource for learning Python. But don’t worry too much about understanding all the finer points of this from the get-go! The TA’s and I will help you use the code that I give you as an example and then you can start learning more about objects and classes and all that jazz.\n\nAs we go along this tutorial: I am re-introducing the training wheels. I am including links to some of the major Python concepts as they come up in the code!\n\n\n\nThe first few steps that we are going to take are very similar to what we did in R: the logic is the same, but the syntax is different. The first thing that we need to do is to read the text and then check that things look correct!\nIn R, what we do at the beginning is:\n\nload a text file into memory,\ncheck how long it is,\npreview the beginning,\nand try to spot obvious encoding or OCR problems early (that is, does the text look weird?).\n\nWe are now going to do the same step in Python. The main difference is that we are going to use a path-handling tool in Python, Path. We are then going to actually read the file with this line: text = path.read_text(encoding=\"utf-8\", errors=\"replace\"). Here, I am asking Python to decode the text using utf-8 and to replace any unreadable characters instead of crashing. The replacement is the Unicode replacement character: �.\n\nfrom pathlib import Path\n\npath = Path(\"texts/wealth.txt\")\ntext = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n# The lines above read the wealth.txt into a long string\n\nprint(\"Characters:\", len(text)) #character count \n\nCharacters: 2411473\n\nprint(\"Lines:\", text.count(\"\\n\") + 1) #line count\n\nLines: 34544\n\n# taking a look that the text looks ok. Similar to head() and slice() in R\nprint(\"\\n--- START ---\\n\")\n\n\n--- START ---\n\nprint(text[:800])\n\nAn Inquiry into the Nature and Causes of the Wealth of Nations\n\nby Adam Smith\n\n\n\n\nContents\n\n\n INTRODUCTION AND PLAN OF THE WORK.\n\n BOOK I. OF THE CAUSES OF IMPROVEMENT IN THE PRODUCTIVE\nPOWERS OF labor, AND OF THE ORDER ACCORDING TO WHICH ITS PRODUCE IS NATURALLY\nDISTRIBUTED AMONG THE DIFFERENT RANKS OF THE PEOPLE.\n CHAPTER I. OF THE DIVISION OF labor.\n CHAPTER II. OF THE PRINCIPLE WHICH GIVES OCCASION TO THE\nDIVISION OF labor.\n CHAPTER III. THAT THE DIVISION OF labor IS LIMITED BY\nTHE EXTENT OF THE MARKET.\n CHAPTER IV. OF THE ORIGIN AND USE OF MONEY.\n CHAPTER V. OF THE REAL AND NOMINAL PRICE OF\nCOMMODITIES, OR OF THEIR PRICE IN labor, AND THEIR PRICE IN MONEY.\n CHAPTER VI. OF THE COMPONENT PART OF THE PRICE OF COMMODITIES.\n CHAPTER VII. OF THE NATURAL AND MARKET PRICE OF COMMODITIES.\n CHA\n\nmid = len(text)//2\nprint(\"\\n--- MIDDLE SLICE ---\\n\")\n\n\n--- MIDDLE SLICE ---\n\nprint(text[mid:mid+800])\n\n Though there are in Europe indeed, a few towns which, in same respects,\n      deserve the name of free ports, there is no country which does so.\n      Holland, perhaps, approaches the nearest to this character of any, though\n      still very remote from it; and Holland, it is acknowledged, not only\n      derives its whole wealth, but a great part of its necessary subsistence,\n      from foreign trade.\n\n      There is another balance, indeed, which has already been explained, very\n      different from the balance of trade, and which, according as it happens to\n      be either favorable or unfavorable, necessarily occasions the prosperity\n      or decay of every nation. This is the balance of the annual produce and\n      consumption. If the exchangeable value of the annual produce, it has\n \n\n\nThe print() function is something that will come up over and over again. If you get lost by how I am using it, do click on this explanation.\nThe next step is to tokenize the text. This is the Python version of unnest_tokens() in tidytext. But in Python, we have to do this using a regex: so, hopefully, you feel comfortable with the regex unit from Week 2. We are also going to lowercase. This tokenizer keeps alphabetic words (and contractions), so it ignores numbers and hyphenated compounds; that’s a simplification for today, not a general rule.\nSince we also want to run some checks on our process (as we did in R), we are going to inspect the most frequent tokens using Counter, the Python parallel to tidytext’s count(word, sort =TRUE).\nMaking things look nicer: You can skip this explanation, if you are new to Python. This is equivalent to: print(w, c). I am just trying to make the display more readable for the class!\nIn the next block below, I am going to use an f-string inside print(). An f-string is a way to format text, so f\"{w:&gt;12} {c}\" means (working inside out):\n\ntake the value of w and convert it to text, right-align it inside a space 12 character wide;\nthen when you print separate the columns (the two spaces between {w:&gt;12} and {c})\n\n\nimport re\nfrom collections import Counter\n\ntokens = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text.lower())\nprint(\"Tokens:\", len(tokens))\n\nTokens: 380080\n\nprint(\"Unique tokens:\", len(set(tokens)))\n\nUnique tokens: 9475\n\ncounts = Counter(tokens)\nprint(\"\\nTop 25 tokens:\")\n\n\nTop 25 tokens:\n\nfor w, c in counts.most_common(25):\n    print(f\"{w:&gt;12}  {c}\")\n\n         the  32244\n          of  24295\n          to  11708\n         and  10284\n          in  9637\n           a  6678\n          it  5392\n       which  4824\n          is  4685\n          be  3828\n        that  3818\n          or  3211\n          as  3088\n          by  2981\n         for  2976\n       their  2523\n        this  2231\n         not  2231\n         are  2168\n        they  2131\n        have  2119\n        upon  2112\n        from  1970\n         but  1962\n       those  1925\n\n\nThe library Scikit-learn is going to allow us to import a standard list of stopwords. More importantly, Scikit-learn is a machine learning library for Python and we will use it more than once!\nWe are going to convert the list of pre-defined stopwords into a set because it will make filtering tokens more efficient. We can then use the in-place OR operator |= to add all the elements of our custom stopwords to the standard stopwords (the link at “sets” will also explain |=).\nAside: if you go to the tutorial, you might see that there is another OR operator | . This operator creates a new set. I am just modifying the preexisting one by adding the custom stopwords.\n\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nstopwords = set(ENGLISH_STOP_WORDS) #turn it into a set\n\n# This is optional and I am only using this as an example of syntax (hence the choice of silly terms)\ncustom_stopwords = {\"barnacle\", \"putine\"}\n\nstopwords |= custom_stopwords\n\n# let's check how many stopwords we have and what they look like\nprint(\"Stopwords loaded:\", len(stopwords))\n\nStopwords loaded: 320\n\nprint(\"Sample:\", sorted(list(stopwords))[:25])\n\nSample: ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any']\n\n\nNow that we have our stopwords as we want them, we can apply it to our tokens. We are also going to remove really short tokens (fewer than 3 characters). Something that sometimes throws people off: I am stating the length requirement as a strict inequality. Now look at the code below: do you understand why have len(t) &gt;= 3 ?\n\nfrom collections import Counter\n\nclean_tokens = [t for t in tokens if t not in stopwords and len(t) &gt;= 3]\n\nprint(\"Tokens (raw):\", len(tokens))\n\nTokens (raw): 380080\n\nprint(\"Tokens (clean):\", len(clean_tokens))\n\nTokens (clean): 148917\n\nprint(\"Unique tokens (clean):\", len(set(clean_tokens)))\n\nUnique tokens (clean): 9156\n\nclean_counts = Counter(clean_tokens)\n\nprint(\"\\nTop 25 tokens after cleaning:\")\n\n\nTop 25 tokens after cleaning:\n\nfor w, c in clean_counts.most_common(25):\n    print(f\"{w:&gt;12}  {c}\")\n\n       great  1583\n       price  1264\n     country  1240\n     greater  1085\n       labor  1011\n       trade  970\n     produce  945\n   different  855\n    quantity  797\n       value  794\n      people  779\n       money  773\n        land  720\n     revenue  691\n      silver  661\n     capital  657\n        time  636\n       stock  601\n       goods  585\n      market  582\n   countries  577\n     expense  561\n  particular  513\n         tax  513\n        gold  508\n\n\n\n\n\nSo far, the steps should be very familiar (with a different coding language). The next step is driven by the model itself. Since LDA does not operate on individual words or on a continuous token stream, we are going to be constrained by the structure that it needs. Word2Vec can work on the entire Wealth of Nations at once, but for the sake of continuity, we will stick to the same structure for both models.\nLDA needs to work with documents or, as in our case, pre-defined segments of a longer text. These are the assumptions behind LDA:\n\neach document is a mixture of topics,\neach topic is a distribution over words.\n\nThis forces us to decide what counts as a “document” in our corpus. In some cases, this will be obvious to you: if you have a collection of political speeches, then each speech is a document. In other cases, we have to make a decision. The Wealth of Nations is one long text and we can segment it in a number of ways. There are two common options (each with its one trade-offs): one is to use internal subdivisions, such as chapters; the other is to use fixed-sized segments. We are going to go for the second one and create 800-token chunks.\nNote: both options are legitimate choices. I am picking fixed-sized segments because I know that the chapters in The Wealth of Nations are of varied length and that a single topic (such as labor or political economy) is split over several chapters. The approach I take here also ensures that no single segment dominates the model simply because of its length. LDA tends to work better when documents are not extremely long. But this choice comes with a trade-off: I am losing track of Adam Smith’s structure. For different purposes (that is, for an actual project rather than a class tutorial), I might choose to “chunk” by chapter instead.\n\n# Segmenting\n\nSEGMENT_SIZE = 800  # fixed-size \"document.\" We can always adjust length\n# slice the entire list of tokens from token #0 to the last token, breaking it into chunks of SEGMENT_SIZE.\nsegments = [\n    clean_tokens[i:i+SEGMENT_SIZE] \n    for i in range(0, len(clean_tokens), SEGMENT_SIZE)\n]\n\n#inspect the segmentation\nprint(\"Number of segments:\", len(segments))\n\nNumber of segments: 187\n\nprint(\"First segment length:\", len(segments[0]))\n\nFirst segment length: 800\n\nprint(\"Last segment length:\", len(segments[-1]))\n\nLast segment length: 117\n\n\nEach segment is now a document for LDA: we have 186 segments, which is a good document count for LDA, and segment sizes are consistent (800; last shorter, which is what we expect). Note: I picked 800 somewhat arbitrarily, but it is meant to capture approximately 2-3 paragraphs per segment given what I know of Smith’s writing. Let’s see what word counts in a segment look like:\n\nfrom collections import Counter\n\nsegment_counts = [Counter(seg) for seg in segments]\n\n# check: top words in segment 0\nprint(\"Top words in segment 0:\")\n\nTop words in segment 0:\n\nfor w, c in segment_counts[0].most_common(15):\n    print(f\"{w:&gt;12}  {c}\")\n\n     chapter  33\n       labor  30\n   different  19\n        book  12\n     produce  11\n    division  11\n    employed  11\n     nations  10\n     society  10\n      number  10\n        work  9\n       stock  8\n  particular  8\n      people  7\n       great  7\n\n\nThis also looks reasonable for the opening of the book (chapter, book, division, labor, work, nations, society) and it means that we can proceed to the next step.\nAt this point, we now have two complementary representations of the same text:\n\nsegments: each segment is an ordered sequence of tokens\nsegment_counts: each segment is represented by word counts (bag-of-words)\n\nWe are going to need both representations, but for two different models. Word2Vec learns meaning from local context and word order, so it can work directly with segments. On the other hand, LDA ignores word order and models documents as mixtures of topics, so it requires a bag-of-words representation, which we will build explicitly in the next section.\nWe will begin with Word2Vec, because it extends ideas you have already encountered: a word’s meaning is shaped by the contexts in which it appears. For a good resource on understanding Word2Vec, see here. But, in short, instead of storing explicit co-occurrence counts, Word2Vec learns dense vectors for each word. These vectors are learned by optimizing a predictive task: given a word, predict its surrounding context (or vice versa). The result is a compressed, smoothed representation of the same information we previously counted directly.\nOK, what does this mean in practice for us? Each segment is a sequence of tokens and Word2Vec slides a fixed-size context window across these sequences and learns vector representations that are good at predicting nearby words.\n\n# Key step: Word2Vec training\n\nfrom gensim.models import Word2Vec  \n\nw2v = Word2Vec(\n    sentences=segments,     # each segment is a list of tokens\n    vector_size=100,        # dimensionality\n    window=5,               # context window (parallel to your co-occurrence window)\n    min_count=10,           # ignore very rare words\n    workers=4,              # number of CPU cores for parallel processing\n    sg=1                    # 1=skip-gram, 0=CBOW\n)\n\nprint(\"Vocabulary size:\", len(w2v.wv.key_to_index))\n\nVocabulary size: 2317\n\n\nWe have to make some choices when we train Word2Vec:\n\nvector_size=100: each word is represented as a 100-dimensional vector (if we choose higher dimension, we capture more information about each word, but it’s slower and requires more text data). Common values in practice: 100-300. 100 is often the default.\nwindow=5 matches what we did in our co-occurrence calculation.\nmin_count=10 keeps the vocab manageable (and its good for a class exercise). Note: the exact choice of 10 (instead of, say, 15 or 20) is based on trail and error. Given that we have 1,395 words in the vocabulary after filtering, I don’t want to be too agressive. For a larger and more varied corpus, I might go for 20.\nImportant: if you have an older machine and it’s freezing; change workers = 4 to 2 or even 1.\nsg=1 (skip-gram) is usually better for capturing rarer, more specific relations.\n\nNow that the model is trained, we can inspect its learned semantic space by asking for a word’s nearest neighbors. The goal is to use cosine similarity to find which word vectors are closest to a given/target word in the vector space. In this case, let’s look at “trade,” “labor,” “price,” “capital,” and “market” to see what kind of results we get.\n\nquick aside if you haven’t worked in Python before: this is the first time we are defining a function using the def keyword.\n\n\n# Nearest neighbors (cosine similarity)\n\ndef show_neighbors(word, topn=10):\n    if word not in w2v.wv:\n        print(f\"'{word}' not in vocabulary.\")\n        return\n    print(f\"\\nNearest neighbors for '{word}':\")\n    for w, sim in w2v.wv.most_similar(word, topn=topn):\n        print(f\"{w:&gt;12}  {sim:.3f}\")\n\nfor target in [\"trade\", \"labor\", \"price\", \"capital\", \"market\"]:\n    show_neighbors(target, topn=10)\n\n\nNearest neighbors for 'trade':\n    carrying  0.890\n       round  0.840\n     carried  0.828\n    branches  0.822\n       carry  0.817\nadvantageous  0.814\n    shipping  0.812\n      branch  0.803\n      forced  0.798\n     returns  0.794\n\nNearest neighbors for 'labor':\n        adds  0.873\n      motion  0.864\n        puts  0.863\n     laborer  0.862\n distributed  0.852\n     enables  0.834\n        vary  0.830\n constitutes  0.829\n  recompence  0.829\n   increases  0.823\n\nNearest neighbors for 'price':\n       rises  0.855\n        sink  0.821\n         low  0.814\n      barley  0.812\n     butcher  0.810\n        rise  0.804\n        meat  0.803\n       lower  0.798\n       bread  0.798\n       risen  0.794\n\nNearest neighbors for 'capital':\n    capitals  0.889\n       stock  0.850\n    replaces  0.847\n    employed  0.837\n       fixed  0.832\n   withdrawn  0.826\n     replace  0.826\n    reserved  0.822\n     employs  0.819\n      unless  0.817\n\nNearest neighbors for 'market':\n    supplied  0.810\n   effectual  0.810\n     thither  0.783\n     brought  0.770\n        come  0.767\n   plentiful  0.756\n    bringing  0.752\n      supply  0.744\n     cheaper  0.742\n        send  0.742\n\n\nFor trade: the terms that we are getting indicate that Smith is using structural and institutional language in conjunction with this term. We have verbs like carrying, carried, and shipping (possibly used as a noun…), as well as structural qualifiers such as branches, returns, and direct. Some words are perhaps more surprising and would require more investigation (such as round).\nFor labor: there is a strong sign the model has learned labor as an activity embedded in the language of key economic activities, such as productive, distributed, enables, and, more obviously, but very reassuring, laborer and tools.\n\nAs an exercise for the students: what do you think of the neighbors for price, capital, and markets? Any surprises?\n\nI think that at this point, it will be helpful to compare with the co-occurrence analysis we did last week. Let’s compute simple co-occurrence neighbors using the same as in week 6 (window = 5) and compare.\n\n# Co-occurrence neighbors (window = 5)\n\nfrom collections import Counter, defaultdict\n\nWINDOW = 5\ncooc = defaultdict(Counter)\n\nfor seg in segments:\n    for i, token in enumerate(seg):\n        start = max(0, i - WINDOW)\n        end = min(len(seg), i + WINDOW + 1)\n        for j in range(start, end):\n            if i != j and seg[j] != token: # otherwise \"trade\" will be the closest to \"trade\" etc.\n                cooc[token][seg[j]] += 1\n\ndef show_cooc_neighbors(word, topn=10):\n    print(f\"\\nCo-occurrence neighbors for '{word}':\")\n    for w, c in cooc[word].most_common(topn):\n        print(f\"{w:&gt;12}  {c}\")\n\nNow we can look at them and compare with Word2Vec neighbors defined by cosine similarity:\n\nfor target in [\"trade\", \"labor\", \"price\"]:\n    show_cooc_neighbors(target, topn=10)\n\n\nCo-occurrence neighbors for 'trade':\n     foreign  192\n       great  164\n     capital  127\n     country  119\n consumption  92\n    employed  82\n     greater  80\n     carried  70\n    monopoly  68\n    branches  67\n\nCo-occurrence neighbors for 'labor':\n     produce  227\n    quantity  216\n       wages  202\n       price  193\n        land  161\n     greater  139\n       stock  122\n       value  122\n  productive  120\n     country  119\n\nCo-occurrence neighbors for 'price':\n      market  200\n       labor  193\n       money  158\n        corn  142\n        real  116\n        rise  106\n commodities  104\n        high  101\n    quantity  94\n     produce  90\n\n\n\nWhat differences do you note in these lists? Both represent “neighbors” of our target words, but obtain through two different methods.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 07: Word2Vec and LDA introduction"
    ]
  },
  {
    "objectID": "week-07-word2vec.html#step-0-transition-to-python",
    "href": "week-07-word2vec.html#step-0-transition-to-python",
    "title": "Week 07: Word2Vec and LDA introduction",
    "section": "",
    "text": "Before working with text in today’s tutorial, we are going to have an introduction to getting started in Python. This is for those of you who haven’t worked in Python before. Note: this doesn’t mean that you want to forget everything that you learned in R up to now! The analyses that we conducted in R allowed us to develop a granular understanding of how to represent and measure textual features. The tasks we will do in Python will be less “transparent”, but you don’t want to give up the intuition you developed so far.\nIf you have never worked with Python, you can find a beginner friendly, step by step guide on how to download Python and set up VS Code (the environment you will use for Python–think of it as the Python version of R studio): here.\nIf you have worked with Python before, you can use whichever environment you want.\nQuick note: you want Python 3.11.x. The bottleneck is scikit-learn, which doesn’t play well with the most recent version of Python.\nA second note about learning Python: If you are new to Python, the first thing that you will see as you look at tutorials is that Python is an object-oriented language. I have added a link that takes you to realpython.com, which is a great resource for learning Python. But don’t worry too much about understanding all the finer points of this from the get-go! The TA’s and I will help you use the code that I give you as an example and then you can start learning more about objects and classes and all that jazz.\n\nAs we go along this tutorial: I am re-introducing the training wheels. I am including links to some of the major Python concepts as they come up in the code!\n\n\n\nThe first few steps that we are going to take are very similar to what we did in R: the logic is the same, but the syntax is different. The first thing that we need to do is to read the text and then check that things look correct!\nIn R, what we do at the beginning is:\n\nload a text file into memory,\ncheck how long it is,\npreview the beginning,\nand try to spot obvious encoding or OCR problems early (that is, does the text look weird?).\n\nWe are now going to do the same step in Python. The main difference is that we are going to use a path-handling tool in Python, Path. We are then going to actually read the file with this line: text = path.read_text(encoding=\"utf-8\", errors=\"replace\"). Here, I am asking Python to decode the text using utf-8 and to replace any unreadable characters instead of crashing. The replacement is the Unicode replacement character: �.\n\nfrom pathlib import Path\n\npath = Path(\"texts/wealth.txt\")\ntext = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n# The lines above read the wealth.txt into a long string\n\nprint(\"Characters:\", len(text)) #character count \n\nCharacters: 2411473\n\nprint(\"Lines:\", text.count(\"\\n\") + 1) #line count\n\nLines: 34544\n\n# taking a look that the text looks ok. Similar to head() and slice() in R\nprint(\"\\n--- START ---\\n\")\n\n\n--- START ---\n\nprint(text[:800])\n\nAn Inquiry into the Nature and Causes of the Wealth of Nations\n\nby Adam Smith\n\n\n\n\nContents\n\n\n INTRODUCTION AND PLAN OF THE WORK.\n\n BOOK I. OF THE CAUSES OF IMPROVEMENT IN THE PRODUCTIVE\nPOWERS OF labor, AND OF THE ORDER ACCORDING TO WHICH ITS PRODUCE IS NATURALLY\nDISTRIBUTED AMONG THE DIFFERENT RANKS OF THE PEOPLE.\n CHAPTER I. OF THE DIVISION OF labor.\n CHAPTER II. OF THE PRINCIPLE WHICH GIVES OCCASION TO THE\nDIVISION OF labor.\n CHAPTER III. THAT THE DIVISION OF labor IS LIMITED BY\nTHE EXTENT OF THE MARKET.\n CHAPTER IV. OF THE ORIGIN AND USE OF MONEY.\n CHAPTER V. OF THE REAL AND NOMINAL PRICE OF\nCOMMODITIES, OR OF THEIR PRICE IN labor, AND THEIR PRICE IN MONEY.\n CHAPTER VI. OF THE COMPONENT PART OF THE PRICE OF COMMODITIES.\n CHAPTER VII. OF THE NATURAL AND MARKET PRICE OF COMMODITIES.\n CHA\n\nmid = len(text)//2\nprint(\"\\n--- MIDDLE SLICE ---\\n\")\n\n\n--- MIDDLE SLICE ---\n\nprint(text[mid:mid+800])\n\n Though there are in Europe indeed, a few towns which, in same respects,\n      deserve the name of free ports, there is no country which does so.\n      Holland, perhaps, approaches the nearest to this character of any, though\n      still very remote from it; and Holland, it is acknowledged, not only\n      derives its whole wealth, but a great part of its necessary subsistence,\n      from foreign trade.\n\n      There is another balance, indeed, which has already been explained, very\n      different from the balance of trade, and which, according as it happens to\n      be either favorable or unfavorable, necessarily occasions the prosperity\n      or decay of every nation. This is the balance of the annual produce and\n      consumption. If the exchangeable value of the annual produce, it has\n \n\n\nThe print() function is something that will come up over and over again. If you get lost by how I am using it, do click on this explanation.\nThe next step is to tokenize the text. This is the Python version of unnest_tokens() in tidytext. But in Python, we have to do this using a regex: so, hopefully, you feel comfortable with the regex unit from Week 2. We are also going to lowercase. This tokenizer keeps alphabetic words (and contractions), so it ignores numbers and hyphenated compounds; that’s a simplification for today, not a general rule.\nSince we also want to run some checks on our process (as we did in R), we are going to inspect the most frequent tokens using Counter, the Python parallel to tidytext’s count(word, sort =TRUE).\nMaking things look nicer: You can skip this explanation, if you are new to Python. This is equivalent to: print(w, c). I am just trying to make the display more readable for the class!\nIn the next block below, I am going to use an f-string inside print(). An f-string is a way to format text, so f\"{w:&gt;12} {c}\" means (working inside out):\n\ntake the value of w and convert it to text, right-align it inside a space 12 character wide;\nthen when you print separate the columns (the two spaces between {w:&gt;12} and {c})\n\n\nimport re\nfrom collections import Counter\n\ntokens = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text.lower())\nprint(\"Tokens:\", len(tokens))\n\nTokens: 380080\n\nprint(\"Unique tokens:\", len(set(tokens)))\n\nUnique tokens: 9475\n\ncounts = Counter(tokens)\nprint(\"\\nTop 25 tokens:\")\n\n\nTop 25 tokens:\n\nfor w, c in counts.most_common(25):\n    print(f\"{w:&gt;12}  {c}\")\n\n         the  32244\n          of  24295\n          to  11708\n         and  10284\n          in  9637\n           a  6678\n          it  5392\n       which  4824\n          is  4685\n          be  3828\n        that  3818\n          or  3211\n          as  3088\n          by  2981\n         for  2976\n       their  2523\n        this  2231\n         not  2231\n         are  2168\n        they  2131\n        have  2119\n        upon  2112\n        from  1970\n         but  1962\n       those  1925\n\n\nThe library Scikit-learn is going to allow us to import a standard list of stopwords. More importantly, Scikit-learn is a machine learning library for Python and we will use it more than once!\nWe are going to convert the list of pre-defined stopwords into a set because it will make filtering tokens more efficient. We can then use the in-place OR operator |= to add all the elements of our custom stopwords to the standard stopwords (the link at “sets” will also explain |=).\nAside: if you go to the tutorial, you might see that there is another OR operator | . This operator creates a new set. I am just modifying the preexisting one by adding the custom stopwords.\n\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nstopwords = set(ENGLISH_STOP_WORDS) #turn it into a set\n\n# This is optional and I am only using this as an example of syntax (hence the choice of silly terms)\ncustom_stopwords = {\"barnacle\", \"putine\"}\n\nstopwords |= custom_stopwords\n\n# let's check how many stopwords we have and what they look like\nprint(\"Stopwords loaded:\", len(stopwords))\n\nStopwords loaded: 320\n\nprint(\"Sample:\", sorted(list(stopwords))[:25])\n\nSample: ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any']\n\n\nNow that we have our stopwords as we want them, we can apply it to our tokens. We are also going to remove really short tokens (fewer than 3 characters). Something that sometimes throws people off: I am stating the length requirement as a strict inequality. Now look at the code below: do you understand why have len(t) &gt;= 3 ?\n\nfrom collections import Counter\n\nclean_tokens = [t for t in tokens if t not in stopwords and len(t) &gt;= 3]\n\nprint(\"Tokens (raw):\", len(tokens))\n\nTokens (raw): 380080\n\nprint(\"Tokens (clean):\", len(clean_tokens))\n\nTokens (clean): 148917\n\nprint(\"Unique tokens (clean):\", len(set(clean_tokens)))\n\nUnique tokens (clean): 9156\n\nclean_counts = Counter(clean_tokens)\n\nprint(\"\\nTop 25 tokens after cleaning:\")\n\n\nTop 25 tokens after cleaning:\n\nfor w, c in clean_counts.most_common(25):\n    print(f\"{w:&gt;12}  {c}\")\n\n       great  1583\n       price  1264\n     country  1240\n     greater  1085\n       labor  1011\n       trade  970\n     produce  945\n   different  855\n    quantity  797\n       value  794\n      people  779\n       money  773\n        land  720\n     revenue  691\n      silver  661\n     capital  657\n        time  636\n       stock  601\n       goods  585\n      market  582\n   countries  577\n     expense  561\n  particular  513\n         tax  513\n        gold  508\n\n\n\n\n\nSo far, the steps should be very familiar (with a different coding language). The next step is driven by the model itself. Since LDA does not operate on individual words or on a continuous token stream, we are going to be constrained by the structure that it needs. Word2Vec can work on the entire Wealth of Nations at once, but for the sake of continuity, we will stick to the same structure for both models.\nLDA needs to work with documents or, as in our case, pre-defined segments of a longer text. These are the assumptions behind LDA:\n\neach document is a mixture of topics,\neach topic is a distribution over words.\n\nThis forces us to decide what counts as a “document” in our corpus. In some cases, this will be obvious to you: if you have a collection of political speeches, then each speech is a document. In other cases, we have to make a decision. The Wealth of Nations is one long text and we can segment it in a number of ways. There are two common options (each with its one trade-offs): one is to use internal subdivisions, such as chapters; the other is to use fixed-sized segments. We are going to go for the second one and create 800-token chunks.\nNote: both options are legitimate choices. I am picking fixed-sized segments because I know that the chapters in The Wealth of Nations are of varied length and that a single topic (such as labor or political economy) is split over several chapters. The approach I take here also ensures that no single segment dominates the model simply because of its length. LDA tends to work better when documents are not extremely long. But this choice comes with a trade-off: I am losing track of Adam Smith’s structure. For different purposes (that is, for an actual project rather than a class tutorial), I might choose to “chunk” by chapter instead.\n\n# Segmenting\n\nSEGMENT_SIZE = 800  # fixed-size \"document.\" We can always adjust length\n# slice the entire list of tokens from token #0 to the last token, breaking it into chunks of SEGMENT_SIZE.\nsegments = [\n    clean_tokens[i:i+SEGMENT_SIZE] \n    for i in range(0, len(clean_tokens), SEGMENT_SIZE)\n]\n\n#inspect the segmentation\nprint(\"Number of segments:\", len(segments))\n\nNumber of segments: 187\n\nprint(\"First segment length:\", len(segments[0]))\n\nFirst segment length: 800\n\nprint(\"Last segment length:\", len(segments[-1]))\n\nLast segment length: 117\n\n\nEach segment is now a document for LDA: we have 186 segments, which is a good document count for LDA, and segment sizes are consistent (800; last shorter, which is what we expect). Note: I picked 800 somewhat arbitrarily, but it is meant to capture approximately 2-3 paragraphs per segment given what I know of Smith’s writing. Let’s see what word counts in a segment look like:\n\nfrom collections import Counter\n\nsegment_counts = [Counter(seg) for seg in segments]\n\n# check: top words in segment 0\nprint(\"Top words in segment 0:\")\n\nTop words in segment 0:\n\nfor w, c in segment_counts[0].most_common(15):\n    print(f\"{w:&gt;12}  {c}\")\n\n     chapter  33\n       labor  30\n   different  19\n        book  12\n     produce  11\n    division  11\n    employed  11\n     nations  10\n     society  10\n      number  10\n        work  9\n       stock  8\n  particular  8\n      people  7\n       great  7\n\n\nThis also looks reasonable for the opening of the book (chapter, book, division, labor, work, nations, society) and it means that we can proceed to the next step.\nAt this point, we now have two complementary representations of the same text:\n\nsegments: each segment is an ordered sequence of tokens\nsegment_counts: each segment is represented by word counts (bag-of-words)\n\nWe are going to need both representations, but for two different models. Word2Vec learns meaning from local context and word order, so it can work directly with segments. On the other hand, LDA ignores word order and models documents as mixtures of topics, so it requires a bag-of-words representation, which we will build explicitly in the next section.\nWe will begin with Word2Vec, because it extends ideas you have already encountered: a word’s meaning is shaped by the contexts in which it appears. For a good resource on understanding Word2Vec, see here. But, in short, instead of storing explicit co-occurrence counts, Word2Vec learns dense vectors for each word. These vectors are learned by optimizing a predictive task: given a word, predict its surrounding context (or vice versa). The result is a compressed, smoothed representation of the same information we previously counted directly.\nOK, what does this mean in practice for us? Each segment is a sequence of tokens and Word2Vec slides a fixed-size context window across these sequences and learns vector representations that are good at predicting nearby words.\n\n# Key step: Word2Vec training\n\nfrom gensim.models import Word2Vec  \n\nw2v = Word2Vec(\n    sentences=segments,     # each segment is a list of tokens\n    vector_size=100,        # dimensionality\n    window=5,               # context window (parallel to your co-occurrence window)\n    min_count=10,           # ignore very rare words\n    workers=4,              # number of CPU cores for parallel processing\n    sg=1                    # 1=skip-gram, 0=CBOW\n)\n\nprint(\"Vocabulary size:\", len(w2v.wv.key_to_index))\n\nVocabulary size: 2317\n\n\nWe have to make some choices when we train Word2Vec:\n\nvector_size=100: each word is represented as a 100-dimensional vector (if we choose higher dimension, we capture more information about each word, but it’s slower and requires more text data). Common values in practice: 100-300. 100 is often the default.\nwindow=5 matches what we did in our co-occurrence calculation.\nmin_count=10 keeps the vocab manageable (and its good for a class exercise). Note: the exact choice of 10 (instead of, say, 15 or 20) is based on trail and error. Given that we have 1,395 words in the vocabulary after filtering, I don’t want to be too agressive. For a larger and more varied corpus, I might go for 20.\nImportant: if you have an older machine and it’s freezing; change workers = 4 to 2 or even 1.\nsg=1 (skip-gram) is usually better for capturing rarer, more specific relations.\n\nNow that the model is trained, we can inspect its learned semantic space by asking for a word’s nearest neighbors. The goal is to use cosine similarity to find which word vectors are closest to a given/target word in the vector space. In this case, let’s look at “trade,” “labor,” “price,” “capital,” and “market” to see what kind of results we get.\n\nquick aside if you haven’t worked in Python before: this is the first time we are defining a function using the def keyword.\n\n\n# Nearest neighbors (cosine similarity)\n\ndef show_neighbors(word, topn=10):\n    if word not in w2v.wv:\n        print(f\"'{word}' not in vocabulary.\")\n        return\n    print(f\"\\nNearest neighbors for '{word}':\")\n    for w, sim in w2v.wv.most_similar(word, topn=topn):\n        print(f\"{w:&gt;12}  {sim:.3f}\")\n\nfor target in [\"trade\", \"labor\", \"price\", \"capital\", \"market\"]:\n    show_neighbors(target, topn=10)\n\n\nNearest neighbors for 'trade':\n    carrying  0.890\n       round  0.840\n     carried  0.828\n    branches  0.822\n       carry  0.817\nadvantageous  0.814\n    shipping  0.812\n      branch  0.803\n      forced  0.798\n     returns  0.794\n\nNearest neighbors for 'labor':\n        adds  0.873\n      motion  0.864\n        puts  0.863\n     laborer  0.862\n distributed  0.852\n     enables  0.834\n        vary  0.830\n constitutes  0.829\n  recompence  0.829\n   increases  0.823\n\nNearest neighbors for 'price':\n       rises  0.855\n        sink  0.821\n         low  0.814\n      barley  0.812\n     butcher  0.810\n        rise  0.804\n        meat  0.803\n       lower  0.798\n       bread  0.798\n       risen  0.794\n\nNearest neighbors for 'capital':\n    capitals  0.889\n       stock  0.850\n    replaces  0.847\n    employed  0.837\n       fixed  0.832\n   withdrawn  0.826\n     replace  0.826\n    reserved  0.822\n     employs  0.819\n      unless  0.817\n\nNearest neighbors for 'market':\n    supplied  0.810\n   effectual  0.810\n     thither  0.783\n     brought  0.770\n        come  0.767\n   plentiful  0.756\n    bringing  0.752\n      supply  0.744\n     cheaper  0.742\n        send  0.742\n\n\nFor trade: the terms that we are getting indicate that Smith is using structural and institutional language in conjunction with this term. We have verbs like carrying, carried, and shipping (possibly used as a noun…), as well as structural qualifiers such as branches, returns, and direct. Some words are perhaps more surprising and would require more investigation (such as round).\nFor labor: there is a strong sign the model has learned labor as an activity embedded in the language of key economic activities, such as productive, distributed, enables, and, more obviously, but very reassuring, laborer and tools.\n\nAs an exercise for the students: what do you think of the neighbors for price, capital, and markets? Any surprises?\n\nI think that at this point, it will be helpful to compare with the co-occurrence analysis we did last week. Let’s compute simple co-occurrence neighbors using the same as in week 6 (window = 5) and compare.\n\n# Co-occurrence neighbors (window = 5)\n\nfrom collections import Counter, defaultdict\n\nWINDOW = 5\ncooc = defaultdict(Counter)\n\nfor seg in segments:\n    for i, token in enumerate(seg):\n        start = max(0, i - WINDOW)\n        end = min(len(seg), i + WINDOW + 1)\n        for j in range(start, end):\n            if i != j and seg[j] != token: # otherwise \"trade\" will be the closest to \"trade\" etc.\n                cooc[token][seg[j]] += 1\n\ndef show_cooc_neighbors(word, topn=10):\n    print(f\"\\nCo-occurrence neighbors for '{word}':\")\n    for w, c in cooc[word].most_common(topn):\n        print(f\"{w:&gt;12}  {c}\")\n\nNow we can look at them and compare with Word2Vec neighbors defined by cosine similarity:\n\nfor target in [\"trade\", \"labor\", \"price\"]:\n    show_cooc_neighbors(target, topn=10)\n\n\nCo-occurrence neighbors for 'trade':\n     foreign  192\n       great  164\n     capital  127\n     country  119\n consumption  92\n    employed  82\n     greater  80\n     carried  70\n    monopoly  68\n    branches  67\n\nCo-occurrence neighbors for 'labor':\n     produce  227\n    quantity  216\n       wages  202\n       price  193\n        land  161\n     greater  139\n       stock  122\n       value  122\n  productive  120\n     country  119\n\nCo-occurrence neighbors for 'price':\n      market  200\n       labor  193\n       money  158\n        corn  142\n        real  116\n        rise  106\n commodities  104\n        high  101\n    quantity  94\n     produce  90\n\n\n\nWhat differences do you note in these lists? Both represent “neighbors” of our target words, but obtain through two different methods.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 07: Word2Vec and LDA introduction"
    ]
  },
  {
    "objectID": "week-07-word2vec.html#lda-topic-modeling",
    "href": "week-07-word2vec.html#lda-topic-modeling",
    "title": "Week 07: Word2Vec and LDA introduction",
    "section": "LDA Topic Modeling:",
    "text": "LDA Topic Modeling:\nUp to this point, we have used Word2Vec to explore how individual words relate to one another based on shared contexts. This approach is well suited to questions like:\n\nWhich words behave similarly across the text? How is a concept used, qualified, or framed?\n\nThis is really helpful if you are trying to think about how, for example, different texts portray the same terminology and concepts. But what if we want to focus on the text as a whole instead? This is the question that topic models—and LDA in particular—are designed to answer.\n\nSome background on LDA:\nIn class, we briefly discussed the Dirichlet distribution. I emphasized that you don’t need to know the details in the background in order to actually work with LDA topic modeling. However, you do need to understand some basics about distributions so as to develop some intuition. First, if you need a clear and to the point reminder of binomial distributions, look at Josh Starmer’s youtube channel [he is a faculty member at UNC; not a random channel]. If you want a more robust discussion, see Peter Dalgaard, Introductory Statistics with R (2008), chapter 3 [statistics applications is one of the reasons I started the semester with R]. Once you are comfortable with that, use the discussion in chapter 6 of Text as Data for Dirichlet distribution (or take it on faith for now!).\nWith that behind us, the next step that we are going to do is the parallel to creating a DFM, but we have to organize our text into the data structure required by LDA. We already have segments and segment_counts, we need a dictionary (this is a key data structure in Python, follow the link if you are unfamiliar with it) and a document-term matrix (DTM). LDA works with a “bag-of-words” representation, so we need to get word counts for each segment. This should be very familiar for our work in R.\nIn the code below, Dictionary builds a vocabulary from our corpus and creates a mapping between each unique word (like “trade”) and a unique integer ID (assigned sequentially, in the order that new words are first encountered when gensim scans the corpus).\n\nfrom gensim.corpora import Dictionary\n\n# Build a dictionary from the segments\ndictionary = Dictionary(segments)\n\n# do some cleaning\ndictionary.filter_extremes(\n    no_below=10,    # must appear in at least 10 segments\n    no_above=0.5   # must appear in no more than 50% of segments\n)\n\nprint(\"Vocabulary size after filtering:\", len(dictionary))\n\nVocabulary size after filtering: 1711\n\n# Convert segments to bag-of-words format\ncorpus = [dictionary.doc2bow(seg) for seg in segments]\n\n#Take a look\nprint(\"First document (bow format):\")\n\nFirst document (bow format):\n\nprint(corpus[0][:10])\n\n[(0, 1), (1, 3), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]\n\n\nIn the filtering step we are mirroring what we did in R when we removed very rare words and stopwords, but with a slightly different logic. Remember, we are trying to create topics and rare words in the corpus won’t really define a what the text is “about,” so we filter out words that appear in fewer than 10 segments by using no_below=10 (out of 186 segments). Conversely, really frequent words tend to be too general to distinguish topics, so we filter for words that appear in more than half of the segments using no_above=0.5. Note: both these values are heuristics that tend to work well for a first run of the model. But you might decide that, based on your question, you want to try different filters!\nAt this point, we have the same kind of data structure we relied on in R for bag-of-words models: documents represented by word counts. The next step will simply convert these counts into the specific input format required by the LDA implementation we are using.\nWe are finally done with preprocessing and we can actually fit the LDA model (yay!). To reiterate: these “topics” are computational constructs; they are estimates of latent word distributions that, taken together, explain the observed word counts reasonably well.\n\n# Train LDA model\n\nfrom gensim.models import LdaModel\n\nlda = LdaModel(\n    corpus=corpus,\n    id2word=dictionary,\n    num_topics=8,\n    random_state=42,\n    passes=10,\n    alpha=\"auto\",\n    eta=\"auto\"\n)\n\nprint(\"LDA model trained.\")\n\nLDA model trained.\n\n\n\nWhy these hyperparameter settings:\n\nnum_topics=8: small enough to read–it’s a good starting place\npasses=10: this is how many times the model goes through the corpus during training. More passes will produce more stable topic estimates, but they take longer to compute. The size of your corpus matters here: with a small corpus, each document will carry a lot of weight and topic estimates are more sensitive to noise, so I will tend to do 10-15 passes. With a larger corpus, you can get away with fewer passes (5-10-ish) because the model encounters each word in more contexts. I don’t have a a clear cut rule because the answer also depends on: “how much time and GPU to you have?”\n\nA nuanced point: the model also has a default number of iterations that we are not changing. Iterations are different from passes. The iterations control how thoroughly the model updates its estimates during each visit: these are the internal optimization steps that occur during each pass.\n\n\nWhen we train an LDA model, we are not only choosing how many topics to use. We are also choosing “how flexible” the model is in assigning topics to documents and words to topics\n\nalpha=\"auto\": the alpha setting governs the document-topic distribution. That is, does the model assume that documents (segments in our case) tend to focus on one or fewer topics (low alpha) or does it assume that they contain a mix of many topics (higher alpha)? When we set it to \"auto\", we allow the model to learn this behavior from the data. I chose \"auto\" because we created arbitrary segments in the text based on a pre-selected number of tokens. If I had a different corpus, I would have to think more about alpha.\n\nWhat does this mean? In gensim‘s LDA implementation, the default setting for the alpha parameter is ’symmetric’. This means that alpha has the same value for all topics. When alpha is symmetric, gensim calculates it using this formula: \\(\\alpha\\) = 1.0 / number of topics. When I say that based on the structure of the corpus, I would have to think about it more, I mean this quite literally: I will base my decisions on the exact question that I am asking with topic modelling and the details of the corpus.\nIf you really want to dig into this, here are the resources that I would review before making these decisions: “Finding scientific topics” (2004); “Optimising Semantic Coherence in Topic Models” (2011); and for good measure, I am including the original paper on LDA, though it wouldn’t be my first stop for practical decisions, “Latent Dirichlet Allocation” (2003).\n\neta=\"auto\": you can think of this setting as, how “chatty” do we want our topics to be? A low eta encourages topics to focus on fewer words, while a high eta encourages more words per topic. I usually select auto as it allows the model to infer how sharp (fewer words) topics should be based on the actual distribution of words in the corpus. In technical literature, you will find this parameter denoted by \\(\\beta\\).\nrandom_state: this will probably seem the most opaque setting. It fixes the sequence of random choices the model makes during initialization. This is a step for reproducibility and transparency. The goal is to be able to obtain the same output from run to run, but [flashing warning light, sirens blaring, warning, warning] only in the case that for each run you have the same corpus, the same preprocessing, the all same hyperparamaters settings, the same software versions (!), and the same hardware behavior. We do what we can to be as transparent as possible. Sometimes, life has other plans.\n\nFor more information on all the hyperparameters in gensim’s LDA follow the link. If you really want to dig further into this topic, I strongly recommend this paper.\nAt the end of the day: LDA results depend on modeling choices and random initialization. Different runs can produce different, but equally reasonable, topic structures.\nOK, we can finally enjoy the fruits of our labor. Let’s inspect the topics:\n\nfor topic_id in range(lda.num_topics):\n    print(f\"\\nTopic {topic_id}:\")\n    for word, weight in lda.show_topic(topic_id, topn=12):\n        print(f\"{word:&gt;12}  {weight:.3f}\")\n\n\nTopic 0:\n        corn  0.020\n       wages  0.016\n        rent  0.010\n      demand  0.008\n     profits  0.007\n        work  0.007\n        rate  0.006\n        rise  0.006\n improvement  0.006\n        high  0.006\n      cattle  0.005\n     average  0.005\n\nTopic 1:\n        bank  0.043\n       paper  0.019\n        cent  0.017\n        gold  0.015\n      silver  0.013\n circulation  0.012\n         sum  0.012\n       bills  0.010\n       notes  0.009\n      credit  0.009\n      pounds  0.008\n       banks  0.008\n\nTopic 2:\n     company  0.010\n  government  0.009\n    colonies  0.008\n established  0.006\n   sovereign  0.006\n    commerce  0.006\n   authority  0.006\n      clergy  0.005\n         law  0.005\n      church  0.005\n      empire  0.005\n     society  0.005\n\nTopic 3:\n      silver  0.043\n        gold  0.034\n        coin  0.014\n      metals  0.012\n       mines  0.011\n     foreign  0.009\n    exchange  0.009\n commodities  0.008\n        east  0.008\n    purchase  0.007\n      annual  0.007\n    standard  0.006\n\nTopic 4:\n        debt  0.017\n         war  0.013\n     britain  0.011\n         new  0.009\n  government  0.009\n        fund  0.008\n       taxes  0.008\n       debts  0.008\n    millions  0.007\n     chapter  0.007\n     private  0.006\n       peace  0.006\n\nTopic 5:\n         tax  0.034\n        rent  0.026\n       taxes  0.015\n      annual  0.008\n  productive  0.007\n    landlord  0.007\n     society  0.007\n      houses  0.006\n consumption  0.006\n        fall  0.006\n     profits  0.006\n       lands  0.006\n\nTopic 6:\n     ancient  0.011\n   education  0.011\n         men  0.009\n     society  0.009\n        life  0.007\n        army  0.006\n   authority  0.006\n         war  0.006\n    standing  0.006\n        body  0.006\n   exercises  0.006\n    military  0.006\n\nTopic 7:\n     foreign  0.019\n      duties  0.014\n     britain  0.013\n        home  0.013\n exportation  0.012\n importation  0.010\n consumption  0.009\n    colonies  0.009\n      bounty  0.009\n    monopoly  0.007\n        duty  0.007\nmanufactures  0.007\n\n\n\n# After training, inspect learned alpha values\nprint(\"Learned alpha values:\", lda.alpha)\n\nLearned alpha values: [0.08285706 0.04526103 0.07984485 0.06372422 0.04359768 0.05820734\n 0.04462427 0.07341722]\n\n\nAnd let’s visualize them with a bar plot that gives us a representation of the topic prevalence across segments:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nK = lda.num_topics\ntopic_mass = np.zeros(K)\n\n# Sum topic probabilities over all segments\nfor bow in corpus:\n    doc_topics = lda.get_document_topics(bow, minimum_probability=0)\n    for k, p in doc_topics:\n        topic_mass[k] += p\n\n# Normalize to proportions (so bars sum to 1)\ntopic_share = topic_mass / topic_mass.sum()\n\n# Plot\nplt.figure()\nplt.bar(range(K), topic_share)\nplt.xticks(range(K), [f\"T{k}\" for k in range(K)])\n\n([&lt;matplotlib.axis.XTick object at 0x0000015265B69710&gt;, &lt;matplotlib.axis.XTick object at 0x0000015265B77B90&gt;, &lt;matplotlib.axis.XTick object at 0x0000015265BC4090&gt;, &lt;matplotlib.axis.XTick object at 0x0000015265BC6790&gt;, &lt;matplotlib.axis.XTick object at 0x0000015265BC4E10&gt;, &lt;matplotlib.axis.XTick object at 0x0000015265BD2210&gt;, &lt;matplotlib.axis.XTick object at 0x0000015265BBC6D0&gt;, &lt;matplotlib.axis.XTick object at 0x0000015265BBEB50&gt;], [Text(0, 0, 'T0'), Text(1, 0, 'T1'), Text(2, 0, 'T2'), Text(3, 0, 'T3'), Text(4, 0, 'T4'), Text(5, 0, 'T5'), Text(6, 0, 'T6'), Text(7, 0, 'T7')])\n\nplt.ylabel(\"Share of topic mass (across segments)\")\nplt.xlabel(\"Topic\")\nplt.title(\"LDA topic prevalence in Wealth of Nations (by segment)\")\nplt.show()\n\n\n\n\n\n\n\n# Print the numeric values too (useful for interpretation)\nfor k, s in enumerate(topic_share):\n    print(f\"Topic {k}: {s:.3f}\")\n\nTopic 0: 0.204\nTopic 1: 0.062\nTopic 2: 0.189\nTopic 3: 0.119\nTopic 4: 0.058\nTopic 5: 0.124\nTopic 6: 0.071\nTopic 7: 0.173\n\n\nEach bar shows the average proportion of that topic across all your 186 segments—i.e., which themes are most prevalent in the book under your segmentation choice.\nOne more visualization: topic prevalence by segment. This makes the structure of The Wealth of Nations visible.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Get per-segment topic distributions\ndoc_topic_matrix = np.array([\n    [p for _, p in lda.get_document_topics(bow, minimum_probability=0)]\n    for bow in corpus\n])\n\nplt.figure()\n\n# Plot only the top 4 topics by overall prevalence (less clutter)\ntop_topics = np.argsort(topic_share)[-4:]\n\nfor k in top_topics:\n    plt.plot(doc_topic_matrix[:, k], label=f\"Topic {k}\")\n\nplt.xlabel(\"Segment index (approx. book progression)\")\nplt.ylabel(\"Topic proportion\")\nplt.title(\"Topic prevalence across Wealth of Nations\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFinal considerations and bringing things together:\nThe eight topics our model identified capture distinct but overlapping themes in The Wealth of Nations. Topic 2 (foreign trade and manufactures) and Topic 6 (precious metals and banking) emerge as the most prevalent, together accounting for over a third of the book’s content. This makes sense given Smith’s focus on international commerce and monetary systems. Topic 1 clearly represents taxation and fiscal policy (tax, taxes, duties, Britain), while Topic 3 captures discussions of war, military spending, and colonial companies—particularly the East India Company. Topics 0 and 4 both relate to labor and wages, but Topic 0 emphasizes the relationship between landlords, rent, and maintenance, while Topic 4 focuses more directly on wage rates, employment, and labor markets. Topic 5 centers on agricultural commodities and their prices over time (corn, wheat, cattle, century, improvement), likely reflecting Smith’s extended discussions of agricultural economics and historical price movements. The distribution of topic prevalence tells us something important (and reassuring given what is known about Smith’s work): Smith doesn’t devote equal attention to all themes—international trade and monetary systems clearly dominate his analysis.\nAs we look at the visualization, we can perhaps get a better sense of some of the structural distribution of these topics. The topic prevalence plot across segments reveals the book’s structure and confirms that LDA has captured meaningful thematic shifts as Smith moves through his argument. Notice how Topic 2 (foreign trade) shows strong presence in the middle and later portions of the book, which aligns with Smith’s extended treatment of commercial policy in Books IV and V. Topic 6 (metals and banking) appears in concentrated bursts rather than being evenly distributed, suggesting Smith returns to monetary questions at specific points in his argument. Topic 3 (war and colonies) is relatively rare overall but spikes sharply in specific segments—these likely correspond to his discussions of colonial administration and military expenditure. The uneven distribution of topics across segments is exactly what we should expect from a structured argument: Smith isn’t randomly mixing concepts and arguments, he’s developing them systematically. If all topics appeared uniformly across all segments, it would suggest either that our segmentation strategy failed to capture the book’s organization, or that LDA wasn’t identifying meaningful thematic distinctions. Instead, the clear peaks and valleys should give us some reassurance that both our preprocessing choices and the model’s topic assignments are capturing real patterns in how Smith structured his economic treatise",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 07: Word2Vec and LDA introduction"
    ]
  }
]