[
  {
    "objectID": "week-01-introduction.html",
    "href": "week-01-introduction.html",
    "title": "Week 01: Introduction",
    "section": "",
    "text": "Welcome to IDS 570, “Text as Data.” For each week of the course, you will find code and notes to match the in-class lecture. For the first few weeks of the semester, we will be working in R. As the term goes on, we will introduce Python concepts and methodologies.\nThis repository is a work in progress: I will update it (and the pull-down menu will grow) as the semester progresses and as I add or subtract materials for the course.\n\n\nThis site is generated from Quarto source files and updated regularly–expect things to change and if you notice a typo, please let me or the TA’s know. Code examples are illustrative and meant to be adapted.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 01: Introduction"
    ]
  },
  {
    "objectID": "week-01-introduction.html#welcome",
    "href": "week-01-introduction.html#welcome",
    "title": "Week 01: Introduction",
    "section": "",
    "text": "Welcome to IDS 570, “Text as Data.” For each week of the course, you will find code and notes to match the in-class lecture. For the first few weeks of the semester, we will be working in R. As the term goes on, we will introduce Python concepts and methodologies.\nThis repository is a work in progress: I will update it (and the pull-down menu will grow) as the semester progresses and as I add or subtract materials for the course.\n\n\nThis site is generated from Quarto source files and updated regularly–expect things to change and if you notice a typo, please let me or the TA’s know. Code examples are illustrative and meant to be adapted.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 01: Introduction"
    ]
  },
  {
    "objectID": "week-01-introduction.html#instruction-for-this-week",
    "href": "week-01-introduction.html#instruction-for-this-week",
    "title": "Week 01: Introduction",
    "section": "Instruction for this week:",
    "text": "Instruction for this week:\nFor each week of the course, you will find code and notes to match the in-class lecture. For this week: you need to become familiar with RStudio and learn how to organize directories and projects. You can find a friendly introduction to R at https://rladiessydney.org/courses/01-basicbasics-0. Complete the three BasicBasics lessons. The goal is to feel comfortable with RStudio, installing packages, and reading data into RStudio.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 01: Introduction"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”",
    "section": "",
    "text": "With the recent explosion in availability of digitized historical and literary texts and the availability of powerful language models, researchers are increasingly turning to computational tools for the analysis of text as data. But not all text is equally amenable to computational approaches. Historical texts often require specialized approaches to bridge the gap between the books as originally produced and analysis-ready data. In this course, students will learn to prepare and analyze historical and literary texts for natural language processing. We will also consider questions of interpretation and the ethics of corpus construction.Our corpora will derive from economic documents and travel narratives from the 17th century. These texts are challenging and require careful curation and cleaning, and they will force you to learn meticulous practices and work-flows in text analysis. Whether working with contemporary or historical texts, most textual data is messy and requires careful preprocessing. By focusing on these Early Modern documents, you will learn how to approach quantitative text analysis with qualitative study of the cultural, economic, and political context that produced the data you are analyzing.",
    "crumbs": [
      "Home",
      "Course",
      "IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”",
    "section": "",
    "text": "With the recent explosion in availability of digitized historical and literary texts and the availability of powerful language models, researchers are increasingly turning to computational tools for the analysis of text as data. But not all text is equally amenable to computational approaches. Historical texts often require specialized approaches to bridge the gap between the books as originally produced and analysis-ready data. In this course, students will learn to prepare and analyze historical and literary texts for natural language processing. We will also consider questions of interpretation and the ethics of corpus construction.Our corpora will derive from economic documents and travel narratives from the 17th century. These texts are challenging and require careful curation and cleaning, and they will force you to learn meticulous practices and work-flows in text analysis. Whether working with contemporary or historical texts, most textual data is messy and requires careful preprocessing. By focusing on these Early Modern documents, you will learn how to approach quantitative text analysis with qualitative study of the cultural, economic, and political context that produced the data you are analyzing.",
    "crumbs": [
      "Home",
      "Course",
      "IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”"
    ]
  },
  {
    "objectID": "lesson-03-regex.html",
    "href": "lesson-03-regex.html",
    "title": "lesson-03-regex",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "lesson-03-regex.html#quarto",
    "href": "lesson-03-regex.html#quarto",
    "title": "lesson-03-regex",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "lesson-03-regex.html#running-code",
    "href": "lesson-03-regex.html#running-code",
    "title": "lesson-03-regex",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed).\n\nheadings: regex to fix stuff\n\n\n\nbullet points: maybe\n\n\n\nlearning goals: more stuff"
  },
  {
    "objectID": "week-02-basics.html#weeks-goals",
    "href": "week-02-basics.html#weeks-goals",
    "title": "Week 02: Basics",
    "section": "Week’s Goals:",
    "text": "Week’s Goals:\nThis week, we compare two early modern economic texts by examining word frequencies and bigrams. Our two texts are by Edward Misselden, one of the major Early Modern mercantilists whose work we will be analyzing. The goal is for you to become familiar with how to start implementing NLP workflows in RStudio. The two texts are available on Canvas, under files in the folder named Week 2.\nNote: this will be the standard set up going forward. Sample code will be here and files will be on Canvas, unless otherwise specified.\n\nOur guiding questions will be: how does Misselden’s language about trade change between 1622 and 1623? This is obviously an artificially simplistic question at this stage, but it will help us explore some useful NLP methods. To begin answering this question, we will compare word frequency between two of his works: Free Trade (London, 1622) and The Circle of Commerce (London, 1623). We will also look at bigram frequencies as a preview of the longer discussion of N-grams next week.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#what-you-need",
    "href": "week-02-basics.html#what-you-need",
    "title": "Week 02: Basics",
    "section": "What you need:",
    "text": "What you need:\nI am assuming that you will be working inside an RStudio Project (recommended). We will read two plain-text files, tidy the tokens/bigrams, and compare frequency patterns. You will need:\n\nA project folder that contains all of your files (try to have a consistent folder organization throughout the semester).\nThe plain-text files on Canvas.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#a-word-of-warning-about-training-wheels",
    "href": "week-02-basics.html#a-word-of-warning-about-training-wheels",
    "title": "Week 02: Basics",
    "section": "A word of warning about training wheels:",
    "text": "A word of warning about training wheels:\nBecause this class doesn’t assume familiarity with R, I am giving extended explanations of the code during this first session (including tips on how to set up your directories and extensive links to R documentation). As the semester proceeds, I will assume that you are gaining confidence and familiarity with R and Rstudio, and the training wheels will slowly come off. Take advantage of the slower pace at the start of the semester to set yourself up for the harder weeks to come!\nThis week I will also show the results of each step of code by printing after each step. This will make (a bit more) explicit how each chunk of code modifies our data. You will be expected to do this more and more on your own starting next week.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#step-by-step-from-raw-texts-to-comparison-plots",
    "href": "week-02-basics.html#step-by-step-from-raw-texts-to-comparison-plots",
    "title": "Week 02: Basics",
    "section": "Step-by-step: from raw texts to comparison plots",
    "text": "Step-by-step: from raw texts to comparison plots",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#setup",
    "href": "week-02-basics.html#setup",
    "title": "Week 02: Basics",
    "section": "Setup",
    "text": "Setup\nAt the beginning of each R file, you will want to call all the packages needed. This will look like:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(tibble)\nlibrary(scales)\n\nI am storing my text files in a directory named texts/ inside my project: I strongly suggest doing the same to make your life easier.\nThe next section of code demonstrates how to read the files and to organize them into a type of dataframe that is well suited for Tidy work: a tibble. For this week, I am using generic “file_a” and “file_b” names to simplify the rather complex original file names. We will discuss the naming conventions of these files as well as how to deal with XML files (the original format of Misselden’s texts) in the next few weeks, but for now, I want you to focus on getting them into the appropriate format for analysis.\n\n# You will need the correct file paths if you don't follow my naming conventions:\nfile_a &lt;- \"texts/A07594__Circle_of_Commerce.txt\"\nfile_b &lt;- \"texts/B14801__Free_Trade.txt\"\n\n# Read the raw text files into R\ntext_a &lt;- read_file(file_a)\ntext_b &lt;- read_file(file_b)\n\n# Combine into a tibble for tidytext workflows\ntexts &lt;- tibble(\n  doc_title = c(\"Text A\", \"Text B\"),\n  text = c(text_a, text_b)\n)\n\ntexts\n\n# A tibble: 2 × 2\n  doc_title text                                                                \n  &lt;chr&gt;     &lt;chr&gt;                                                               \n1 Text A    THE CIRCLE OF COMMERCE. The Prooeme. HERODOTVS in his CLIO, reporte…\n2 Text B    CAP. I. The Causes of the want of Money in England. IT hauing pleas…\n\n\nAs you can see from the .txt files, these are Early Modern texts with (at least some) of the original orthography. In reality, these have already been partially cleaned as I have accessed them from EarlyPrint (we will discuss different sources of texts, historical and otherwise) as you start thinking about your term project.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#tokenization-stopwords-and-counting-words",
    "href": "week-02-basics.html#tokenization-stopwords-and-counting-words",
    "title": "Week 02: Basics",
    "section": "Tokenization, stopwords, and counting words:",
    "text": "Tokenization, stopwords, and counting words:\nWe will turn each text into a table of one word per row, this is the tokenization process. Tokens are the basic unit of text that we are working on. For right now, we are going to consider each word to be a token, but we could break up the text into characters or parts of words if needed. We will then we remove stopwords (words like “the” or “and” that are not useful for this analysis) so that the remaining words are more meaningful for comparison.\nWe will do this in two steps:\n\nStopwords: we will create an “all stopwords” list by combining tidytext’s built-in list with our own, corpus-specific, list. Our list is going to be a one-column tibble to match the format of the built-in stopwords:\n\n\n# Start with tidytext's built-in stopword list\ndata(\"stop_words\")\n\n# Add our own project-specific stopwords (you can, and will, expand this list later)\ncustom_stopwords &lt;- tibble(\n  word = c(\n    \"vnto\", \"haue\", \"doo\", \"hath\", \"bee\", \"ye\", \"thee\"\n  )\n)\n\nall_stopwords &lt;- bind_rows(stop_words, custom_stopwords) %&gt;%\n  distinct(word)\n\nall_stopwords %&gt;% slice(1:10)\n\n# A tibble: 10 × 1\n   word       \n   &lt;chr&gt;      \n 1 a          \n 2 a's        \n 3 able       \n 4 about      \n 5 above      \n 6 according  \n 7 accordingly\n 8 across     \n 9 actually   \n10 after      \n\n\nNext we are going to create a function that tokenizes the texts, removes both standard and custom stopwords, and then counts which words appear most frequently in each document. As you look over this section, you will want to become familiar with the pipe in R (%&gt;%).\n\ntexts is a tibble where each row corresponds to a document;\nunnest_tokens(word, text): splits the text column into individual word tokens, creates a new column called word, and expands the tibble so each row is now one word occurrence\nmutate: here we are normalizing all words to lower case. This is a research decision: I want to count words like “Money,” “MONEY,” and “money” as the same token, so that I can ask how often the texts talk about “money” regardless of capitalization. Note: mutate is part of the dplyr package.\nanti_join: this is where we remove the stopwords we defined earlier and keeps only the tokens that should (we think!) give us the information we want. Look over the information on joins here and make sure that you understand what it’s doing and understand why the tibble format is important! We will be using anti_join frequently.\nFinally we count.\n\n\nword_counts &lt;- texts %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  mutate(word = str_to_lower(word)) %&gt;%\n  anti_join(all_stopwords, by = \"word\") %&gt;%\n  count(doc_title, word, sort = TRUE)\n\nword_counts\n\n# A tibble: 7,443 × 3\n   doc_title word            n\n   &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;\n 1 Text A    trade         232\n 2 Text A    exchange      186\n 3 Text B    trade         185\n 4 Text A    ſo            184\n 5 Text A    malynes       158\n 6 Text A    merchants     126\n 7 Text A    mony          118\n 8 Text A    hee           115\n 9 Text A    kingdome      113\n10 Text A    commodities    96\n# ℹ 7,433 more rows",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#comparing-word-frequencies-across-texts",
    "href": "week-02-basics.html#comparing-word-frequencies-across-texts",
    "title": "Week 02: Basics",
    "section": "Comparing word frequencies across texts",
    "text": "Comparing word frequencies across texts\nLet’s compare how individual words differ across texts after stopword removal through a first visualization. To keep the visualization readable, we focus on the top 20 most frequent words overall. For now, we won’t get into all the details of the visualization code; we will focus on the main components.\nThe goal of this visualization is a side-by-side comparison plot showing the top 20 most frequent words across the texts with one facet per text. Note: these are the same words for both texts. This means that we are creating a direct comparison of how Misselden uses the most frequent terms.\n\nWe start by putting word_count in the correct format with pivot_wider: each word will have a column for each document’s count.\nvalues_fill = 0: if a word never appears in a text, it gets a 0 instead of NA.\nNext, we rank words based on this criterion: for each word, look at how often it appears in Text A and how often it appears in Text B, and keep the largest count.\n\nThis is what is achieved by computing the row-wise maximum with pmax and then defining the vector: max_n = pmax(TextA, TextB).\nmutate() adds a column to word_comparison_tbl (the wide version of word_count we are defining in this block) based on how we just defined max_n (that is: it takes the vector and attaches it as a column to the tibble).\narrange(desc(max_n) sorts the words from most to least frequent based on max_n.\n\n\n\nStop and regroup:\nI have just implicitly (sneakily) introduced some new things and you might find it helpful to take a look at this introduction to data structures. We will reinforce this topic as we go so don’t panic if this is new to you!\n\n\nBack to the code:\nWe have produced a wide, ranked comparison table. We want to turn into a tidy table that can easily be plotted. To do this we need three main steps:\n\nselect the top 20 words to plot (based on our ranking criterion above). We will do this with slice_head(n = plot_n_words), which takes the first plot_n_words of word_comparison_tbl.\nreshape from wide to long to please ggplot: we will talk about ggplot more in the future, for now, we can treat it as a blackbox that produces nice visualizations.\norder the words for the plot with mutate(word = fct_reorder(word, n, .fun = max)). Note: here too, I am asking you to take this as a blackbox for now. What this step is trying to solve is the problem (for us) that a character vector has no inherent order.\n\nFinally we plot!\n\nplot_n_words &lt;- 20  # you can change this as needed\n\n# Select the most frequent words overall\nword_comparison_tbl &lt;- word_counts %&gt;%\n  pivot_wider(\n    names_from = doc_title,\n    values_from = n,\n    values_fill = 0\n  ) %&gt;%\n  mutate(max_n = pmax(`Text A`, `Text B`)) %&gt;%\n  arrange(desc(max_n))\n\nword_plot_data &lt;- word_comparison_tbl %&gt;%\n  slice_head(n = plot_n_words) %&gt;%\n  pivot_longer(\n    cols = c(`Text A`, `Text B`),\n    names_to = \"doc_title\",\n    values_to = \"n\"\n  ) %&gt;%\n  mutate(word = fct_reorder(word, n, .fun = max))\n\nggplot(word_plot_data, aes(x = n, y = word)) + #black magic happens thanks to ggplot\n  geom_col() +\n  facet_wrap(~ doc_title, scales = \"free_x\") +\n  labs(\n    title = \"Most frequent words (stopwords removed)\",\n    subtitle = paste0(\n      \"Top \", plot_n_words,\n      \" words by maximum frequency across both texts\"\n    ),\n    x = \"Word frequency\",\n    y = NULL\n  ) +\n  theme_minimal()",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#bigrams-starting-to-think-about-context",
    "href": "week-02-basics.html#bigrams-starting-to-think-about-context",
    "title": "Week 02: Basics",
    "section": "Bigrams: starting to think about context",
    "text": "Bigrams: starting to think about context\nSingle-word frequencies tell us what terms are common, but they don’t tell us about the context of these words. Our next step will be to try to get a first, basic understanding of how words in our texts fit together. Bigrams allow us to see which words appear together, capturing short phrases and recurring ideas. This is also our first step towards exploring collocations, formulaic language, and discursive patterns, rather than just isolated vocabulary.\n\nYou should be able to understand the syntax of this first step (make sure that you do!):\n\n\nbigrams &lt;- texts %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\nbigrams\n\n# A tibble: 53,809 × 2\n   doc_title bigram           \n   &lt;chr&gt;     &lt;chr&gt;            \n 1 Text A    the circle       \n 2 Text A    circle of        \n 3 Text A    of commerce      \n 4 Text A    commerce the     \n 5 Text A    the prooeme      \n 6 Text A    prooeme herodotvs\n 7 Text A    herodotvs in     \n 8 Text A    in his           \n 9 Text A    his clio         \n10 Text A    clio reportes    \n# ℹ 53,799 more rows\n\n\nCurrently, each bigram is stored as a single string. We want to remove stopwords (using the custom list we created earlier). In order to do that, we need to be able to inspect each word separately in the bigram. separate() does just that! It takes the bigram column and splits each string at the space character. It then creates two new columns: “word1” and “word2.”\n\nbigrams_separated &lt;- bigrams %&gt;%\n  separate(bigram, into = c(\"word1\", \"word2\"), sep = \" \")\n\nbigrams_separated\n\n# A tibble: 53,809 × 3\n   doc_title word1     word2    \n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    \n 1 Text A    the       circle   \n 2 Text A    circle    of       \n 3 Text A    of        commerce \n 4 Text A    commerce  the      \n 5 Text A    the       prooeme  \n 6 Text A    prooeme   herodotvs\n 7 Text A    herodotvs in       \n 8 Text A    in        his      \n 9 Text A    his       clio     \n10 Text A    clio      reportes \n# ℹ 53,799 more rows\n\n\n\nNow we can remove all_stopwords\n\n\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(\n    !word1 %in% all_stopwords$word,\n    !word2 %in% all_stopwords$word\n  )\n\nbigrams_filtered\n\n# A tibble: 7,622 × 3\n   doc_title word1    word2    \n   &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;    \n 1 Text A    prooeme  herodotvs\n 2 Text A    clio     reportes \n 3 Text A    croesvs  king     \n 4 Text A    ſonne    borne    \n 5 Text A    borne    dumbe    \n 6 Text A    king     himſelf  \n 7 Text A    imminent danger   \n 8 Text A    certaine perſian  \n 9 Text A    perſian  ready    \n10 Text A    lay      violent  \n# ℹ 7,612 more rows\n\n\nWe remove bigrams where either word is a stopword, since phrases like “of the” or “and the” are rarely meaningful analytically.\n\nbigram_counts &lt;- bigrams_filtered %&gt;%\n  count(doc_title, word1, word2, sort = TRUE)\n\nbigram_counts\n\n# A tibble: 6,449 × 4\n   doc_title word1     word2           n\n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;\n 1 Text B    common    wealth         48\n 2 Text A    merchants adventurers    35\n 3 Text B    latin     alphabet       34\n 4 Text A    low       countries      21\n 5 Text B    east      india          20\n 6 Text A    free      trade          19\n 7 Text A    latin     alphabet       17\n 8 Text A    natiue    commodities    17\n 9 Text A    forraine  commodities    16\n10 Text A    letters   patents        14\n# ℹ 6,439 more rows\n\n\nNow that we removed the stopwords, we can use unite to put the bigrams back together:\n\nbigram_counts &lt;- bigram_counts %&gt;%\n  unite(bigram, word1, word2, sep = \" \")\n\nbigram_counts\n\n# A tibble: 6,449 × 3\n   doc_title bigram                    n\n   &lt;chr&gt;     &lt;chr&gt;                 &lt;int&gt;\n 1 Text B    common wealth            48\n 2 Text A    merchants adventurers    35\n 3 Text B    latin alphabet           34\n 4 Text A    low countries            21\n 5 Text B    east india               20\n 6 Text A    free trade               19\n 7 Text A    latin alphabet           17\n 8 Text A    natiue commodities       17\n 9 Text A    forraine commodities     16\n10 Text A    letters patents          14\n# ℹ 6,439 more rows",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#comparing-bigrams",
    "href": "week-02-basics.html#comparing-bigrams",
    "title": "Week 02: Basics",
    "section": "Comparing bigrams:",
    "text": "Comparing bigrams:\nSo far, we have looked at the most frequent bigrams within each text. But frequency alone does not tell us what is distinctive. If we compare how often the same bigrams appear across texts, we should be able to start seeing some differences between the two texts.\n\nSince the two texts by Misselden differ in length and in number of bigrams, we want to normalize by the number of bigrams within each document to compare the two.\nWe then reshape the data for comparison: in tidy, we want one row per bigram so that we can easily compare across the two text.\n\n\nbigram_relative &lt;- bigram_counts %&gt;%\n  group_by(doc_title) %&gt;%\n  mutate(\n    total_bigrams = sum(n),\n    proportion = n / total_bigrams\n  ) %&gt;%\n  ungroup()\n\nbigram_wide &lt;- bigram_relative %&gt;%\n  select(doc_title, bigram, proportion) %&gt;%\n  pivot_wider(\n    names_from = doc_title,\n    values_from = proportion,\n    values_fill = 0\n  )\n\nbigram_wide\n\n# A tibble: 6,379 × 3\n   bigram                `Text B` `Text A`\n   &lt;chr&gt;                    &lt;dbl&gt;    &lt;dbl&gt;\n 1 common wealth         0.0187    0.00257\n 2 merchants adventurers 0         0.00692\n 3 latin alphabet        0.0133    0.00336\n 4 low countries         0.00156   0.00415\n 5 east india            0.00780   0      \n 6 free trade            0         0.00376\n 7 natiue commodities    0.00273   0.00336\n 8 forraine commodities  0         0.00316\n 9 letters patents       0.000390  0.00277\n10 thouſand pounds       0         0.00277\n# ℹ 6,369 more rows\n\n\nNext, we are going to contrast the bigrams in the two texts by identifying which biagrams are most likely to distinguish one text from the other. N.B.: this is not a statistical test, we are just exploring the differences in the bigrams between the two texts. To recap, up to this point, we have:\n\nextracted bigrams\nfiltered them\nnormalized them within each document\n\nWe now want to know which bigram is most characteristic of “Text A” (The Circle of Commerce) relative to “Text B” (Free Trade).\n\nbigram_diff &lt;- bigram_wide %&gt;%\n  mutate(\n    diff = `Text A` - `Text B`\n  ) %&gt;%\n  arrange(desc(abs(diff)))\n\nbigram_diff %&gt;% slice(1:20)\n\n# A tibble: 20 × 4\n   bigram                `Text B` `Text A`     diff\n   &lt;chr&gt;                    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 common wealth         0.0187    0.00257 -0.0161 \n 2 latin alphabet        0.0133    0.00336 -0.00989\n 3 east india            0.00780   0       -0.00780\n 4 merchants adventurers 0         0.00692  0.00692\n 5 merchants aduenturers 0.00390   0       -0.00390\n 6 free trade            0         0.00376  0.00376\n 7 forraine commodities  0         0.00316  0.00316\n 8 thouſand pounds       0         0.00277  0.00277\n 9 low countries         0.00156   0.00415  0.00259\n10 letters patents       0.000390  0.00277  0.00238\n11 publique vtility      0.00234   0       -0.00234\n12 cloth trade           0.00429   0.00198 -0.00231\n13 33 ſh                 0         0.00218  0.00218\n14 20 ſhillings          0         0.00198  0.00198\n15 ſh 4                  0         0.00198  0.00198\n16 disorderly trade      0.00195   0       -0.00195\n17 fishing vpon          0.00195   0       -0.00195\n18 india stocke          0.00195   0       -0.00195\n19 kings honour          0.00195   0       -0.00195\n20 maiesties subiects    0.00195   0       -0.00195\n\n\nWhat does diff tell us? If diff &gt; 0, then the bigram is more prominent in Text A; if diff &lt;0, in Text B. If diff is approximately 0, then it is used more or less in similar proportion in the two texts.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  }
]