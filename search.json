[
  {
    "objectID": "week-05-representation.html",
    "href": "week-05-representation.html",
    "title": "week-05-representation.qmd",
    "section": "",
    "text": "In this tutorial, we’ll explore three measures of lexical complexity (also called lexical diversity or lexical richness) using two historical texts; one you know well by now, the other one is a new mystery text that you can find on Canvas:\n\n“The Circle of Commerce” by Edward Misselden (1623)\n“A69858.txt”\n\nWe want to see see if the two texts differ in terms of vocabulary diversity in a meaningful way.\nWhat is Lexical Complexity? Lexical complexity measures how varied a writer’s vocabulary is. A text with high lexical diversity uses many different words, while a text with low diversity repeats the same words frequently. We’ll learn three different, but well-validated, measures:\n\nType-Token Ratio (TTR) - The simplest measure\nGuiraud Index - Corrects for text length\nMeasure of Textual Lexical Diversity (MTLD) - The most sophisticated measure\n\n\n\nWe are going to introduce a new library for MTLD (koRpus). This is a great library to be familiar with because it includes functions for automatics language detection, hyphenation, several indices of lexical diversity (including MTLD), and indices of readability. It’s also well documented and a standard package in academic work.\n\nlibrary(readr) \nlibrary(tidyverse)\nlibrary(tidyr) \nlibrary(tidytext) \nlibrary(ggplot2) \nlibrary(udpipe)\nlibrary(koRpus)\n\n#install.packages(\"koRpus.lang.en\") # You will need to do this once\nlibrary(koRpus.lang.en)\n\ntheme_set(theme_minimal(base_size = 14))\n\n\n# Read the text files\ncircle &lt;- read_file(\"texts/A07594__Circle_of_Commerce.txt\")\nmystery &lt;- read_file(\"texts/A69858.txt\")\n\n# Create a tidy data frame with both texts\ntexts_df &lt;- tibble(\n  document = c(\"Circle of Commerce\", \"A69858\"),\n  author = c(\"Misselden\", \"Unknown\"),\n  text = c(circle, mystery)\n)\n\n# Display basic information\ntexts_df %&gt;%\n  select(document, author) %&gt;%\n  knitr::kable(caption = \"Our Two Texts\")\n\n\nOur Two Texts\n\n\ndocument\nauthor\n\n\n\n\nCircle of Commerce\nMisselden\n\n\nA69858\nUnknown\n\n\n\n\n\nNow that we have our two texts loaded, let start with Type-Toke Ratio (TTR). TTR is the simplest measure of lexical diversity. It’s calculated as:\n\\[\\text{TTR} = \\frac{\\text{Types (unique words)}}{\\text{Tokens (total words)}}\\]\n\n# Tokenize the texts and do some basic cleaning (review week 2 if you are usure!)\ntokens &lt;- texts_df %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  mutate(word = str_to_lower(word))\n\n# Calculate TTR for each text\nttr_results &lt;- tokens %&gt;%\n  group_by(document, author) %&gt;%\n  summarise(\n    tokens = n(),                    # Count total words\n    types = n_distinct(word),        # Count unique words\n    ttr = types / tokens,            # Calculate TTR\n    .groups = \"drop\"\n  )\n\n# Display results\nttr_results %&gt;%\n  knitr::kable(\n    digits = 3,\n    caption = \"Type-Token Ratio Results\",\n    col.names = c(\"Document\", \"Author\", \"Total Words\", \"Unique Words\", \"TTR\")\n  )\n\n\nType-Token Ratio Results\n\n\nDocument\nAuthor\nTotal Words\nUnique Words\nTTR\n\n\n\n\nA69858\nUnknown\n5942\n1317\n0.222\n\n\nCircle of Commerce\nMisselden\n33894\n4769\n0.141\n\n\n\n\n\nNote: I am using a new function, knitr::kable(x), to display the results as a simple table. It takes whatever is the x (=data frame or matrix) and makes it into a table.\nLet’s visualize the results that we just got:\n\n# Create a bar plot comparing TTR\nggplot(ttr_results, aes(x = author, y = ttr, fill = author)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(ttr, 3)), \n            vjust = -0.5, size = 5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Type-Token Ratio Comparison\",\n    subtitle = \"Higher values = more diverse vocabulary\",\n    x = NULL,\n    y = \"Type-Token Ratio (TTR)\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(0, max(ttr_results$ttr) * 1.15)\n\n\n\n\n\n\n\n\nFrom this first measure, we see that Misselden’s text has a higher TTR than the unknown one. What does this mean, exactly? Well, we can see that in The Circle of Commerce about 14.1% of the words are new word forms (that is, a word string–after normalization–that has not appeared before in exactly that form in the text so far), while for the “mystery” text, only 11.6% of the words are new word forms. That is, TTR is a measure of “non-repetition” of word forms. Another way to think about it is that for The Circle of Commerce, if you encounter any random word in the text, that’s a 14.1% chance that it’s a word type you haven’t seen before. So, The Circle of Commerce has greater lexical variety than A69858: it repeats itself slightly less than A69858.\nWarning: longer texts will almost always have a lower TTR score than shorter texts, even if they have a richer vocabulary (can you see why?). Earlier, we saw that A6858 has, in total, 36285 words, while Circle of Commerce has, in total, 33894. The two texts aren’t substantially different in length (2,391 words, so about 7%), but we need to double check with a length-corrected measure (such as Guiraud and MTLD).\nWhy you might still want to compute TTR: it’s a standard across disciplines and you will encounter it in publications, so you need to know what you are looking at. Anyone reading a corpus analysis will recognize it and be able to understand what it represents. TTR is a standard measure of lexical variety. It’s easy to audit and easy to reproduce. It’s also a good first diagnostic check.\n\n\n\nThe Guiraud Index (also called Root TTR) solves the text length problem by using a mathematical correction:\n\\[\\text{Guiraud} = \\frac{\\text{Types}}{\\sqrt{\\text{Tokens}}}\\]\nBy dividing by the square root of tokens instead of the raw count, the Guiraud Index is more stable across different text lengths. This adjustment is a practical approximation that assumes that as the length of a text doubles, its vocabulary does not double. The square root in the Guiraud index is there because vocabulary growth in texts is empirically sublinear: as a text gets longer, new words appear more slowly. The Guiraud Index essentially asks: how many distinct word forms does a text introduce once we partially correct for the fact that longer texts repeat terms more often?\nCalculate Guiraud Index:\n\n# Calculate Guiraud Index for each text\nguiraud_results &lt;- tokens %&gt;%\n  group_by(document, author) %&gt;%\n  summarise(\n    tokens = n(),\n    types = n_distinct(word),\n    guiraud = types / sqrt(tokens),   # Guiraud formula\n    .groups = \"drop\"\n  )\n\n# Display results\nguiraud_results %&gt;%\n  knitr::kable(\n    digits = 3,\n    caption = \"Guiraud Index Results\",\n    col.names = c(\"Document\", \"Author\", \"Total Words\", \"Unique Words\", \"Guiraud Index\")\n  )\n\n\nGuiraud Index Results\n\n\n\n\n\n\n\n\n\nDocument\nAuthor\nTotal Words\nUnique Words\nGuiraud Index\n\n\n\n\nA69858\nUnknown\n5942\n1317\n17.085\n\n\nCircle of Commerce\nMisselden\n33894\n4769\n25.904\n\n\n\n\n\nAnd let’s visualize it:\n\n# Create a bar plot comparing Guiraud Index\nggplot(guiraud_results, aes(x = author, y = guiraud, fill = author)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(guiraud, 2)), \n            vjust = -0.5, size = 5) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(\n    title = \"Guiraud Index Comparison\",\n    subtitle = \"Length-corrected measure of lexical diversity\",\n    x = NULL,\n    y = \"Guiraud Index\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(0, max(guiraud_results$guiraud) * 1.15)\n\n\n\n\n\n\n\n\nThis result confirms what we noticed with TTR: The Circle has a more varied vocabulary than the mystery text even when we partially account for the length of the text. Great!\nHowever, you want to make sure that when you compare texts using the Guiraud Index, you are looking at texts that are of similar genre and (though less so) similar length. Diferrent types of texts are associated with different demands onn vocabulary and rhetorical strategies (for example, a legal argument will require different vocabulary choice than a news report of the same court proceeding). Because the index still depends on text length and genre, there is no universal ‘benchmark’ scale. Instead, values should be interpreted relatively, as we are doing here.\nThis leads us to:\n\n\n\nWhat is MTLD?\nMTLD is the most sophisticated measure we’ll use. It was designed to account for the problems of the measures above. TTR and Guiraud essentially ask: given a text of length N, how many distinct word forms does it contain? MTD instead asks: how long can a text continue before its lexical diversity drops below a fixed threshold? It works by:\n\nReading through the text sequentially\nCalculating a running TTR as it goes\nCounting how many times the TTR drops below 0.72 (this has been determined empirically across many text types and genres). Once you detect that the document’s TTR drops below 0.72, you count one factor. Then you reset and restart computing TTR on the next chunk of text.\nThe result is the average “length” needed before vocabulary starts repeating and it’s computed by:\n\n\n\n\nThe Measure of Textual Lexical Diversity (MTLD) is computed as:\n\\[\n\\text{MTLD} = \\frac{N}{F}\n\\]\nwhere:\n\nN = total number of word tokens in the text\nF = total number of factors, including any partial factor at the end of the text\n\nIn practice, MTLD is usually computed in both directions (forward and backward through the text), and the final value is the mean of the two:\n\\[\n\\text{MTLD}_{\\text{final}} =\n\\frac{\\text{MTLD}_{\\text{forward}} + \\text{MTLD}_{\\text{backward}}}{2}\n\\]\nHigher MTLD = more diverse vocabulary (the author can go longer before repeating words).\nLet’s compute this second version (both forward and backward through the text using the koRpus package (and use the formatting that it requires). Note: koRpus gives you a number of options which you can find here.\n\n# Function to calculate MTLD for a text\ncalculate_mtld &lt;- function(text_string, doc_name) {\n  # Create a temporary file (koRpus requirement)\n  temp_file &lt;- tempfile(fileext = \".txt\")\n  writeLines(text_string, temp_file)\n  \n  # Tokenize with koRpus\n  tokenized &lt;- tokenize(temp_file, lang = \"en\")\n  \n  # Calculate MTLD\n  mtld_result &lt;- MTLD(tokenized)\n  \n  # Extract the MTLD value\n  mtld_value &lt;- mtld_result@MTLD$MTLD\n  \n  # Clean up\n  unlink(temp_file)\n  \n  return(mtld_value)\n}\n\n# Calculate MTLD for both texts\nmtld_results &lt;- texts_df %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    mtld = calculate_mtld(text, document)\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(document, author, mtld)\n\nLanguage: \"en\"\nLanguage: \"en\"\n\n# Display results\nmtld_results %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"MTLD Results\",\n    col.names = c(\"Document\", \"Author\", \"MTLD\")\n  )\n\n\nMTLD Results\n\n\nDocument\nAuthor\nMTLD\n\n\n\n\nCircle of Commerce\nMisselden\n63.09\n\n\nA69858\nUnknown\n84.17\n\n\n\n\n\nAn R coding note for those of you who are new to coding. The MTLD computation using koRups is being put together in the curly brackets that define all the steps that the new function we are defining, calculate_mtld, will perform. NB: calculate_mtlddoes not actually compute MTLD, it just takes a text string (our Tidy data), converts it to a file for koRpus, calls in koRpus to compute MTLD, and then returns everything to Tidy.\nLet’s visualize the results:\n\n# Create a bar plot comparing MTLD\nggplot(mtld_results, aes(x = author, y = mtld, fill = author)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(mtld, 1)), \n            vjust = -0.5, size = 5) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"MTLD Comparison\",\n    subtitle = \"Mean Length of Sequential Word Strings (Higher = More Diverse)\",\n    x = NULL,\n    y = \"MTLD Score\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(0, max(mtld_results$mtld) * 1.15)\n\n\n\n\n\n\n\n\nWell, this is interesting and important in understanding how these measures work. Unlike TTR and Guiraud, MTLD assigns a greater lexical variety to our mystery text. The two documents are of similar length, so we can’t just attribute it to that. What MTLD is picking up is a difference in how lexical variety is distributed across the text.\nTTR and Guiraud only care about how many distinct word forms there are overall in the texts. They are both global measures of variety.\nMTLD is sensitive to how repetition is patterned through text “time” (think of reading the text as your measure of text “time”). A higher MTLD in the mystery text means that it sustains lexical variety more consistently across its length while Misselden’s lower MTLD means that The Circle includes repetition-heavy chunks earlier on and more often (even though it globally ends up with more unique token types).\nGive this kind of result, we expect that in The Circle, Misselden includes lots of repeated terms and phrases within sections (local repetition as he perhaps defines and explains a given concept/idea), but overall introduces more terms. The mystery text, instead, distributes its vocabulary more evenly. A69858 uses fewer unique words in total, but sustain more lexical variety within any given passage.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 05: Text Representation (2)"
    ]
  },
  {
    "objectID": "week-05-representation.html#measures-of-lexical-complexity",
    "href": "week-05-representation.html#measures-of-lexical-complexity",
    "title": "week-05-representation.qmd",
    "section": "",
    "text": "In this tutorial, we’ll explore three measures of lexical complexity (also called lexical diversity or lexical richness) using two historical texts; one you know well by now, the other one is a new mystery text that you can find on Canvas:\n\n“The Circle of Commerce” by Edward Misselden (1623)\n“A69858.txt”\n\nWe want to see see if the two texts differ in terms of vocabulary diversity in a meaningful way.\nWhat is Lexical Complexity? Lexical complexity measures how varied a writer’s vocabulary is. A text with high lexical diversity uses many different words, while a text with low diversity repeats the same words frequently. We’ll learn three different, but well-validated, measures:\n\nType-Token Ratio (TTR) - The simplest measure\nGuiraud Index - Corrects for text length\nMeasure of Textual Lexical Diversity (MTLD) - The most sophisticated measure\n\n\n\nWe are going to introduce a new library for MTLD (koRpus). This is a great library to be familiar with because it includes functions for automatics language detection, hyphenation, several indices of lexical diversity (including MTLD), and indices of readability. It’s also well documented and a standard package in academic work.\n\nlibrary(readr) \nlibrary(tidyverse)\nlibrary(tidyr) \nlibrary(tidytext) \nlibrary(ggplot2) \nlibrary(udpipe)\nlibrary(koRpus)\n\n#install.packages(\"koRpus.lang.en\") # You will need to do this once\nlibrary(koRpus.lang.en)\n\ntheme_set(theme_minimal(base_size = 14))\n\n\n# Read the text files\ncircle &lt;- read_file(\"texts/A07594__Circle_of_Commerce.txt\")\nmystery &lt;- read_file(\"texts/A69858.txt\")\n\n# Create a tidy data frame with both texts\ntexts_df &lt;- tibble(\n  document = c(\"Circle of Commerce\", \"A69858\"),\n  author = c(\"Misselden\", \"Unknown\"),\n  text = c(circle, mystery)\n)\n\n# Display basic information\ntexts_df %&gt;%\n  select(document, author) %&gt;%\n  knitr::kable(caption = \"Our Two Texts\")\n\n\nOur Two Texts\n\n\ndocument\nauthor\n\n\n\n\nCircle of Commerce\nMisselden\n\n\nA69858\nUnknown\n\n\n\n\n\nNow that we have our two texts loaded, let start with Type-Toke Ratio (TTR). TTR is the simplest measure of lexical diversity. It’s calculated as:\n\\[\\text{TTR} = \\frac{\\text{Types (unique words)}}{\\text{Tokens (total words)}}\\]\n\n# Tokenize the texts and do some basic cleaning (review week 2 if you are usure!)\ntokens &lt;- texts_df %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  mutate(word = str_to_lower(word))\n\n# Calculate TTR for each text\nttr_results &lt;- tokens %&gt;%\n  group_by(document, author) %&gt;%\n  summarise(\n    tokens = n(),                    # Count total words\n    types = n_distinct(word),        # Count unique words\n    ttr = types / tokens,            # Calculate TTR\n    .groups = \"drop\"\n  )\n\n# Display results\nttr_results %&gt;%\n  knitr::kable(\n    digits = 3,\n    caption = \"Type-Token Ratio Results\",\n    col.names = c(\"Document\", \"Author\", \"Total Words\", \"Unique Words\", \"TTR\")\n  )\n\n\nType-Token Ratio Results\n\n\nDocument\nAuthor\nTotal Words\nUnique Words\nTTR\n\n\n\n\nA69858\nUnknown\n5942\n1317\n0.222\n\n\nCircle of Commerce\nMisselden\n33894\n4769\n0.141\n\n\n\n\n\nNote: I am using a new function, knitr::kable(x), to display the results as a simple table. It takes whatever is the x (=data frame or matrix) and makes it into a table.\nLet’s visualize the results that we just got:\n\n# Create a bar plot comparing TTR\nggplot(ttr_results, aes(x = author, y = ttr, fill = author)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(ttr, 3)), \n            vjust = -0.5, size = 5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Type-Token Ratio Comparison\",\n    subtitle = \"Higher values = more diverse vocabulary\",\n    x = NULL,\n    y = \"Type-Token Ratio (TTR)\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(0, max(ttr_results$ttr) * 1.15)\n\n\n\n\n\n\n\n\nFrom this first measure, we see that Misselden’s text has a higher TTR than the unknown one. What does this mean, exactly? Well, we can see that in The Circle of Commerce about 14.1% of the words are new word forms (that is, a word string–after normalization–that has not appeared before in exactly that form in the text so far), while for the “mystery” text, only 11.6% of the words are new word forms. That is, TTR is a measure of “non-repetition” of word forms. Another way to think about it is that for The Circle of Commerce, if you encounter any random word in the text, that’s a 14.1% chance that it’s a word type you haven’t seen before. So, The Circle of Commerce has greater lexical variety than A69858: it repeats itself slightly less than A69858.\nWarning: longer texts will almost always have a lower TTR score than shorter texts, even if they have a richer vocabulary (can you see why?). Earlier, we saw that A6858 has, in total, 36285 words, while Circle of Commerce has, in total, 33894. The two texts aren’t substantially different in length (2,391 words, so about 7%), but we need to double check with a length-corrected measure (such as Guiraud and MTLD).\nWhy you might still want to compute TTR: it’s a standard across disciplines and you will encounter it in publications, so you need to know what you are looking at. Anyone reading a corpus analysis will recognize it and be able to understand what it represents. TTR is a standard measure of lexical variety. It’s easy to audit and easy to reproduce. It’s also a good first diagnostic check.\n\n\n\nThe Guiraud Index (also called Root TTR) solves the text length problem by using a mathematical correction:\n\\[\\text{Guiraud} = \\frac{\\text{Types}}{\\sqrt{\\text{Tokens}}}\\]\nBy dividing by the square root of tokens instead of the raw count, the Guiraud Index is more stable across different text lengths. This adjustment is a practical approximation that assumes that as the length of a text doubles, its vocabulary does not double. The square root in the Guiraud index is there because vocabulary growth in texts is empirically sublinear: as a text gets longer, new words appear more slowly. The Guiraud Index essentially asks: how many distinct word forms does a text introduce once we partially correct for the fact that longer texts repeat terms more often?\nCalculate Guiraud Index:\n\n# Calculate Guiraud Index for each text\nguiraud_results &lt;- tokens %&gt;%\n  group_by(document, author) %&gt;%\n  summarise(\n    tokens = n(),\n    types = n_distinct(word),\n    guiraud = types / sqrt(tokens),   # Guiraud formula\n    .groups = \"drop\"\n  )\n\n# Display results\nguiraud_results %&gt;%\n  knitr::kable(\n    digits = 3,\n    caption = \"Guiraud Index Results\",\n    col.names = c(\"Document\", \"Author\", \"Total Words\", \"Unique Words\", \"Guiraud Index\")\n  )\n\n\nGuiraud Index Results\n\n\n\n\n\n\n\n\n\nDocument\nAuthor\nTotal Words\nUnique Words\nGuiraud Index\n\n\n\n\nA69858\nUnknown\n5942\n1317\n17.085\n\n\nCircle of Commerce\nMisselden\n33894\n4769\n25.904\n\n\n\n\n\nAnd let’s visualize it:\n\n# Create a bar plot comparing Guiraud Index\nggplot(guiraud_results, aes(x = author, y = guiraud, fill = author)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(guiraud, 2)), \n            vjust = -0.5, size = 5) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(\n    title = \"Guiraud Index Comparison\",\n    subtitle = \"Length-corrected measure of lexical diversity\",\n    x = NULL,\n    y = \"Guiraud Index\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(0, max(guiraud_results$guiraud) * 1.15)\n\n\n\n\n\n\n\n\nThis result confirms what we noticed with TTR: The Circle has a more varied vocabulary than the mystery text even when we partially account for the length of the text. Great!\nHowever, you want to make sure that when you compare texts using the Guiraud Index, you are looking at texts that are of similar genre and (though less so) similar length. Diferrent types of texts are associated with different demands onn vocabulary and rhetorical strategies (for example, a legal argument will require different vocabulary choice than a news report of the same court proceeding). Because the index still depends on text length and genre, there is no universal ‘benchmark’ scale. Instead, values should be interpreted relatively, as we are doing here.\nThis leads us to:\n\n\n\nWhat is MTLD?\nMTLD is the most sophisticated measure we’ll use. It was designed to account for the problems of the measures above. TTR and Guiraud essentially ask: given a text of length N, how many distinct word forms does it contain? MTD instead asks: how long can a text continue before its lexical diversity drops below a fixed threshold? It works by:\n\nReading through the text sequentially\nCalculating a running TTR as it goes\nCounting how many times the TTR drops below 0.72 (this has been determined empirically across many text types and genres). Once you detect that the document’s TTR drops below 0.72, you count one factor. Then you reset and restart computing TTR on the next chunk of text.\nThe result is the average “length” needed before vocabulary starts repeating and it’s computed by:\n\n\n\n\nThe Measure of Textual Lexical Diversity (MTLD) is computed as:\n\\[\n\\text{MTLD} = \\frac{N}{F}\n\\]\nwhere:\n\nN = total number of word tokens in the text\nF = total number of factors, including any partial factor at the end of the text\n\nIn practice, MTLD is usually computed in both directions (forward and backward through the text), and the final value is the mean of the two:\n\\[\n\\text{MTLD}_{\\text{final}} =\n\\frac{\\text{MTLD}_{\\text{forward}} + \\text{MTLD}_{\\text{backward}}}{2}\n\\]\nHigher MTLD = more diverse vocabulary (the author can go longer before repeating words).\nLet’s compute this second version (both forward and backward through the text using the koRpus package (and use the formatting that it requires). Note: koRpus gives you a number of options which you can find here.\n\n# Function to calculate MTLD for a text\ncalculate_mtld &lt;- function(text_string, doc_name) {\n  # Create a temporary file (koRpus requirement)\n  temp_file &lt;- tempfile(fileext = \".txt\")\n  writeLines(text_string, temp_file)\n  \n  # Tokenize with koRpus\n  tokenized &lt;- tokenize(temp_file, lang = \"en\")\n  \n  # Calculate MTLD\n  mtld_result &lt;- MTLD(tokenized)\n  \n  # Extract the MTLD value\n  mtld_value &lt;- mtld_result@MTLD$MTLD\n  \n  # Clean up\n  unlink(temp_file)\n  \n  return(mtld_value)\n}\n\n# Calculate MTLD for both texts\nmtld_results &lt;- texts_df %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    mtld = calculate_mtld(text, document)\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(document, author, mtld)\n\nLanguage: \"en\"\nLanguage: \"en\"\n\n# Display results\nmtld_results %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"MTLD Results\",\n    col.names = c(\"Document\", \"Author\", \"MTLD\")\n  )\n\n\nMTLD Results\n\n\nDocument\nAuthor\nMTLD\n\n\n\n\nCircle of Commerce\nMisselden\n63.09\n\n\nA69858\nUnknown\n84.17\n\n\n\n\n\nAn R coding note for those of you who are new to coding. The MTLD computation using koRups is being put together in the curly brackets that define all the steps that the new function we are defining, calculate_mtld, will perform. NB: calculate_mtlddoes not actually compute MTLD, it just takes a text string (our Tidy data), converts it to a file for koRpus, calls in koRpus to compute MTLD, and then returns everything to Tidy.\nLet’s visualize the results:\n\n# Create a bar plot comparing MTLD\nggplot(mtld_results, aes(x = author, y = mtld, fill = author)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(mtld, 1)), \n            vjust = -0.5, size = 5) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"MTLD Comparison\",\n    subtitle = \"Mean Length of Sequential Word Strings (Higher = More Diverse)\",\n    x = NULL,\n    y = \"MTLD Score\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(0, max(mtld_results$mtld) * 1.15)\n\n\n\n\n\n\n\n\nWell, this is interesting and important in understanding how these measures work. Unlike TTR and Guiraud, MTLD assigns a greater lexical variety to our mystery text. The two documents are of similar length, so we can’t just attribute it to that. What MTLD is picking up is a difference in how lexical variety is distributed across the text.\nTTR and Guiraud only care about how many distinct word forms there are overall in the texts. They are both global measures of variety.\nMTLD is sensitive to how repetition is patterned through text “time” (think of reading the text as your measure of text “time”). A higher MTLD in the mystery text means that it sustains lexical variety more consistently across its length while Misselden’s lower MTLD means that The Circle includes repetition-heavy chunks earlier on and more often (even though it globally ends up with more unique token types).\nGive this kind of result, we expect that in The Circle, Misselden includes lots of repeated terms and phrases within sections (local repetition as he perhaps defines and explains a given concept/idea), but overall introduces more terms. The mystery text, instead, distributes its vocabulary more evenly. A69858 uses fewer unique words in total, but sustain more lexical variety within any given passage.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 05: Text Representation (2)"
    ]
  },
  {
    "objectID": "week-05-representation.html#measures-of-syntactic-complexity",
    "href": "week-05-representation.html#measures-of-syntactic-complexity",
    "title": "week-05-representation.qmd",
    "section": "Measures of Syntactic Complexity",
    "text": "Measures of Syntactic Complexity\nSo much for vocabulary measures! What if we want to understand the complexity (or lack thereof) of the syntax of a text?\nThe core idea that we will use for this is to measure structure via frequency counts (e.g., number of clauses, number of dependent clauses), ratios (e.g., clauses per sentence; dependent clauses per clause), or length indices (e.g., mean length of clause in words).\nNote: research in syntax complexity is particularly well-represented in second language acquisition research (see, for example here for a strong discussion).\nWe are still working with the same two texts as above!\n\ntexts_df &lt;- tibble(document = c(\"Circle of Commerce\", \"A69858\"),\n                   author = c(\"Misselden\", \"Unknown\"),\n                   text = c(circle, mystery)\n                   )\n\ntexts_df\n\n# A tibble: 2 × 3\n  document           author    text                                             \n  &lt;chr&gt;              &lt;chr&gt;     &lt;chr&gt;                                            \n1 Circle of Commerce Misselden \"THE CIRCLE OF COMMERCE. The Prooeme. HERODOTVS …\n2 A69858             Unknown   \"THe Author craves leave in the first place, to …\n\n\nWe are going to use something new UDPipe to: tokenize, do parts of speech tagging, lemmatize, and do dependency parsing. Note: for Early Modern texts there is a package out of Northwestern (MorphAdorner) that performs better. Since I don’t have any Early Modernists this semester, I am going to stick with a more generalizable workflow.\n\n#Load an English UD (= Univesal Dependencies) model ONCE\n\nmodel_info &lt;- udpipe_download_model(language = \"english-ewt\")\n\nDownloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe to D:/Users/astri/Desktop/R-work/IDS_570_TAD/english-ewt-ud-2.5-191206.udpipe\n\n\n - This model has been trained on version 2.5 of data from https://universaldependencies.org\n\n\n - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0\n\n\n - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details.\n\n\n - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette('udpipe-train', package = 'udpipe')\n\n\nDownloading finished, model stored at 'D:/Users/astri/Desktop/R-work/IDS_570_TAD/english-ewt-ud-2.5-191206.udpipe'\n\nud_model &lt;- udpipe_load_model(model_info$file_model)\n\nI chose english-ewt because it’s widely adopted and well-documented for English prose. It captures the core syntactic relations and it does a decent job on pre-Modern texts. I would not use this for my own research, but that’s a niche issue.\nNow, let’s annotate both texts using UDPipe:\n\nanno_df &lt;- texts_df %&gt;%\n  mutate(\n    # Parse each text with the UD parser; set doc_id to our document name\n    anno = map2(text, document, ~ udpipe_annotate(ud_model, x = .x, doc_id = .y) %&gt;%\n      as.data.frame())\n  ) %&gt;%\n  # Keep only parsed annotations, then unnest into rows\n  select(anno) %&gt;%\n  unnest(anno) %&gt;%\n  # Use the UD doc_id as our document label (and drop any duplicates cleanly)\n  rename(document = doc_id) %&gt;%\n  # Select columns for syntactic analysis\n  select(\n    document,\n    paragraph_id,\n    sentence_id,\n    token_id,\n    token,\n    lemma,\n    upos,          # part of speech\n    feats,         # grammatical features (e.g., verb form)\n    head_token_id, # head of dependency relation\n    dep_rel        # dependency relation type\n  )\n\nanno_df %&gt;% glimpse()\n\nRows: 47,121\nColumns: 10\n$ document      &lt;chr&gt; \"Circle of Commerce\", \"Circle of Commerce\", \"Circle of C…\n$ paragraph_id  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ sentence_id   &lt;int&gt; 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ token_id      &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"…\n$ token         &lt;chr&gt; \"THE\", \"CIRCLE\", \"OF\", \"COMMERCE\", \".\", \"The\", \"Prooeme\"…\n$ lemma         &lt;chr&gt; \"the\", \"circle\", \"of\", \"Commerce\", \".\", \"the\", \"Prooeme\"…\n$ upos          &lt;chr&gt; \"DET\", \"NOUN\", \"ADP\", \"PROPN\", \"PUNCT\", \"DET\", \"NOUN\", \"…\n$ feats         &lt;chr&gt; \"Definite=Def|PronType=Art\", \"Number=Sing\", NA, \"Number=…\n$ head_token_id &lt;chr&gt; \"2\", \"0\", \"4\", \"2\", \"2\", \"2\", \"0\", \"2\", \"6\", \"4\", \"4\", \"…\n$ dep_rel       &lt;chr&gt; \"det\", \"root\", \"case\", \"nmod\", \"punct\", \"det\", \"root\", \"…\n\n\nFor head_token_id, what we are doing at (*) is important in creating a dependency tree. Here’s a toy example:\n\n# Create an example parsed sentence\nexample_sentence &lt;- tibble(\n  token = c(\"The\", \"big\", \"dog\", \"barks\"),\n  token_id = c(1, 2, 3, 4),\n  head_token_id = c(3, 3, 4, 0),\n  Relationship = c(\n    '\"The\" depends on word #3 (dog)',\n    '\"big\" depends on word #3 (dog)',\n    '\"dog\" depends on word #4 (barks)',\n    '\"barks\" is the ROOT (doesn\\'t depend on anything)'\n  )\n)\n\nexample_sentence %&gt;%\n  knitr::kable(\n    caption = 'Example: Dependency structure of \"The big dog barks\"',\n    align = c(\"l\", \"c\", \"c\", \"l\")\n  )\n\n\nExample: Dependency structure of “The big dog barks”\n\n\n\n\n\n\n\n\ntoken\ntoken_id\nhead_token_id\nRelationship\n\n\n\n\nThe\n1\n3\n“The” depends on word #3 (dog)\n\n\nbig\n2\n3\n“big” depends on word #3 (dog)\n\n\ndog\n3\n4\n“dog” depends on word #4 (barks)\n\n\nbarks\n4\n0\n“barks” is the ROOT (doesn’t depend on anything)\n\n\n\n\n\nThere will be grammar: we are going to define some syntactic complexity features by using dependency relations. In the code below, we are going to create binary flags for different syntactic structures.\n\nsyntax_df &lt;- anno_df %&gt;%\nmutate(\nis_word = upos != \"PUNCT\", #&lt;--is it a word (and not punctuation?)\n\n\n# Is this an indipendent clause? finite verbs are proxy for indipendent clauses\nis_clause = (upos %in% c(\"VERB\", \"AUX\")) &\n            str_detect(coalesce(feats, \"\"), \"VerbForm=Fin\"),\n\n# Dependent clause? \nis_dep_clause = dep_rel %in% c(\n  \"advcl\", #adverbial clause \n  \"ccomp\", # clausal complement\n  \"xcomp\", #open clausal complement\n  \"acl\", #adnomial clause\n  \"acl:relcl\" #relative clause\n),\n\n# Is this coordination? That is, does it use \"and\" \"or\" etc.?\nis_coord = dep_rel %in% c(\"conj\", \"cc\"),\n\n# Nominal complexity: these relations make noun phrases more complex\nis_complex_nominal = dep_rel %in% c(\n  \"amod\", # adjective modifier (\"big cup\")\n  \"nmod\", #nominal modifier (\"cup of tea\")\n  \"compound\", # compound (\"lemon tea\")\n  \"appos\" #apposition (\"team my favorite\")\n)\n\n)\n\nA quick note about punctuation: we filter out punctuation because punctuation itself doesn’t add to a sentence complexity. Think of the following\nIf you want to know a bit more about clauses, you can find the full list here. But a quick summary for our needs is below:\n\n\n\n\nTable 1\n\n\n\n\n\n\nRelation\nFull Name\nWhat It Modifies\nExample\nUD Documentation\n\n\n\n\nadvcl\nAdverbial clause\nVerb/predicate\nI left because I was tired\nadvcl\n\n\nccomp\nClausal complement\nVerb (as object)\nShe said that he arrived\nccomp\n\n\nxcomp\nOpen clausal complement\nVerb (shares subject)\nI want to leave\nxcomp\n\n\nacl\nAdnominal clause\nNoun\nThe dog sleeping there\nacl\n\n\nrelcl\nRelative clause\nNoun\nThe book that I read\nacl:relcl\n\n\n\n\n\n\n\n\nLet’s check what we did:\n\n# looking at the first 20 rows in the data frame\nsyntax_df %&gt;% \n  select(document, token, upos, is_clause, is_dep_clause) %&gt;%\n  head(20)\n\n# A tibble: 20 × 5\n   document           token     upos  is_clause is_dep_clause\n   &lt;chr&gt;              &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;     &lt;lgl&gt;        \n 1 Circle of Commerce THE       DET   FALSE     FALSE        \n 2 Circle of Commerce CIRCLE    NOUN  FALSE     FALSE        \n 3 Circle of Commerce OF        ADP   FALSE     FALSE        \n 4 Circle of Commerce COMMERCE  PROPN FALSE     FALSE        \n 5 Circle of Commerce .         PUNCT FALSE     FALSE        \n 6 Circle of Commerce The       DET   FALSE     FALSE        \n 7 Circle of Commerce Prooeme   NOUN  FALSE     FALSE        \n 8 Circle of Commerce .         PUNCT FALSE     FALSE        \n 9 Circle of Commerce HERODOTVS NOUN  FALSE     FALSE        \n10 Circle of Commerce in        ADP   FALSE     FALSE        \n11 Circle of Commerce his       PRON  FALSE     FALSE        \n12 Circle of Commerce CLIO      NOUN  FALSE     FALSE        \n13 Circle of Commerce ,         PUNCT FALSE     FALSE        \n14 Circle of Commerce reportes  VERB  TRUE      FALSE        \n15 Circle of Commerce that      SCONJ FALSE     FALSE        \n16 Circle of Commerce CROESVS   PROPN FALSE     FALSE        \n17 Circle of Commerce King      PROPN FALSE     FALSE        \n18 Circle of Commerce of        ADP   FALSE     FALSE        \n19 Circle of Commerce LYDIA     PROPN FALSE     FALSE        \n20 Circle of Commerce had       VERB  TRUE      TRUE         \n\n\nNow that we have parsed our texts and identified syntactic features, we can measure complexity. Let’s do this through five measures. Important: these measures are meant for relative comparison between documents!\n\nCalculate Mean Length of Sentence (MLS): how long are sentences on average?\nCalculate Clausal Density (C/S): how many clauses per sentence?\nSubordination (DC/C, DC/S): how much embedding?\nCoordination (Coord/C, Coord/S): how much coordination?\nPhrasal Complexity (CN/C, CN/S): how complex are noun phrases?\n\nBefore we jump into the details of these measures, we need to aggregate at the sentence level and count syntactic features for each individual sentence:\n\nsentence_df &lt;- syntax_df %&gt;%\n  filter(is_word) %&gt;%           #count words (not punctuation)\n  group_by(document, sentence_id) %&gt;%   #group by document and sentence\n  summarise(\n    words          = n(),   #number of words per sentence\n    clauses        = sum(is_clause), # number of clauses per sentence\n    dep_clauses    = sum(is_dep_clause), #number of dependent clauses per sentence\n    .groups = \"drop\"\n  )\n\nsentence_df\n\n# A tibble: 1,872 × 5\n   document sentence_id words clauses dep_clauses\n   &lt;chr&gt;          &lt;int&gt; &lt;int&gt;   &lt;int&gt;       &lt;int&gt;\n 1 A69858             1    29       1           2\n 2 A69858             2    44       3           2\n 3 A69858             3    45       6           5\n 4 A69858             4    28       1           3\n 5 A69858             5     6       0           0\n 6 A69858             6    39       1           0\n 7 A69858             7    23       0           2\n 8 A69858             8    64       7           5\n 9 A69858             9    35       3           3\n10 A69858            10    27       1           2\n# ℹ 1,862 more rows\n\n\n1. Mean length of sentence: now we measure the average sentence length by number of words. The assumption behind this step is that longer sentences tend to be more syntactically complex.\n\nmls_df &lt;- sentence_df %&gt;%\ngroup_by(document) %&gt;%\nsummarise(\nMLS = mean(words), # Average words per sentence\n.groups = \"drop\"\n)\n\n# Let's take a look\nmls_df \n\n# A tibble: 2 × 2\n  document             MLS\n  &lt;chr&gt;              &lt;dbl&gt;\n1 A69858              22.9\n2 Circle of Commerce  21.2\n\n\n2. Overall Sentence Complexity (C/S = Clauses per sentence): while longer sentences tend to be more complex, that’s not the only way complexity works. A short sentence in English, such as “The dog I saw bit my cousin,” can be structurally complex. The next step is to count clauses. More clauses allow for the possibility of more syntactic complexity via coordination and subordination.\n\n#Calculate clauses per sentence\nclausal_density_df &lt;- sentence_df %&gt;%\ngroup_by(document) %&gt;%\nsummarise(\nsentences = n(),\nclauses   = sum(clauses),\nC_per_S   = clauses / sentences,\n.groups = \"drop\"\n)\n\n# Let's check and how do they compare?\nclausal_density_df\n\n# A tibble: 2 × 4\n  document           sentences clauses C_per_S\n  &lt;chr&gt;                  &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt;\n1 A69858                   262     451    1.72\n2 Circle of Commerce      1610    2581    1.60\n\n\n3. Subordination (DC/C and DC/S): this measures how much the author of the text uses dependent clauses (subordination). Subordination creates hierarchical, embedded structures. For example, think of this (monster) sentence: “Although the merchants insisted that the shortage was temporary, and because the city council feared that the unrest would spread unless action were taken immediately, the King decided that he would issue new regulations after he had consulted his advisors.”\nIt contains multiple embedded dependent clauses (“Although…”, “that the shortage…”, “because…”, “unless…”, “that he would…”, “after he had…”), demonstrating how subordination creates a layered, hierarchical structure that is characteristic of complex syntax.”\n\nsubordination_df &lt;- sentence_df %&gt;%\ngroup_by(document) %&gt;%\nsummarise(\nclauses = sum(clauses),\ndep_clauses = sum(dep_clauses),\nsentences = n(),\nDC_per_C = dep_clauses / pmax(clauses, 1), #avoid division by 0 (shouldn't happen, but if no clauses were detected in the sentence, we still want it to run)\nDC_per_S = dep_clauses / sentences,\n.groups = \"drop\"\n)\n\nsubordination_df\n\n# A tibble: 2 × 6\n  document           clauses dep_clauses sentences DC_per_C DC_per_S\n  &lt;chr&gt;                &lt;int&gt;       &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 A69858                 451         306       262    0.678     1.17\n2 Circle of Commerce    2581        1795      1610    0.695     1.11\n\n\nWhat do these measures mean?\nDC/C (Dependent Clauses per Clause): What proportion of clauses are dependent? Higher value means that there is more subordination relative to total clauses.\nDC/S (Dependent Clauses per Sentence): How many dependent clauses in each sentence on average? The higher the measure, the more embeddings/subordination per sentence.\n4. Coordination (Coord/C and Coord/S): we are next going to measure how much the author use **coordination** (linking clauses with “and”, “but”, “or”). This is a measure of how “horizontal,” rather than hierarchical subordination.\n\ncoordination_df &lt;- syntax_df %&gt;%\ngroup_by(document) %&gt;%\nsummarise(\ncoord_relations = sum(is_coord),\nclauses         = sum(is_clause),\nsentences       = n_distinct(sentence_id),\nCoord_per_C     = coord_relations / pmax(clauses, 1),\nCoord_per_S     = coord_relations / sentences,\n.groups = \"drop\"\n)\n\ncoordination_df\n\n# A tibble: 2 × 6\n  document           coord_relations clauses sentences Coord_per_C Coord_per_S\n  &lt;chr&gt;                        &lt;int&gt;   &lt;int&gt;     &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 A69858                         691     451       262        1.53        2.64\n2 Circle of Commerce            4071    2581      1611        1.58        2.53\n\n\nThese are “densities” of coordination:\nCoord/C (Coordination per Clause): Total number of coordination (“and”, “or”, “but”…) markers divided by number of clauses.\nCoord/S (Coordination per Sentence): Total number of coordination markers divided by number of sentences.\n5. Phrasal Complexity (CN/C and CN/S): writing increases in complexity not just through clauses, but through elaborated phrases. This is especially common in specialized or technical writing. Noun phrases are phrases built around a noun (or pronoun) that act as a unit in a sentence (if you are rusty on noun phrases aka nominal phrases, check this wikipedia entry).\n\nnominal_df &lt;- syntax_df %&gt;%\ngroup_by(document) %&gt;%\nsummarise(\ncomplex_nominals = sum(is_complex_nominal),\nclauses          = sum(is_clause),\nsentences        = n_distinct(sentence_id),\nCN_per_C         = complex_nominals / pmax(clauses, 1),\nCN_per_S         = complex_nominals / sentences,\n.groups = \"drop\"\n)\n\nnominal_df\n\n# A tibble: 2 × 6\n  document           complex_nominals clauses sentences CN_per_C CN_per_S\n  &lt;chr&gt;                         &lt;int&gt;   &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 A69858                          994     451       262     2.20     3.79\n2 Circle of Commerce             5828    2581      1611     2.26     3.62\n\n\nAs a way to understand the difference between clausal and phrasal complexity, look over this example:\n\nMore clauses: The ship was large and fast and was in port. (Not great construction, but plenty of phrases).\nMore complex phrase: The large fast ship was in port.\n\nLet’s bring everything together:\n\n# Combine all measures\nall_measures &lt;- mls_df %&gt;%  # ← Added mls_df %&gt;%\n  left_join(clausal_density_df %&gt;% select(document, C_per_S), by = \"document\") %&gt;%\n  left_join(subordination_df %&gt;% select(document, DC_per_C, DC_per_S), by = \"document\") %&gt;%\n  left_join(coordination_df %&gt;% select(document, Coord_per_C, Coord_per_S), by = \"document\") %&gt;%\n  left_join(nominal_df %&gt;% select(document, CN_per_C, CN_per_S), by = \"document\")\n\nall_measures %&gt;%\n  knitr::kable(\n    digits = 2,\n    col.names = c(\"Document\", \"MLS\", \"C/S\", \"DC/C\", \"DC/S\", \n                  \"Coord/C\", \"Coord/S\", \"CN/C\", \"CN/S\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocument\nMLS\nC/S\nDC/C\nDC/S\nCoord/C\nCoord/S\nCN/C\nCN/S\n\n\n\n\nA69858\n22.85\n1.72\n0.68\n1.17\n1.53\n2.64\n2.20\n3.79\n\n\nCircle of Commerce\n21.23\n1.60\n0.70\n1.11\n1.58\n2.53\n2.26\n3.62\n\n\n\n\n\nAnd let’s visualize it\n\n# Reshape for plotting\nsyntax_long &lt;- all_measures %&gt;%  # ← Added %&gt;%\n  pivot_longer(\n    cols = -document,\n    names_to = \"Measure\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(\n    Category = case_when(\n      Measure == \"MLS\" ~ \"Sentence Length\",\n      Measure == \"C_per_S\" ~ \"Clausal Density\",\n      Measure %in% c(\"DC_per_C\", \"DC_per_S\") ~ \"Subordination\",\n      Measure %in% c(\"Coord_per_C\", \"Coord_per_S\") ~ \"Coordination\",\n      Measure %in% c(\"CN_per_C\", \"CN_per_S\") ~ \"Phrasal Complexity\"\n    )\n  )\n\n# Plot\nggplot(syntax_long, aes(x = Measure, y = Value, fill = document)) +\n  geom_col(position = \"dodge\", width = 0.7) +\n  facet_wrap(~Category, scales = \"free\", ncol = 2) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Syntactic Complexity: Complete Profile\",\n    subtitle = \"Comparing multiple dimensions of syntactic complexity\",\n    x = NULL,\n    y = \"Value\",\n    fill = \"Document\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"top\"\n  )",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 05: Text Representation (2)"
    ]
  },
  {
    "objectID": "week-03-dictionaries.html",
    "href": "week-03-dictionaries.html",
    "title": "Week 03: Dictionaries",
    "section": "",
    "text": "A Quick Aside: Fixing Spelling with Regex\nWhen looking at the data from week 2, you may have noticed some unusual characters (see, for example, the print out of the tibble “word_counts”). We need to make some decisions on how to handle them. This is where regular expressions come in. Regular expressions, abbreviated to regex, are a way to define patterns within strings. It will be helpful to get familiar with the package stringr (which we have used before without comment) by looking at the documentation here.\n\nlibrary(readr) \nlibrary(dplyr) \nlibrary(stringr) \nlibrary(tibble)  \n\ncircle_raw &lt;- read_file(\"texts/A07594__Circle_of_Commerce.txt\") \n\ntext_tbl &lt;- tibble(   \n  doc_title = \"The Circle of Commerce\",   \n  text = circle_raw \n  )  \n\n# Taking a look at text_tbl (or at least part of it since it's very long):\ntext_tbl %&gt;% select(doc_title)\n\n# A tibble: 1 × 1\n  doc_title             \n  &lt;chr&gt;                 \n1 The Circle of Commerce\n\nnchar(text_tbl$text)\n\n[1] 191605\n\nstr_sub(text_tbl$text, 1, 400)\n\n[1] \"THE CIRCLE OF COMMERCE. The Prooeme. HERODOTVS in his CLIO, reportes that CROESVS King of LYDIA had a ſonne borne dumbe: and his Countrey being invaded, and the King himſelf in imminent danger of death by a certaine Perſian ready to lay violent hands on him; the Kings ſonne affected with the preſent danger, then ſpake that neuer ſpake before, and cryed alowd, O homo ne perimas Patrem! O man kill n\"\n\n\nBefore we change any parts of the text, we are going to preserve the original text in case we change our mind later on. So we are creating a new tibble named text_tbl that contains the original text in a column named text_original:\n\ntext_tbl &lt;- text_tbl %&gt;%\n  mutate(text_original = text)\n\nIn early modern print, the long s (ſ) represents the same letter as the modern s (both were used in print). When it appears explicitly in transcription or OCR, we can safely normalize it to a modern s. In historical texts, regex is often used for normalization, not “correction.” the long s isn’t an error, it’s just a variant (as the British “colour” vs. the American “color” are variants of the same word).\nRegular expressions allow us to:\n\nidentify patterns in text, and\nreplace them systematically.\n\n\n text_tbl &lt;- text_tbl %&gt;%   \n  mutate(     \n    text_clean = str_replace_all(text_original, \"ſ\", \"s\")   \n    )  \n\n\n# Let's count how many \"ſ\" we had before and after to check that the substitution worked:\n\ntibble(\n  long_s_before = str_count(text_tbl$text_original, \"ſ\"),\n  long_s_after  = str_count(text_tbl$text_clean, \"ſ\")\n)\n\n# A tibble: 1 × 2\n  long_s_before long_s_after\n          &lt;int&gt;        &lt;int&gt;\n1          4517            0\n\n\nA bit of warning:\n\nWe replace only the explicit character ſ\nSometimes, the long S is represented as an “f” in transcribed/OCR’ed texts, but I do not recommending trying to guess when an f “should be” an s. Make sure that you know why! If not, ask!\n\nFinally, to pick up another loose thread from Week 2, we can use regex to selectively keep some punctuation and symbols. Remember that we used unnest_tokens to tokenize our text, but we learnt that it automatically removes punctuation. We can set that parameter to FALSE and keep all punctuation. But we can also be more selective. For example, if I only want to keep currency markers (British pounds for our texts), apostrophes, and hyphens, I can do the following:\ntext_tbl &lt;- text_tbl %&gt;%\n  mutate(\n    text_clean = str_replace_all(text_clean, regex(\"[[:punct:]&&[^£'-]]\"), \" \") # note I am writing over \"text_clean\", we could also create a new                                                                                       column instead\n  )\nNB: “^ at the start of a character class in regex means”NOT”. I am removing all punctuation except the list I am defining.\n\nStandardizing Name Spellings:\nEarly Modern spelling was not standardized, meaning that the same word may have been spelled in a number of different ways by the same author within the same document. Name spelling was also not standardized and this fact presents some serious problems for us. While it’s pretty easy to decide that “friend” = “freind” (a peculiar spelling by the poet John Milton) for normalization processes, standardizing names in historical texts is not a neutral preprocessing step. It requires biographical and historical research.\nIf we decide that, for example, Smythe, Smyth, and Smith all refer to the same name, we are making a scholarly claim about identity, authorship, and equivalence across spelling variation. Different research questions may require different choices. Therefore, name standardization rules should always be documented and justified as part of your research.\nHaving said that, how would we go about standardizing the variant of Smith?\nThe first step is to define an explicit standardization map. This step will seem excessive right now, but if you are standardizing a large number of words, this vector will keep track of your decisions and it can be expanded as you go.\n\nname_map &lt;- c(   \n\"Smythe\" = \"Smith\",   \"Smyth\"  = \"Smith\",   \"Smithe\" = \"Smith\" \n)\n\nNow, we do not want to simply “search and replace” based on this map. The name-place “Smythfield” (which I am making up right now) should not be replaced by “Smithfield.” What we want to replace is the specific string “Smyth”. To do this, we have to think about word boundaries and typesetting (we want to catch all versions of the name regardless of case):\n\ntext_standard &lt;- text_tbl %&gt;%\n  mutate(\n    text_norm = text_clean %&gt;%\n      str_replace_all(regex(\"\\\\bSmythe\\\\b\", ignore_case = TRUE), \"Smith\") %&gt;%\n      str_replace_all(regex(\"\\\\bSmyth\\\\b\",  ignore_case = TRUE), \"Smith\") %&gt;%\n      str_replace_all(regex(\"\\\\bSmithe\\\\b\", ignore_case = TRUE), \"Smith\")\n  )\n\nThe code above uses \\\\b to denote the word boundary. This syntax is peculiar to R (in Python is would be \\b instead). The boundary matches the position between: a word character ([A–Z a–z 0–9 _]) AND a non-word character (space, punctuation, start/end of string). So, \\\\bSmithe\\\\b will catch: “Smithe”, “Smithe.”, “Smithe)”, etc. but not Smithefield.\nNote also that I added a new column with mutate, called text_norm. This way, the original column text_clean is not changed.\n\n\n\nDescribing the Text: N-grams and Trade\nLet’s start to explore how The Circle of Commerce talks about “trade.” Last week we focused on single words and on all bigrams. But there are limitations to what word counts and context-free bigrams tell us about a text. What if we wanted to know more than just how often the word “trade” is used? An obvious, first question is to ask how Misselden describes trade in The Circle: is he focusing on “domestic trade” or “foreign trade”? Does the think that “trade is growing” or that “trade is declining”? We can start exploring these kinds of relationship by using n-grams containing the word trade. The first steps of this analysis will be a bit of review.\nI am going to focus on bigrams as it will keep the lesson clean and easy to follow. Let’s start by setting up the needed packages (in addition to the ones we already loaded):\n\nlibrary(tidyr) \nlibrary(tidytext) \nlibrary(ggplot2) \nlibrary(forcats)\n\nBigrams tell us which words tend to appear next to each other in our text. But their raw frequencies can be dominated by turns of phrase (such as “the trade” or “this trade”) that may or may not be helpful in our analysis. So we will have to make some decisions on how to clean the texts and how to interpret our results.\nWe are going to start by tokenizing into bigrams. To do so, we are going to take two words at the time rather than one at the time when we use unnest_tokens.\n\nbigrams_raw &lt;- text_standard %&gt;%\n select(doc_title, text_norm) %&gt;%\n unnest_tokens(output = \"bigram\", input = text_norm, token = \"ngrams\", n = 2)\n\n#let's take a look\n\nbigrams_raw %&gt;% count(bigram, sort = TRUE) %&gt;% slice_head(n = 10)\n\n# A tibble: 10 × 2\n   bigram       n\n   &lt;chr&gt;    &lt;int&gt;\n 1 of the     386\n 2 in the     224\n 3 to the     144\n 4 and the    113\n 5 it is      102\n 6 of trade    91\n 7 for the     90\n 8 to be       80\n 9 of his      77\n10 that the    73\n\n\n\nCleaning the bigrams:\nAs you can see from the list above, we never removed stopwords from text_standard. We are going to do this now. We will have to split up the bigrams into single words and remove stopwords for each “half” of the bigram (see week 2 for more details on cleaning):\n\ndata(\"stop_words\") # the standard list from tidytext, but you can adapt the process from week 2 to include custom stop words\n\nbigrams_clean &lt;- bigrams_raw %&gt;%\nseparate(bigram, into = c(\"word1\", \"word2\"), sep = \" \") %&gt;%\nfilter(!word1 %in% stop_words$word) %&gt;%\nfilter(!word2 %in% stop_words$word) %&gt;%\nfilter(str_detect(word1, \"^[a-z]+$\")) %&gt;% # note: here I am removing ALL punctuation (earlier we kept specific symbols)\nfilter(str_detect(word2, \"^[a-z]+$\"))\n\nbigrams_clean %&gt;% count(word1, word2, sort = TRUE) %&gt;% slice_head(n = 10)\n\n# A tibble: 10 × 3\n   word1     word2           n\n   &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;\n 1 merchants adventurers    35\n 2 low       countries      21\n 3 free      trade          20\n 4 latin     alphabet       17\n 5 natiue    commodities    17\n 6 forraine  commodities    16\n 7 common    wealth         14\n 8 letters   patents        14\n 9 thousand  pounds         14\n10 cloth     trade          10\n\n\nNow we glue the bigrams back together and visualize them using ggplot:\n\nbigram_counts &lt;- bigrams_clean %&gt;%\ncount(word1, word2, sort = TRUE) %&gt;%\nunite(\"bigram\", word1, word2, sep = \" \")\n\nbigram_counts %&gt;%\nslice_head(n = 20) %&gt;%\nmutate(bigram = fct_reorder(bigram, n)) %&gt;%\nggplot(aes(x = n, y = bigram)) +\ngeom_col() +\nlabs(\ntitle = \"Most frequent bigrams (after stopword filtering)\",\nx = \"Count\",\ny = NULL\n)\n\n\n\n\n\n\n\n\n\n\nHoming in on the concept of trade:\nWe are going to work on two straightforward “trade-focused” strategies that will give us a preliminary way to capture the concept of trade through token analysis:\n\nFilter bigrams that literally contain the token trade\nUse a small “trade lexicon” to capture near-synonyms (e.g., traffick, commerce, merchant, exchange)\n\nLet’s start with the first strategy: which bigrams contain the token trade?\n\ntrade_bigrams &lt;- bigram_counts %&gt;%\nfilter(str_detect(bigram, \"\\\\btrade\\\\b\")) # this line should look familiar\n\ntrade_bigrams %&gt;% slice_head(n = 25)\n\n# A tibble: 25 × 2\n   bigram                n\n   &lt;chr&gt;             &lt;int&gt;\n 1 free trade           20\n 2 cloth trade          10\n 3 trade cap             5\n 4 adventurers trade     4\n 5 fishing trade         3\n 6 kingdomes trade       3\n 7 forraine trade        2\n 8 india trade           2\n 9 persia trade          2\n10 trade free            2\n# ℹ 15 more rows\n\ntrade_bigrams %&gt;%\nslice_head(n = 20) %&gt;%\nmutate(bigram = fct_reorder(bigram, n)) %&gt;%\nggplot(aes(x = n, y = bigram)) +\ngeom_col() +\nlabs(\ntitle = \"Bigrams that include the word 'trade'\",\nx = \"Count\",\ny = NULL\n)\n\n\n\n\n\n\n\n\nThis is helpful: it lets us get a first sense of what types of trade might be most important conceptually to Misselden (“free trade” is clearly something that he is concerned with). But what about trade related terms, such as “commodities” or “exchange”? How do their bigrams fit in?\nLet’s create a trade lexicon and check:\n\ntrade_lexicon &lt;- c(\n  \"trade\", \"traffick\", \"traffic\", \"commerce\", \"merchant\", \"merchants\",\n  \"exchange\", \"export\", \"import\", \"commodity\", \"commodities\",\n  \"navigation\", \"shipping\", \"market\", \"markets\"\n)\n\ntrade_theme_bigrams &lt;- bigrams_clean %&gt;%\n  filter(word1 %in% trade_lexicon | word2 %in% trade_lexicon) %&gt;%\n  count(word1, word2, sort = TRUE) %&gt;%\n  unite(\"bigram\", word1, word2, sep = \" \")\n\ntrade_theme_bigrams %&gt;% slice_head(n = 25)\n\n# A tibble: 25 × 2\n   bigram                    n\n   &lt;chr&gt;                 &lt;int&gt;\n 1 merchants adventurers    35\n 2 free trade               20\n 3 natiue commodities       17\n 4 forraine commodities     16\n 5 cloth trade              10\n 6 exchange betweene         8\n 7 commodities exported      5\n 8 commodities imported      5\n 9 low exchange              5\n10 politique exchange        5\n# ℹ 15 more rows\n\ntrade_theme_bigrams %&gt;%\nslice_head(n = 20) %&gt;%\nmutate(bigram = fct_reorder(bigram, n)) %&gt;%\nggplot(aes(x = n, y = bigram)) +\ngeom_col() +\nlabs(\ntitle = \"Trade-theme bigrams (lexicon-based)\",\nx = \"Count\",\ny = NULL\n)\n\n\n\n\n\n\n\n\nWe now see that “natiue commodities” and “forraine commodities” are almost as important as “free trade” in The Circle of Commerce, and “merchants adventurers” is even more prominent.\nWarning: while my lexicon tracks the alternate spelling for “traffic” (= “traffick”), it doesn’t account for other possible spelling variations (such as “native” = “natiue”). In addition, it treats the variations as distinct words, so that “naval traffic” is being counted as a different bigram from “naval traffick.” This is fine (in my opinion) at this exploratory stage, but it’s something that will need addressing in our analysis down the line.\n\nFor your own entertainment: test your understanding of regex by standardizing “traffick” to “traffic” in the text upstream of the bigram analysis.\n\n\n\n\nSentiment Analysis:\nSo far, we have explore the language around trade (and trade-related words), but what about the sentiment of this language? Sentiment analysis is a pretty standard technique in NLP projects, but, as we discussed in class, it doesn’t translate smoothly to specialized and historical discourse. For our purposes, we will want to create a targeted sentiment analysis where we measure sentiment only in passages near “trade” (and related terms), instead of across the whole document. This will give us more granular insight over the documents’ tone around the concept of trade.\nLet’s compare The Circle of Commerce (1623) to the earlier text, Free Trade (1622). We will do this in steps:\n\nDefine a trade keyword set (e.g., trade/commerce/merchant).\nExtract token windows around each keyword occurrence (±30 words). There is no hard and fast rule on the size of the window that you want to select. I settled on 30 as a good estimate/guess based on my experience in reading Early Modern texts.\nCompute sentiment inside those windows only.\nCompare two texts.\n\nLet’s start the usual way:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(ggplot2)\n\ncircle_raw &lt;- read_file(\"texts/A07594__Circle_of_Commerce.txt\")\nfree_raw   &lt;- read_file(\"texts/B14801__Free_Trade.txt\")\n\ntexts_miss &lt;- tibble(\n  doc_title = c(\"Circle of Commerce\", \"Free Trade\"),\n  text = c(circle_raw, free_raw)\n)\n\nWe are going to do some basic normalization:\n\ntexts_miss &lt;- texts_miss %&gt;%\n  mutate(\n    text_norm = text %&gt;%\n      str_replace_all(\"ſ\", \"s\") %&gt;%   # long s as above\n      str_replace_all(\"\\\\s+\", \" \") %&gt;% # collapse whitespace\n      str_to_lower()\n  )\n\nNote: I am adding a line to collapse whitespace because historical texts can have irregular spaces due to archaic typography. We want to remove extraneous whitespaces and replace them with a single whitespace, such as “The circle of commerce” –&gt; “The circle of commerce.” This problem is not unique to historical texts: as you are aware, social media posts don’t always adhere to the best typesetting (or spelling or grammar…) rules!\nImportant: since we are going to want to create a window of 30 words around our target trade terms, we are going to need to keep track of the position of each word in the document. We do this with the index (token_id).\n\ntokens &lt;- texts_miss %&gt;%\n  unnest_tokens(word, text_norm, token = \"words\") %&gt;%\n  group_by(doc_title) %&gt;%\n  mutate(token_id = row_number()) %&gt;%\n  ungroup()\n\nTo make sure that you understand what we are doing above, start by reviewing unnest_tokens from week 2. That should give you a sense of why we want token_id = row_number. Finally, inspect tokens in RStudio–does it look the way you expect it to?\nNext, we are going to identify where our target terms (in this case, I am going to look at “trade”, “commerce”, “merchant”, and “merchants”, but you can play around with these choices) are in the texts:\n\ntrade_terms &lt;- c(\"trade\", \"commerce\", \"merchant\", \"merchants\")\n\ntrade_hits &lt;- tokens %&gt;%\n  filter(word %in% trade_terms) %&gt;%\n  select(doc_title, hit_word = word, hit_token_id = token_id)\n\nNow that we have located the position of the trade terms (trade_hits), we can create the ±30 token window around them:\n\nwindow_size &lt;- 30\n\ntrade_windows &lt;- tokens %&gt;%\n  inner_join(trade_hits, by = \"doc_title\") %&gt;%\n  filter(token_id &gt;= hit_token_id - window_size,\n         token_id &lt;= hit_token_id + window_size) %&gt;%\n  mutate(window_id = paste(doc_title, hit_token_id, sep = \"_\"))\n\nA couple of notes about the code above:\n\nI gave you a link to information about joins last week, go back to it to understand inner_join.\nWhen you run this code, you will get a warning about “an unexpected many-to-many relationship”: this is fine and expected. Our target words appear frequently in the documents (they are, after all, mercantile documents about trade) and so the windows we are creating are overlapping, leading R to warn us about this issue. You can silence the warning if you want.\n\nLet’s take a look at what one window looks like:\n\ntrade_windows %&gt;%\n  filter(window_id == nth(unique(window_id), 10)) %&gt;%\n  summarise(window_text = str_c(word, collapse = \" \")) %&gt;%\n  pull(window_text) %&gt;%\n  cat()\n\nvnfitnesse of my pen to represent such pieces so also had i not the happines to attend those then or these since in any of their assemblies as did other merchants whereby my discourse might haue receiued some life and force from their worth and influence their good acceptation of my poore endeauours together with the approbation of many other noble\n\n\nJust as you expected (right?), this is a 61-token window centered around one of our target words (“merchants” in this case).\nNow we are reading to actually compute sentiments. Tidytext gives us access to a number of sentiment lexicons. We are going to use a binary one, bing (by Bing Liu and his collaborators), that categorizes words into positive or negative. As we discussed at the onset, this is not going to be historically accurate, but it will give us a reasonable baseline.\nWe are going to get into the code details in a second, but let’s start with the reasoning behind this approach:\n\nWe are going to look at all the 61-token windows around “trade”, “merchant”, etc.\nMatch words to the sentiment dictionary\nCount: how many positive words vs negative words in each window?\nCalculate a net score: positive count minus negative count [so, a window with 5 positive, 2 negative → net = +3 (overall positive)].\n\nThe code looks like this:\n\nbing &lt;- get_sentiments(\"bing\")\n\nwindow_sentiment &lt;- trade_windows %&gt;%\n  inner_join(bing, by = \"word\") %&gt;%  # keeps only sentiment-bearing words\n  count(doc_title, window_id, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(net_sentiment = positive - negative)\n\nNext, we create an overall summary of sentiments for each text. First:\n\nWe are going to find all occurrences of trade-related tokens and create a fixed size “window” around each term.\nWe then inner_join those tokens to the Bing sentiment lexicon so that each token can be labeled as either positive or negative. In this step, if a token doesn’t appear in the Bing lexicon, it gets dropped (a window that contains no Bing words will have no rows in window_sentiment). This means that n() is counting rows of window_sentiment. That is, n() counts the number of word-tokens matched in Bing, aggregated by doc_title.\n\nThen, from window_sentiment, we compute:\n\nThe number of positive and negative Bing-matched token occurrences (so, if “good” appears five times, it contributes five positive occurrences) in our trade-related window.\nThe total net sentiment (= positive - negative)\nThe average net sentiment per sentiment-bearing window = net_sentiment / n_distinct(window_id) . That is, the denominator is the number of windows with at least one Bing match.\n\n\ntext_sentiment_summary &lt;- window_sentiment %&gt;%\n  group_by(doc_title) %&gt;%\n  summarise(\n    windows = n(),\n    total_positive = sum(positive),\n    total_negative = sum(negative),\n    total_net_sentiment = sum(net_sentiment),      \n    avg_net_per_window = mean(net_sentiment),\n    .groups = \"drop\"\n  )\ntext_sentiment_summary\n\n# A tibble: 2 × 6\n  doc_title          windows total_positive total_negative total_net_sentiment\n  &lt;chr&gt;                &lt;int&gt;          &lt;int&gt;          &lt;int&gt;               &lt;int&gt;\n1 Circle of Commerce     338            556            327                 229\n2 Free Trade             249            404            310                  94\n# ℹ 1 more variable: avg_net_per_window &lt;dbl&gt;\n\n\nBased on this summary, we can start to see that The Circle of Commerce uses more positive language around the trade-related terms that we selected. To get a better handle of this, we are going to plot the distribution of sentiment scores across all the trade windows for each document as a bar graph:\n\nggplot(window_sentiment, aes(x = net_sentiment)) +\n  geom_histogram(binwidth = 1) +\n  facet_wrap(~ doc_title, ncol = 1) +\n  labs(\n    title = \"Sentiment in Trade-Centered Windows (±30 words)\",\n    x = \"Net sentiment (positive - negative) per window\",\n    y = \"Number of trade windows\"\n  )\n\n\n\n\n\n\n\n\n\n\nAddendum about Sentiments:\nWe want to be careful in how we interpret sentiment analysis. What we are computing above is a score based on the common (standard?) usage of terms such as “wonderful” or “happy” or “depressing”, where we label certain terms as expressing either positive or negative sentiments. These are of course deeply cultural questions. We all know the stereotypes of Italians being effusive while the British are reserved (as an Italian I do not in the least approve of this absolutely terrible and unfair statement)–does a “great!” from two different speakers mean the same level of positive approval?\nThe past is culturally different to the present and we have to be very careful with sentiment analysis with historical texts. The correct solution would be to create our own, historically based lexicon by becoming familiar with the writing style and linguistic nuances of the 17th century and then scoring terminology (or better yet passages and phrases) for positive or negative sentiments. This is obviously labor intensive. But competent text analysis on any specialized language (historical, medical, legal, etc) requires expertise and care.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 03: Dictionaries"
    ]
  },
  {
    "objectID": "week-01-introduction.html",
    "href": "week-01-introduction.html",
    "title": "Week 01: Introduction",
    "section": "",
    "text": "Welcome to IDS 570, “Text as Data.” For each week of the course, you will find code and notes to match the in-class lecture. For the first few weeks of the semester, we will be working in R. As the term goes on, we will introduce Python concepts and methodologies.\nThis repository is a work in progress: I will update it (and the pull-down menu will grow) as the semester progresses and as I add or subtract materials for the course.\n\n\nThis site is generated from Quarto source files and updated regularly–expect things to change and if you notice a typo, please let me or the TA’s know. Code examples are illustrative and meant to be adapted.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 01: Introduction"
    ]
  },
  {
    "objectID": "week-01-introduction.html#welcome",
    "href": "week-01-introduction.html#welcome",
    "title": "Week 01: Introduction",
    "section": "",
    "text": "Welcome to IDS 570, “Text as Data.” For each week of the course, you will find code and notes to match the in-class lecture. For the first few weeks of the semester, we will be working in R. As the term goes on, we will introduce Python concepts and methodologies.\nThis repository is a work in progress: I will update it (and the pull-down menu will grow) as the semester progresses and as I add or subtract materials for the course.\n\n\nThis site is generated from Quarto source files and updated regularly–expect things to change and if you notice a typo, please let me or the TA’s know. Code examples are illustrative and meant to be adapted.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 01: Introduction"
    ]
  },
  {
    "objectID": "week-01-introduction.html#instruction-for-this-week",
    "href": "week-01-introduction.html#instruction-for-this-week",
    "title": "Week 01: Introduction",
    "section": "Instruction for this week:",
    "text": "Instruction for this week:\nFor each week of the course, you will find code and notes to match the in-class lecture. For this week: you need to become familiar with RStudio and learn how to organize directories and projects. You can find a friendly introduction to R at https://rladiessydney.org/courses/01-basicbasics-0. Complete the three BasicBasics lessons. The goal is to feel comfortable with RStudio, installing packages, and reading data into RStudio.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 01: Introduction"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”",
    "section": "",
    "text": "With the recent explosion in availability of digitized historical and literary texts and the availability of powerful language models, researchers are increasingly turning to computational tools for the analysis of text as data. But not all text is equally amenable to computational approaches. Historical texts often require specialized approaches to bridge the gap between the books as originally produced and analysis-ready data. In this course, students will learn to prepare and analyze historical and literary texts for natural language processing. We will also consider questions of interpretation and the ethics of corpus construction.Our corpora will derive from economic documents and travel narratives from the 17th century. These texts are challenging and require careful curation and cleaning, and they will force you to learn meticulous practices and work-flows in text analysis. Whether working with contemporary or historical texts, most textual data is messy and requires careful preprocessing. By focusing on these Early Modern documents, you will learn how to approach quantitative text analysis with qualitative study of the cultural, economic, and political context that produced the data you are analyzing.",
    "crumbs": [
      "Home",
      "Course",
      "IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”",
    "section": "",
    "text": "With the recent explosion in availability of digitized historical and literary texts and the availability of powerful language models, researchers are increasingly turning to computational tools for the analysis of text as data. But not all text is equally amenable to computational approaches. Historical texts often require specialized approaches to bridge the gap between the books as originally produced and analysis-ready data. In this course, students will learn to prepare and analyze historical and literary texts for natural language processing. We will also consider questions of interpretation and the ethics of corpus construction.Our corpora will derive from economic documents and travel narratives from the 17th century. These texts are challenging and require careful curation and cleaning, and they will force you to learn meticulous practices and work-flows in text analysis. Whether working with contemporary or historical texts, most textual data is messy and requires careful preprocessing. By focusing on these Early Modern documents, you will learn how to approach quantitative text analysis with qualitative study of the cultural, economic, and political context that produced the data you are analyzing.",
    "crumbs": [
      "Home",
      "Course",
      "IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”"
    ]
  },
  {
    "objectID": "week-02-basics.html#weeks-goals",
    "href": "week-02-basics.html#weeks-goals",
    "title": "Week 02: Basics",
    "section": "Week’s Goals:",
    "text": "Week’s Goals:\nThis week, we compare two early modern economic texts by examining word frequencies and bigrams. Our two texts are by Edward Misselden, one of the major Early Modern mercantilists whose work we will be analyzing. The goal is for you to become familiar with how to start implementing NLP workflows in RStudio. The two texts are available on Canvas, under files in the folder named Text Files (note: there are other files in there which we will use later on in the term).\nNote: this will be the standard set up going forward. Sample code will be here and files will be on Canvas, unless otherwise specified.\n\nOur guiding questions will be: how does Misselden’s language about trade change between 1622 and 1623? This is obviously an artificially simplistic question at this stage, but it will help us explore some useful NLP methods. To begin answering this question, we will compare word frequency between two of his works: Free Trade (London, 1622) and The Circle of Commerce (London, 1623). We will also look at bigram frequencies as a preview of the longer discussion of N-grams next week.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#what-you-need",
    "href": "week-02-basics.html#what-you-need",
    "title": "Week 02: Basics",
    "section": "What you need:",
    "text": "What you need:\nI am assuming that you will be working inside an RStudio Project (recommended). We will read two plain-text files, tidy the tokens/bigrams, and compare frequency patterns. You will need:\n\nA project folder that contains all of your files (try to have a consistent folder organization throughout the semester).\nThe plain-text files on Canvas.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#a-word-of-warning-about-training-wheels",
    "href": "week-02-basics.html#a-word-of-warning-about-training-wheels",
    "title": "Week 02: Basics",
    "section": "A word of warning about training wheels:",
    "text": "A word of warning about training wheels:\nBecause this class doesn’t assume familiarity with R, I am giving extended explanations of the code during this first session (including tips on how to set up your directories and extensive links to R documentation). As the semester proceeds, I will assume that you are gaining confidence and familiarity with R and Rstudio, and the training wheels will slowly come off. Take advantage of the slower pace at the start of the semester to set yourself up for the harder weeks to come!\nThis week I will also show the results of each step of code by printing after each step. This will make (a bit more) explicit how each chunk of code modifies our data. You will be expected to do this more and more on your own starting next week.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#step-by-step-from-raw-texts-to-comparison-plots",
    "href": "week-02-basics.html#step-by-step-from-raw-texts-to-comparison-plots",
    "title": "Week 02: Basics",
    "section": "Step-by-step: from raw texts to comparison plots",
    "text": "Step-by-step: from raw texts to comparison plots",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#setup",
    "href": "week-02-basics.html#setup",
    "title": "Week 02: Basics",
    "section": "Setup",
    "text": "Setup\nAt the beginning of each R file, you will want to call all the packages needed. This will look like:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(tibble)\nlibrary(scales)\n\nI am storing my text files in a directory named texts/ inside my project: I strongly suggest doing the same to make your life easier.\nThe next section of code demonstrates how to read the files and to organize them into a type of dataframe that is well suited for Tidy work: a tibble. For this week, I am using generic “file_a” and “file_b” names to simplify the rather complex original file names. We will discuss the naming conventions of these files as well as how to deal with XML files (the original format of Misselden’s texts) in the next few weeks, but for now, I want you to focus on getting them into the appropriate format for analysis.\n\n# You will need the correct file paths if you don't follow my naming conventions:\nfile_a &lt;- \"texts/A07594__Circle_of_Commerce.txt\"\nfile_b &lt;- \"texts/B14801__Free_Trade.txt\"\n\n# Read the raw text files into R\ntext_a &lt;- read_file(file_a)\ntext_b &lt;- read_file(file_b)\n\n# Combine into a tibble for tidytext workflows\ntexts &lt;- tibble(\n  doc_title = c(\"Text A\", \"Text B\"),\n  text = c(text_a, text_b)\n)\n\ntexts\n\n# A tibble: 2 × 2\n  doc_title text                                                                \n  &lt;chr&gt;     &lt;chr&gt;                                                               \n1 Text A    THE CIRCLE OF COMMERCE. The Prooeme. HERODOTVS in his CLIO, reporte…\n2 Text B    CAP. I. The Causes of the want of Money in England. IT hauing pleas…\n\n\nAs you can see from the .txt files, these are Early Modern texts with (at least some) of the original orthography. In reality, these have already been partially cleaned as I have accessed them from EarlyPrint (we will discuss different sources of texts, historical and otherwise) as you start thinking about your term project.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#tokenization-stopwords-and-counting-words",
    "href": "week-02-basics.html#tokenization-stopwords-and-counting-words",
    "title": "Week 02: Basics",
    "section": "Tokenization, stopwords, and counting words:",
    "text": "Tokenization, stopwords, and counting words:\nWe will turn each text into a table of one word per row, this is the tokenization process in Tidy. Tokens are the basic unit of text that we are working on and the process of tokenization “breaks down” the text into these units for computational analysis. For right now, we are going to consider each word to be a token, but we could break up the text into characters or parts of words if needed. We will then we remove stopwords (words like “the” or “and” that are not useful for this analysis) so that the remaining words are more meaningful for comparison.\nWe will do this in two steps:\n\nStopwords: we will create an “all stopwords” list by combining tidytext’s built-in list with our own, corpus-specific, list. Our list is going to be a one-column tibble to match the format of the built-in stopwords:\n\n\n# Start with tidytext's built-in stopword list\ndata(\"stop_words\")\n\n# Add our own project-specific stopwords (you can, and will, expand this list later)\ncustom_stopwords &lt;- tibble(\n  word = c(\n    \"vnto\", \"haue\", \"doo\", \"hath\", \"bee\", \"ye\", \"thee\"\n  )\n)\n\nall_stopwords &lt;- bind_rows(stop_words, custom_stopwords) %&gt;%\n  distinct(word)\n\nall_stopwords %&gt;% slice(1:10)\n\n# A tibble: 10 × 1\n   word       \n   &lt;chr&gt;      \n 1 a          \n 2 a's        \n 3 able       \n 4 about      \n 5 above      \n 6 according  \n 7 accordingly\n 8 across     \n 9 actually   \n10 after      \n\n\nNext we are going to create a function that tokenizes the texts, removes both standard and custom stopwords, and then counts which words appear most frequently in each document. As you look over this section, you will want to become familiar with the pipe in R (%&gt;%).\n\ntexts is a tibble where each row corresponds to a document;\nunnest_tokens(word, text): splits the text column into individual word tokens, creates a new column called word, and expands the tibble so each row is now one word occurrence\nmutate: here we are normalizing all words to lower case. This is a research decision: I want to count words like “Money,” “MONEY,” and “money” as the same token, so that I can ask how often the texts talk about “money” regardless of capitalization. Note: mutate is part of the dplyr package.\nanti_join: this is where we remove the stopwords we defined earlier and keeps only the tokens that should (we think!) give us the information we want. Look over the information on joins here and make sure that you understand what it’s doing and understand why the tibble format is important! We will be using anti_join frequently.\nFinally we count.\n\n\nword_counts &lt;- texts %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  mutate(word = str_to_lower(word)) %&gt;%\n  anti_join(all_stopwords, by = \"word\") %&gt;%\n  count(doc_title, word, sort = TRUE)\n\nword_counts\n\n# A tibble: 7,438 × 3\n   doc_title word            n\n   &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;\n 1 Text A    trade         233\n 2 Text A    exchange      186\n 3 Text B    trade         185\n 4 Text A    ſo            184\n 5 Text A    malynes       158\n 6 Text A    merchants     126\n 7 Text A    mony          118\n 8 Text A    hee           115\n 9 Text A    kingdome      113\n10 Text A    commodities    96\n# ℹ 7,428 more rows\n\n\nNote: unnest_tokes(word, text) removes punctuation and lowercases automatically. You can a try couple of options to solve this issue. The first is to change a couple of parameters in the function: unnest_tokens(word, text, strip_punct = FALSE) to keep the punctuation and unnest_tokens(word, text, to_lower = FALSE) to keep upper cases. What if you want to keep only certain symbols and types of punctuation? We will discuss this at the beginning of Week 3.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#comparing-word-frequencies-across-texts",
    "href": "week-02-basics.html#comparing-word-frequencies-across-texts",
    "title": "Week 02: Basics",
    "section": "Comparing word frequencies across texts",
    "text": "Comparing word frequencies across texts\nLet’s compare how individual words differ across texts after stopword removal through a first visualization. To keep the visualization readable, we focus on the top 20 most frequent words overall. For now, we won’t get into all the details of the visualization code; we will focus on the main components.\nThe goal of this visualization is a side-by-side comparison plot showing the top 20 most frequent words across the texts with one facet per text. Note: these are the same words for both texts. This means that we are creating a direct comparison of how Misselden uses the most frequent terms.\n\nWe start by putting word_count in the correct format with pivot_wider: each word will have a column for each document’s count.\nvalues_fill = 0: if a word never appears in a text, it gets a 0 instead of NA.\nNext, we rank words based on this criterion: for each word, look at how often it appears in Text A and how often it appears in Text B, and keep the largest count.\n\nThis is what is achieved by computing the row-wise maximum with pmax and then defining the vector: max_n = pmax(TextA, TextB).\nmutate() adds a column to word_comparison_tbl (the wide version of word_count we are defining in this block) based on how we just defined max_n (that is: it takes the vector and attaches it as a column to the tibble).\narrange(desc(max_n) sorts the words from most to least frequent based on max_n.\n\n\n\nStop and regroup:\nI have just implicitly (sneakily) introduced some new things and you might find it helpful to take a look at this introduction to data structures. We will reinforce this topic as we go so don’t panic if this is new to you!\n\n\nBack to the code:\nWe have produced a wide, ranked comparison table. We want to turn into a tidy table that can easily be plotted. To do this we need three main steps:\n\nselect the top 20 words to plot (based on our ranking criterion above). We will do this with slice_head(n = plot_n_words), which takes the first plot_n_words of word_comparison_tbl.\nreshape from wide to long to please ggplot: we will talk about ggplot more in the future, but, for now, we can treat it as a blackbox that produces nice visualizations.\norder the words for the plot with mutate(word = fct_reorder(word, n, .fun = max)). Note: here too, I am asking you to take this as a blackbox for now. What this step is trying to solve is the problem (for us) that a character vector has no inherent order.\n\nFinally we plot!\n\nplot_n_words &lt;- 20  # you can change this as needed\n\n# Select the most frequent words overall\nword_comparison_tbl &lt;- word_counts %&gt;%\n  pivot_wider(\n    names_from = doc_title,\n    values_from = n,\n    values_fill = 0\n  ) %&gt;%\n  mutate(max_n = pmax(`Text A`, `Text B`)) %&gt;%\n  arrange(desc(max_n))\n\nword_plot_data &lt;- word_comparison_tbl %&gt;%\n  slice_head(n = plot_n_words) %&gt;%\n  pivot_longer(\n    cols = c(`Text A`, `Text B`),\n    names_to = \"doc_title\",\n    values_to = \"n\"\n  ) %&gt;%\n  mutate(word = fct_reorder(word, n, .fun = max))\n\nggplot(word_plot_data, aes(x = n, y = word)) + #black magic happens thanks to ggplot\n  geom_col() +\n  facet_wrap(~ doc_title, scales = \"free_x\") +\n  labs(\n    title = \"Most frequent words (stopwords removed)\",\n    subtitle = paste0(\n      \"Top \", plot_n_words,\n      \" words by maximum frequency across both texts\"\n    ),\n    x = \"Word frequency\",\n    y = NULL\n  ) +\n  theme_minimal()",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#bigrams-starting-to-think-about-context",
    "href": "week-02-basics.html#bigrams-starting-to-think-about-context",
    "title": "Week 02: Basics",
    "section": "Bigrams: starting to think about context",
    "text": "Bigrams: starting to think about context\nSingle-word frequencies tell us which terms are common, but they don’t tell us about the context of these words. Our next step will be to try to get a first, basic understanding of how words in our texts fit together. Bigrams allow us to see which words appear together, capturing short phrases and recurring ideas. This is also our first step towards exploring formulaic language,and discursive patterns, rather than just isolated vocabulary.\n\nYou should be able to understand the syntax of this first step (make sure that you do!):\n\n\nbigrams &lt;- texts %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\nbigrams\n\n# A tibble: 53,816 × 2\n   doc_title bigram           \n   &lt;chr&gt;     &lt;chr&gt;            \n 1 Text A    the circle       \n 2 Text A    circle of        \n 3 Text A    of commerce      \n 4 Text A    commerce the     \n 5 Text A    the prooeme      \n 6 Text A    prooeme herodotvs\n 7 Text A    herodotvs in     \n 8 Text A    in his           \n 9 Text A    his clio         \n10 Text A    clio reportes    \n# ℹ 53,806 more rows\n\n\nCurrently, each bigram is stored as a single string. We want to remove stopwords (using the custom list we created earlier). In order to do that, we need to be able to inspect each word separately in the bigram. separate() does just that! It takes the bigram column and splits each string at the space character. It then creates two new columns: “word1” and “word2.”\n\nbigrams_separated &lt;- bigrams %&gt;%\n  separate(bigram, into = c(\"word1\", \"word2\"), sep = \" \")\n\nbigrams_separated\n\n# A tibble: 53,816 × 3\n   doc_title word1     word2    \n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    \n 1 Text A    the       circle   \n 2 Text A    circle    of       \n 3 Text A    of        commerce \n 4 Text A    commerce  the      \n 5 Text A    the       prooeme  \n 6 Text A    prooeme   herodotvs\n 7 Text A    herodotvs in       \n 8 Text A    in        his      \n 9 Text A    his       clio     \n10 Text A    clio      reportes \n# ℹ 53,806 more rows\n\n\n\nNow we can remove all_stopwords\n\n\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(\n    !word1 %in% all_stopwords$word,\n    !word2 %in% all_stopwords$word\n  )\n\nbigrams_filtered\n\n# A tibble: 7,625 × 3\n   doc_title word1    word2    \n   &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;    \n 1 Text A    prooeme  herodotvs\n 2 Text A    clio     reportes \n 3 Text A    croesvs  king     \n 4 Text A    ſonne    borne    \n 5 Text A    borne    dumbe    \n 6 Text A    king     himſelf  \n 7 Text A    imminent danger   \n 8 Text A    certaine perſian  \n 9 Text A    perſian  ready    \n10 Text A    lay      violent  \n# ℹ 7,615 more rows\n\n\nWe remove bigrams where either word is a stopword, since phrases like “of the” or “and the” are rarely meaningful analytically.\n\nbigram_counts &lt;- bigrams_filtered %&gt;%\n  count(doc_title, word1, word2, sort = TRUE)\n\nbigram_counts\n\n# A tibble: 6,447 × 4\n   doc_title word1     word2           n\n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;\n 1 Text B    common    wealth         48\n 2 Text A    merchants adventurers    35\n 3 Text B    latin     alphabet       34\n 4 Text A    low       countries      21\n 5 Text A    free      trade          20\n 6 Text B    east      india          20\n 7 Text A    latin     alphabet       17\n 8 Text A    natiue    commodities    17\n 9 Text A    forraine  commodities    16\n10 Text A    common    wealth         14\n# ℹ 6,437 more rows\n\n\nNow that we removed the stopwords, we can use unite to put the bigrams back together:\n\nbigram_counts &lt;- bigram_counts %&gt;%\n  unite(bigram, word1, word2, sep = \" \")\n\nbigram_counts\n\n# A tibble: 6,447 × 3\n   doc_title bigram                    n\n   &lt;chr&gt;     &lt;chr&gt;                 &lt;int&gt;\n 1 Text B    common wealth            48\n 2 Text A    merchants adventurers    35\n 3 Text B    latin alphabet           34\n 4 Text A    low countries            21\n 5 Text A    free trade               20\n 6 Text B    east india               20\n 7 Text A    latin alphabet           17\n 8 Text A    natiue commodities       17\n 9 Text A    forraine commodities     16\n10 Text A    common wealth            14\n# ℹ 6,437 more rows\n\n\nImportant conceptual point: why are we creating bigrams first, cleaning them from stopwords, and then gluing them back together? Shouldn’t we just tokenize the text into individual words, clean the stopwords, and then find the bigrams?\nLet’s test this out with a phrase such as: “strong economy of international trade.” If we do bigrams first, we get: “strong economy”, “economy of”, “of international”, “international trade.” Removing stopwords will give us: “strong economy”, “international trade” (make sure this makes sense to you).\nIf we cleaned the stopwords first, the phrase would turn into: “strong economy international trade.” The bigrams would then be: “strong economy”, “economy international”, “international trade.” We now have an “extra” bigram, “economy international” (can you see why?).\nNB: there will be situations where the second method (clean first, bigrams second) may be the better option! Think of the phrase: “the state of the art.” The clean first method will give us “state art,” while our method (bigrams first, clean second) won’t capture this at all. The bigram “state art” is a distortion of the concept “the state of the art”–which method is better depends on the question you are asking, but you need to be aware that the choices you make have downstream effects.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-02-basics.html#comparing-bigrams",
    "href": "week-02-basics.html#comparing-bigrams",
    "title": "Week 02: Basics",
    "section": "Comparing bigrams:",
    "text": "Comparing bigrams:\nSo far, we have looked at the most frequent bigrams within each text. But frequency alone does not tell us what is distinctive about a text. Remember: we started by (somewhat artificially) asking how the language of the two texts differ. If we compare how often the same bigrams appear across texts, we should be able to start seeing some differences between them.\n\nSince the two texts by Misselden differ in length and in number of bigrams, we want to normalize by the number of bigrams within each document to compare the two.\nWe then reshape the data for comparison: in tidy, we want one row per bigram so that we can easily compare across the two text.\n\n\nbigram_relative &lt;- bigram_counts %&gt;%\n  group_by(doc_title) %&gt;%\n  mutate(\n    total_bigrams = sum(n),\n    proportion = n / total_bigrams\n  ) %&gt;%\n  ungroup()\n\nbigram_wide &lt;- bigram_relative %&gt;%\n  select(doc_title, bigram, proportion) %&gt;%\n  pivot_wider(\n    names_from = doc_title,\n    values_from = proportion,\n    values_fill = 0\n  )\n\nbigram_wide\n\n# A tibble: 6,377 × 3\n   bigram                `Text B` `Text A`\n   &lt;chr&gt;                    &lt;dbl&gt;    &lt;dbl&gt;\n 1 common wealth         0.0187    0.00277\n 2 merchants adventurers 0         0.00692\n 3 latin alphabet        0.0133    0.00336\n 4 low countries         0.00156   0.00415\n 5 free trade            0         0.00395\n 6 east india            0.00779   0      \n 7 natiue commodities    0.00273   0.00336\n 8 forraine commodities  0         0.00316\n 9 letters patents       0.000390  0.00277\n10 thouſand pounds       0         0.00277\n# ℹ 6,367 more rows\n\n\nNext, we are going to contrast the bigrams in the two texts by identifying which biagrams are most likely to distinguish one text from the other. Note: this is not a statistical test, we are just exploring the differences in the bigrams between the two texts. To recap, up to this point, we have:\n\nextracted bigrams\nfiltered them\nnormalized them within each document\n\nWe now want to know which bigram is most characteristic of “Text A” (The Circle of Commerce) relative to “Text B” (Free Trade).\n\nbigram_diff &lt;- bigram_wide %&gt;%\n  mutate(\n    diff = `Text A` - `Text B`\n  ) %&gt;%\n  arrange(desc(abs(diff)))\n\nbigram_diff %&gt;% slice(1:20)\n\n# A tibble: 20 × 4\n   bigram                `Text B` `Text A`     diff\n   &lt;chr&gt;                    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 common wealth         0.0187    0.00277 -0.0159 \n 2 latin alphabet        0.0133    0.00336 -0.00989\n 3 east india            0.00779   0       -0.00779\n 4 merchants adventurers 0         0.00692  0.00692\n 5 free trade            0         0.00395  0.00395\n 6 merchants aduenturers 0.00390   0       -0.00390\n 7 forraine commodities  0         0.00316  0.00316\n 8 thouſand pounds       0         0.00277  0.00277\n 9 low countries         0.00156   0.00415  0.00259\n10 letters patents       0.000390  0.00277  0.00238\n11 publique vtility      0.00234   0       -0.00234\n12 cloth trade           0.00429   0.00198 -0.00231\n13 33 ſh                 0         0.00217  0.00217\n14 20 ſhillings          0         0.00198  0.00198\n15 ſh 4                  0         0.00198  0.00198\n16 disorderly trade      0.00195   0       -0.00195\n17 fishing vpon          0.00195   0       -0.00195\n18 india stocke          0.00195   0       -0.00195\n19 kings honour          0.00195   0       -0.00195\n20 maiesties subiects    0.00195   0       -0.00195\n\n\nWhat does diff tell us? If diff &gt; 0, then the bigram is more prominent in Text A; if diff &lt;0, in Text B. If diff is approximately 0, then it is used more or less in similar proportion in the two texts.\n\nQuestions:\n\nHow do you interpret the results above? What do they tell you?\nThere is a problem with the list above due to spelling inconsistencies in the period (as well as the fact that we ignored numbers). How do you think this is affecting our results? Next week, you will learn a way to fix this.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 02: Basics"
    ]
  },
  {
    "objectID": "week-04-representation.html",
    "href": "week-04-representation.html",
    "title": "Week 04: Text Representation (1)",
    "section": "",
    "text": "Document-Feature Matrices\nIn Week 3, we began to compare two of Misselden’s documents through sentiment analysis around trade terminology. In class on Wednesday, we discussed measures of similarity between texts and what a Document-Feature Matrix (DFM) looks like. Let’s implement what we learned in class by comparing the two Misselden’s texts to “mystery” text A06785.txt.\nTo create a DFM of the three texts, we will use the quanteda package. We start by loading the packages, reading the texts, and combining into a quanteda corpus named corp.\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\n\ntxt_circle    &lt;- read_file(\"texts/A07594__Circle_of_Commerce.txt\")\ntxt_free      &lt;- read_file(\"texts/B14801__Free_Trade.txt\")\ntxt_third     &lt;- read_file(\"texts/A06785.txt\")\n\ntexts &lt;- c(\n  \"Circle_of_Commerce\" = txt_circle,\n  \"Free_Trade\"         = txt_free,\n  \"Third_Text_A06785\"  = txt_third\n)\n\ncorp &lt;- corpus(texts)\n\nWe then tokenize and do some basic cleaning (most of this should be very familiar by now):\n\ntoks &lt;- tokens(\n  corp,\n  remove_punct   = TRUE,\n  remove_numbers = TRUE,\n  remove_symbols = TRUE\n)\n\ntoks &lt;- tokens_tolower(toks)\n\ncustom_stop &lt;- c(\n  \"vnto\",\"haue\",\"doo\",\"hath\",\"bee\",\"ye\",\"thee\",\"hee\",\"shall\",\"hast\",\"doe\",\n  \"beene\",\"thereof\",\"thus\" \n)\n\ntoks &lt;- tokens_remove(toks, pattern = c(stopwords(\"en\"), custom_stop))\n\nThe choices that we made when tokenizing do matter! Note that I doing the following:\n\nI am removing punctuation. With this decision, I am asserting that for this analysis, punctuation doesn’t matter (so I want to treat “commerce.” = “commerce!” = “commerce”). You may disagree! If were were doing an analysis of style, then punctuation would matter a lot.\nI am removing numbers. Since these are mercantile texts (including the “mystery” one), it may seems weird to remove potential economic data. My choice is based on a concern for distracting or inaccurate numbers (such as pagination or rounded guesses about population and trade estimates). Again, you may disagree and want to keep them (that’s an easy fix in the code above).\nI am removing symbols. This goes hand-in-hand with the choice about numbers. Most symbols are going to be transcription/OCR errors. The only (likely) valuable ones are currency markers. Since I opted to remove numbers, I decided to remove symbols.\n\nAs I have taken pains to demonstrate, these are all decisions that you can change. But you have to have a principled reason to do so!\nNow we can build the DFM using quanteda’s dfm() and inspect it to make sure that it looks reasonable:\n\n# Document-feature matrix (DFM)\ndfm_mat &lt;- dfm(toks)\n\n# Inspect by raw count (in our corpus) the top 25 features \ndfm_mat\n\nDocument-feature matrix of: 3 documents, 9,083 features (56.93% sparse) and 0 docvars.\n                    features\ndocs                 circle commerce prooeme herodotvs clio reportes croesvs\n  Circle_of_Commerce     14       25       1         1    2        1       1\n  Free_Trade              1       22       0         0    0        0       0\n  Third_Text_A06785      41       31       0         0    0        0       0\n                    features\ndocs                 king lydia ſonne\n  Circle_of_Commerce   35     1     5\n  Free_Trade           53     0     0\n  Third_Text_A06785    36     0     0\n[ reached max_nfeat ... 9,073 more features ]\n\ntopfeatures(dfm_mat, 25)\n\n      trade    exchange commodities       money   merchants         may \n        602         478         293         290         282         272 \n        one      moneys       great    kingdome         now        made \n        268         267         214         188         187         186 \n       said          ſo        much           p     malynes       cloth \n        186         184         183         170         162         162 \n      value     without        time       price        upon         man \n        150         148         147         146         145         142 \n    whereby \n        133 \n\n\nOK, so far so good. This is a sparse matrix and the top features seem quite plausible. We could also use this as a way to (potentially) remove some more stop words (I am not sure that “one” or “now” are giving us much information). I am going to keep things as they are. In the aggregate, stopwords won’t be a problem for what we are doing today (this is an empirical statement based on experience; I present no evidence for this here), and we are going to have a better way to get rid of overly common terms in the next section.\n\nFor your entertainment: try playing around with stopwords and difference choices in the tokenization step.\n\nTo compare the documents, we are going to use the second new library we introduced today: quanteda.textstats. It will gives us a number of ways to compare our three texts. We are going to try two of them and compare the results.\n\nFirst:\nLet’s start with a correlation measure. We are going to treat each document as a word frequency vector. So, if we have N words in our corpus, each document is represented by an N-dimensional vector based on the frequency of each word within that text. The vectors are simply the rows of the DFM (where each row is a document).\nWhat we are going to measure in this step is the pairwise Pearson correlation between the vectors across all the word features (the columns of the DFM).\n\n# correlation similarity\nsim_cor &lt;- textstat_simil(\n  dfm_mat,\n  method = \"correlation\",\n  margin = \"documents\"\n)\nsim_cor\n\ntextstat_simil object; method = \"correlation\"\n                   Circle_of_Commerce Free_Trade Third_Text_A06785\nCircle_of_Commerce              1.000      0.569             0.505\nFree_Trade                      0.569      1.000             0.441\nThird_Text_A06785               0.505      0.441             1.000\n\n\n\n\nSecond:\nNow that we understand how each document is represented at a vector, we can measure the distance between them as vectors by measuring the angle between them. We do this using cosine similarity (as discussed in class). We just have to change the method in textstat_simil().\n\nsim_cos &lt;- textstat_simil(\n  dfm_mat,\n  method = \"cosine\",\n  margin = \"documents\"\n)\n\nsim_cos\n\ntextstat_simil object; method = \"cosine\"\n                   Circle_of_Commerce Free_Trade Third_Text_A06785\nCircle_of_Commerce              1.000      0.596             0.533\nFree_Trade                      0.596      1.000             0.471\nThird_Text_A06785               0.533      0.471             1.000\n\n\nIn both cases, we (reassuringly) got 1.000 down the diagonal (each text is perfectly similar to itself) and the matrices are symmetric (can you see why this should be the case?). We also notice that the two texts by Misselden are more similar to each other than to the “mystery” text in both measures. This is also good news since the third text is by a different author.\nThe third text is Gerard Malynes’s The Center of the Circle of Commerce (1623). Malynes and Misselden disagree over governmental intervention in trade, and their works address each other’s arguments.\n\nQuestion for you: given that Malynes’s work is closely related to Misselden’s what do you think of the measures of similarity we just computed? As you think about this, remind yourself of how the DFM is set up.\n\n\n\n\nTF-IDF:\nNot all words are created equal in a document. We know this because we remove stopwords and we even create custom stopwords to account for idiosyncratically common words. This can be due to historical context or to technical, but non-characteristic language. For example, my research is on 17th century Puritan sermons and the use of the word “Scripture” is not a particularly distinguishing feature of these texts. But, as I say of many things in this class, this is a value judgement and dependent on technique. It may turn out that keeping a common word such as “Scripture” in my corpus would actually produce better results. By making these decisions explicit in a stopwords list, we can always retrace our steps.\nThe downside of relying on stopwords is that, in a sense, we have to take educated guesses at which words don’t carry enough discriminating meaning between texts for our corpus. This is where TF-IDF comes in.\nThere are two components to TF-IDF:\n\nTerm frequency (TF), which captures how often a word appears in a document (what we have been counting when setting up the DFM)\nInverse Document Frequency (IDF), which measures how rare a word is across the corpus. A word that appears in only a few documents will have high IDF. This means that a word with high IDF can be a distinguishing feature for a document. \\(\\text{IDF} = \\log\\left(\\frac{\\text{number of total docs}}{\\text{number of docs with term}}\\right)\\)\n\nNote: quanteda gives you the option to use either natural or base 10 log. The default is base 10. See the full documentation here. In some other applications, you will see a preference for natural logarithm. The difference between the two is really irrelevant (it’s easy to apply a change of base) and what matters is the compression. What the log does: it keeps the monotonic property of IDF (rarer words get higher IDF values), but it won’t let something like a typo in one document dominate.\n\n\nTF-IDF is just the product of TF with IDF. Quanteda will do all the work for us.\nLet’s go ahead and see how TF-IDF changes our DFM matrix:\n\ndfm_tfidf &lt;- dfm_tfidf(dfm_mat)\n\ndfm_tfidf\n\nDocument-feature matrix of: 3 documents, 9,083 features (56.93% sparse) and 0 docvars.\n                    features\ndocs                 circle commerce   prooeme herodotvs      clio  reportes\n  Circle_of_Commerce      0        0 0.4771213 0.4771213 0.9542425 0.4771213\n  Free_Trade              0        0 0         0         0         0        \n  Third_Text_A06785       0        0 0         0         0         0        \n                    features\ndocs                   croesvs king     lydia    ſonne\n  Circle_of_Commerce 0.4771213    0 0.4771213 2.385606\n  Free_Trade         0            0 0         0       \n  Third_Text_A06785  0            0 0         0       \n[ reached max_nfeat ... 9,073 more features ]\n\ntopfeatures(dfm_tfidf, 20)\n\n      ſo     upon   moneys  foreign     alſo       us     unto kingdome \n87.79031 69.18258 47.01637 41.50955 37.21546 34.82985 33.39849 33.10516 \n    ſuch     said  becauſe    realm    theſe ballance forraine    thoſe \n32.92137 32.75297 31.96712 30.53576 29.58152 29.10440 29.10440 28.62728 \n    muſt  malynes     loss  balance \n28.62728 28.52678 28.15015 24.81031 \n\n#Note: so far, dfm_tfidf is a quanteda object that has a compact storage format. If we want to use it with base R, we need to change it into a general R matrix by doing the following: \n\ntfidf_mat &lt;- as.matrix(dfm_tfidf)\n\nYou can think of tfidf_mat as a matrix where the rows are the documents, the columns are the terms in the corpus, and the values are the TF-IDF weights (instead of the raw counts). Now we can extract the words with the top TF_IDF values for each text:\n\n# Circle of Commerce\ncircle_tfidf &lt;- tfidf_mat[\"Circle_of_Commerce\", ]\n\n# Sort and get top 20\ntop_circle &lt;- sort(circle_tfidf, decreasing = TRUE)[1:20]\ntop_circle\n\n      ſo     alſo     ſuch  becauſe    theſe ballance forraine    thoſe \n87.79031 37.21546 32.92137 31.96712 29.58152 29.10440 29.10440 28.62728 \n    muſt  malynes     ſame    ſhall     ſome    firſt    leſſe     mony \n28.62728 27.82242 24.33318 22.42470 21.94758 20.99334 20.99334 20.42659 \n     vſe kingdome   ſhould himſelfe \n20.03909 19.89831 19.08485 19.08485 \n\n# Free Trade\nfree_tfidf &lt;- tfidf_mat[\"Free_Trade\", ]\n\n# Sort and get top 20\ntop_free &lt;- sort(free_tfidf, decreasing = TRUE)[1:20]\ntop_free\n\n    maiesties  christendome      kingdome           vse          also \n    17.653486     14.313638     13.206844     12.882274     12.150297 \n        selfe          vpon      maiestie         seeme common-wealth \n    10.496668     10.037202     10.019546      8.588183      8.452380 \n       causes      forreine    gouernment    domestique      encrease \n     7.924107      7.748015      7.395833      7.156819      6.679698 \n        vsury      speciall       iustice     non-latin      alphabet \n     6.679698      6.202576      6.202576      5.987103      5.987103 \n\n# A06786\nA06785_tfidf &lt;- tfidf_mat[\"Third_Text_A06785\", ]\n\n# Sort and get top 20\ntop_A06785 &lt;- sort(A06785_tfidf, decreasing = TRUE)[1:20]\ntop_A06785\n\n          upon         moneys        foreign             us           unto \n      69.18258       46.48809       41.50955       34.82985       33.39849 \n         realm           said           loss        balance     merchant's \n      30.53576       30.28770       28.15015       24.81031       24.81031 \n             s         silver         native        stivers      misselden \n      20.25049       19.08485       19.08485       17.65349       17.17637 \n          coin undervaluation  overbalancing       enhanced         weight \n      17.17637       17.17637       17.17637       15.74500       15.74500 \n\n\n\nA technical R note: here tfidf_mat[\"Circle_of_Commerce\", ] were are subsetting the matrix tfidf_mat using [row, column] notation. I am asking R to go to the row named “Circle_of_Commerce” from the matrix and grab all of the columns (the blank space after the comma). For a reference on how to subset data in R, go here. To test yourself, what would you expect to see if you were to print the result of the following? (You have all the information you need on this page!)\nsubset_test &lt;- tfidf_mat[\"Circle_of_Commerce\", c(\"malynes\", \"forraine\", \"ballance\")]\n\nWe can now visualize the most characteristic terms for each document:\n\ntfidf_top_tbl &lt;- bind_rows(\n  tibble(document = \"Circle of Commerce\", term = names(top_circle), tfidf = unname(top_circle)),\n  tibble(document = \"Free Trade\",         term = names(top_free),   tfidf = unname(top_free)),\n  tibble(document = \"Third Text\",         term = names(top_A06785),  tfidf = unname(top_A06785))\n)\n\nggplot(tfidf_top_tbl, aes(x = tfidf, y = reorder(term, tfidf))) +\n  geom_col() +\n  facet_wrap(~ document, scales = \"free_y\") +\n  labs(\n    title = \"Most Characteristic Terms by Document (TF–IDF)\",\n    x = \"TF–IDF score\",\n    y = NULL\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWARNING and a task for you:\nThe visualization above isn’t quite right. I don’t mean that it’s technically wrong: the code is correct and the visualization is displaying the correct terms for each document. However, I made some choices along the way that resulted in some less than illuminating results.\n\nWhat would you change? Why? Hint: think through the discussion about how TF-IDF is computed and what it captures.\nHow would you change it? Hint: you may want to review the lesson from week 3.",
    "crumbs": [
      "Home",
      "Weeks",
      "Week 04: Text Representation (1)"
    ]
  }
]