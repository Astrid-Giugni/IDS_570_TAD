<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Week 10: Classifying New Text &amp; spaCy – Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b57d1312aa7c10f38068bb8d7c282b76.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar docked nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Text as Data</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./week-01-introduction.html">Weeks</a></li><li class="breadcrumb-item"><a href="./week-10-spaCy.html">Week 10: Classifying New Text &amp; spaCy</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Course</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDS 570: “Text as Data: Language Models, AI, and NLP Techniques for Historical and Literary Texts”</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Weeks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-01-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 01: Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-02-basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 02: Basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-03-dictionaries.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 03: Dictionaries</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-04-representation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 04: Text Representation (1)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-05-representation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 05: Text Representation (2)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-06-cooccurrence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 06: Co-occurrence &amp; PMI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-07-word2vec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 07: Word2Vec and LDA introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-08-09-word2vec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 08 and 09:Weak Supervision &amp; Supervised Text Classification in Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week-10-spaCy.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Week 10: Classifying New Text &amp; spaCy</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./week-01-introduction.html">Weeks</a></li><li class="breadcrumb-item"><a href="./week-10-spaCy.html">Week 10: Classifying New Text &amp; spaCy</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Week 10: Classifying New Text &amp; spaCy</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This week’s plan: our goal is to use our classifier from Week 9 to learn how to do Named Entity Recognition (NER) using spaCy.</p>
<p><strong>Named Entity Recognition:</strong> I mentioned NER earlier in the semester and it is a core task in NLP. Its goal is to identify and classify spans of text into predefined semantic categories: people, organizations, locations, dates, and quantities (and you train it to add your own categories to fit your own questions). Formally, it can be framed as a sequence labeling problem, where each token is assigned a tag indicating whether it begins, continues, or falls outside a named entity. Modern NER systems typically rely on contextual word embeddings (e.g., from transformer models like BERT–our next topic, so stay tuned for that!) to capture long-range dependencies and disambiguate surface forms such as “Washington” as a person versus a place, for instance.</p>
<p><u><strong>spaCy</strong> aka, your new best friend</u>: <a href="#0">spaCy</a> is a powerful Python library for NLP. It provides an integrated pipeline architecture (tokenization, part-of-speech tagging, dependency parsing, lemmatization, and NER) all accessible through a clean, consistent API. Its NER component is quite powerful and it has served me well with complex NER questions on Early Modern Texts (which I find impressive given the various inconsistencies and idiosyncrasies in my corpora). The other option is <a href="#0">NLTK</a>, which was built primarily as a teaching toolkit. I have successfully worked with both and we use NLTK in the tutorial for Weeks 08 and 9, but spaCy prioritizes speed and memory efficiency, making it well-suited for processing large corpora. NLTK returns results more slowly than spaCy, but spaCy does trade off higher memory usage for that speed… ymmv. I want you to be exposed to both libraries.</p>
<p>You should feel free to experiment with both, but I will use spaCy for this tutorial as it tends to be more widely used outside of academia. I do want to emphasize that spaCy is not always superior:</p>
<ul>
<li><p>I tend to work with NLTK in early stages of research as it tends to be used more often in academic contexts, making it easier to compare results. If you are planning to stay in academia, do look into NLTK. But NLTK doesn’t have native word vector support at all, which is one of the meaningful gaps between it and more modern libraries.</p></li>
<li><p>In general spaCy ships with pre-trained pipelines that (can) include word vectors, so when you call <code>token.vector</code> on spaCy, you are not training your own model, but you’re getting a pre-trained word vector. Compare this with what we did with word2vec last week in order to expand on our seed term “merchant.” SpaCy loads vectors that were trained externally and baked into the pipeline. So you can’t easily swap in “custom” vectors the way <code>Gensim</code> lets you. <strong>Importantly for this tutorial</strong>: <code>en_core_web_sm</code> (which is what we install below) doesn’t include real word vectors. It relies on contextual representations rather than static word vectors. This distinction doesn’t matter for our NER workflow at all (NER uses the <code>tok2vec</code> representations regardless). If you want full word vectors, you can install <code>en_core_web_md</code> or <code>en_core_web_lg</code>.</p></li>
</ul>
<p>You need to think through your corpus and your research questions to make meaningful choices here.</p>
<p><u><strong>Overview of this week’s tutorial</strong></u>:</p>
<ol type="1">
<li><p>Reuse last week’s model: organize your corpus and use the model we trained in Weeks 8-9 <u>to find the subset</u> we want to focus on (that is, texts that discuss the theme/topic/concept of “merchant” as we defined last time)</p></li>
<li><p>Switch to spaCy: learn to use spaCy’s pipeline on our target subset of texts from step 1.</p></li>
<li><p>Extract entities as structured data and analyze the results.</p></li>
</ol>
<section id="step-0-load-a-folder-of-.txt-files-and-build-a-json-dataset" class="level2">
<h2 class="anchored" data-anchor-id="step-0-load-a-folder-of-.txt-files-and-build-a-json-dataset">Step 0: Load a folder of <code>.txt</code> files and build a JSON dataset</h2>
<p>So far, we have been working with folders of .txt files. This was fine while we were working with one folder of texts and with only a few of them. Now that our corpus is growing, we want to find a better way to organize our data. Enter JSON (JavaScript Object Notation)! This is a structured text format that stores data as key-value pairs, similar to a Python dictionary. Instead of managing hundreds of separate .txt files scattered across folders, JSON lets us bundle all our texts together in a single, organized file where each text has a unique identifier and associated metadata (like filename, author, date, printer, publisher, location of publication…). This makes our life a lot easier: we can load everything with one command, texts can’t accidentally get separated from their metadata, and we can easily add new information fields without reorganizing our entire file system.</p>
<p>This is the format that I (and pretty much everyone else in my line of work) uses. So, how do we do this? First of all, download the folder named “Post_Spring_Break_Texts” from <strong>Canvas</strong> (it should contain 147 files).</p>
<p>Start by creating a new python file (I named it <code>text_store.py</code>, we won’t reuse it, so it doesn’t matter all that much). Now we can read each .txt file, store it in JSON format, and save:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Point to the folder with our new Post Spring Break Texts (or whatever you named it)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>NEW_TEXTS_DIR <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"Post_Spring_Break_Texts"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Read all .txt files</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>paths <span class="op">=</span> <span class="bu">sorted</span>(NEW_TEXTS_DIR.glob(<span class="st">"*.txt"</span>))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Found .txt files:"</span>, <span class="bu">len</span>(paths))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># How the JSON file will be organized</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>records <span class="op">=</span> []</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> paths:</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> p.read_text(encoding<span class="op">=</span><span class="st">"utf-8"</span>, errors<span class="op">=</span><span class="st">"ignore"</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    records.append({</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">"doc_id"</span>: p.stem,           <span class="co"># filename without extension</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">"filename"</span>: p.name,         <span class="co"># full filename</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="st">"text"</span>: text                <span class="co"># raw text</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Quick check</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First record keys:"</span>, records[<span class="dv">0</span>].keys())</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First doc_id:"</span>, records[<span class="dv">0</span>][<span class="st">"doc_id"</span>])</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First text snippet:"</span>, records[<span class="dv">0</span>][<span class="st">"text"</span>][:<span class="dv">200</span>])</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the corpus as JSON</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>OUT_JSON <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"new_texts.json"</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(OUT_JSON, <span class="st">"w"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    json.dump(records, f, ensure_ascii<span class="op">=</span><span class="va">False</span>, indent<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Saved:"</span>, OUT_JSON)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Run the code and take a look at the file by opening it. Each entry should look something like:</p>
<p><code>{</code></p>
<p><code>"doc_id": "A01923",</code></p>
<p><code>"filename": "A01923.txt",</code></p>
<p><code>"text": "a Panegyrique of congratulation for the concord of the realm of great Britain in unity of religion under one King a ancient writer say that the ground and maintenance of all monarchy and empire be concord their ruin...."</code></p>
<p><code>}</code></p>
<p><strong>A quick aside:</strong></p>
<p>The files that we have in this folder don’t have much metadata associated with them. What if we had a different type of text file? Well, a lot will depend on the format, but I often have to turn folders of <code>.xml</code> files into JSON format. I know that you <strong>all</strong> remember the <code>.xml</code> info that I linked to on the syllabus in week 3 (here it is again in the very unlikely case that you don’t have it memorized: TEI and XML: <a href="https://guides.library.illinois.edu/xml/tei#:~:text=The%20Text%20Encoding%20Initiative%20(TEI)%20is%20an,the%20guidelines%20for%20text%20encoding%20in%20TEI.">guide here</a>; make sure to look over the examples ♠).</p>
<p>I am going to give you an example of how to do this. We <strong>won’t</strong> use this for our tutorial, but I want to give you this example as a resource. You can use this to help you modify the section of code under “How the JSON file will be organized” in the file we just created to look something like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>records <span class="op">=</span> []</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> paths:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parse the XML file</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    tree <span class="op">=</span> ET.parse(p)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> tree.getroot()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract metadata (these paths depend on your XML structure--which are often inconsistent, yes, life does suck a lot of times)</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Common patterns for TEI XML:</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Try to find author</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    author <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    author_elem <span class="op">=</span> root.find(<span class="st">".//{http://www.tei-c.org/ns/1.0}author"</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> author_elem <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> author_elem.text:</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        author <span class="op">=</span> author_elem.text.strip()</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Try to find publication date</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    date <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    date_elem <span class="op">=</span> root.find(<span class="st">".//{http://www.tei-c.org/ns/1.0}date"</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> date_elem <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> date_elem.text:</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        date <span class="op">=</span> date_elem.text.strip()</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract the full text (removing XML tags)</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get all text content from the document</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="st">" "</span>.join(root.itertext())</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    records.append({</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">"doc_id"</span>: p.stem,</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        <span class="st">"filename"</span>: p.name,</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        <span class="st">"author"</span>: author,</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="st">"date"</span>: date,</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="st">"text"</span>: text</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    })</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This can get really messy unfortunately! OK, back to our regularly scheduled program.</p>
</section>
<section id="step-1-load-the-classifier-artifacts-from-week-9" class="level2">
<h2 class="anchored" data-anchor-id="step-1-load-the-classifier-artifacts-from-week-9">Step 1: Load the classifier artifacts from Week 9</h2>
<p>Create one file for this step. Mine is: <code>week10_step1.py</code> because I am creative that way. Add the chunks of code as you move through the step. <strong>NOTE</strong>: You will want to add comments as well! I am commenting less than usual <em>within</em> the code because I am narrating what I am doing as part of the tutorial. This is your chance to start practicing good commenting!</p>
<p>As I mentioned at the end of Week 9 tutorial, we want to reuse what we created there. So we are going to load the TF-IDF vectorizer and classifier that we trained then and apply it to the texts we just organized as “new_texts.json”:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>MODEL_DIR <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"models"</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the SAME TF-IDF vectorizer and classifier you trained in Week 08–09</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> joblib.load(MODEL_DIR <span class="op">/</span> <span class="st">"tfidf_vectorizer.joblib"</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>clf        <span class="op">=</span> joblib.load(MODEL_DIR <span class="op">/</span> <span class="st">"merchant_logreg.joblib"</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loaded TF-IDF vectorizer + logistic regression classifier."</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">### apply to JSON corpus</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(Path.cwd() <span class="op">/</span> <span class="st">"new_texts.json"</span>, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    records <span class="op">=</span> json.load(f)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [r[<span class="st">"text"</span>] <span class="cf">for</span> r <span class="kw">in</span> records]</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> vectorizer.transform(texts)    <span class="co"># IMPORTANT: use transform, not fit_transform</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> clf.predict_proba(X_new)[:, <span class="dv">1</span>]</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> (probs <span class="op">&gt;=</span> <span class="fl">0.50</span>).astype(<span class="bu">int</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> r, p, yhat <span class="kw">in</span> <span class="bu">zip</span>(records, probs, preds):</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    r[<span class="st">"pred_prob_merchant"</span>] <span class="op">=</span> <span class="bu">float</span>(p)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    r[<span class="st">"pred_merchant"</span>] <span class="op">=</span> <span class="bu">int</span>(yhat)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Classified:"</span>, <span class="bu">len</span>(records), <span class="st">"documents"</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted merchant (threshold .50):"</span>, <span class="bu">sum</span>(preds))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>From this process, you should be seeing that:</p>
<ul>
<li><p>You found and loaded <strong>147</strong> <code>.txt</code> files.</p></li>
<li><p>The JSON serialization worked <code>new_texts.json</code></p></li>
<li><p>You successfully loaded <strong>both</strong> classifier artifacts (vectorizer + logreg) from last week.</p></li>
</ul>
<p>In my case, I classified all documents, and at threshold <strong>0.50</strong>, I got <strong>36 “merchant”</strong> predictions. You <em>might</em>&nbsp;have something slightly different (I am genuinely curious) because I changed the training set before loading it on Canvas for Weeks 08 and 09.</p>
<p>In any case, now we want to (1) save the classified JSON and (2) create a high-confidence “merchant” subset. That is, we set the threshold at 0.5, which is pretty forgiving. It would be good to have a subset with a stricter threshold for our analysis. For spaCy, you usually want precision over recall (fewer false positives).</p>
<p>First, we will save the classified corpus by adding the predictions into the JSON file:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>OUT_CLASSIFIED <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"classified_texts.json"</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(OUT_CLASSIFIED, <span class="st">"w"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    json.dump(records, f, ensure_ascii<span class="op">=</span><span class="va">False</span>, indent<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Saved classified dataset:"</span>, OUT_CLASSIFIED.resolve())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then, we will create our higher confidence subset, setting the threshold to 0.70, and then check the results. This is a good starting point for a higher confidence level for “merchant”, but in real-life scenarios, we might have to adjust it based on the requirements of our project.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>THRESH <span class="op">=</span> <span class="fl">0.70</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>merchant_only <span class="op">=</span> [r <span class="cf">for</span> r <span class="kw">in</span> records <span class="cf">if</span> r[<span class="st">"pred_prob_merchant"</span>] <span class="op">&gt;=</span> THRESH]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>OUT_MERCHANT <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"merchant_texts_for_spacy.json"</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(OUT_MERCHANT, <span class="st">"w"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    json.dump(merchant_only, f, ensure_ascii<span class="op">=</span><span class="va">False</span>, indent<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"High-confidence merchant texts (p &gt;= </span><span class="sc">{</span>THRESH<span class="sc">}</span><span class="ss">):"</span>, <span class="bu">len</span>(merchant_only))</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Saved:"</span>, OUT_MERCHANT.resolve())</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">### checking how this higher threshold compares with the 0.50 one</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>top5 <span class="op">=</span> <span class="bu">sorted</span>(records, key<span class="op">=</span><span class="kw">lambda</span> r: r[<span class="st">"pred_prob_merchant"</span>], reverse<span class="op">=</span><span class="va">True</span>)[:<span class="dv">5</span>]</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> r <span class="kw">in</span> top5:</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(r[<span class="st">"doc_id"</span>], r[<span class="st">"pred_prob_merchant"</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>My results are promising. I had 36 predicted merchant texts at 0.50 threshold and now I have 23 “high-confidence” merchant texts at threshold 0.70. The top five confidence scores look very reassuring:</p>
<p><code>B14801 =&gt; 0.9999994900493665</code></p>
<p><code>A22547``=&gt;``0.996078735040299</code></p>
<p><code>B11348``=&gt;``0.987688407922301</code></p>
<p><code>A68617``=&gt;``0.9688126041191412</code></p>
<p><code>A02626``=&gt;``0.9271125403503189</code></p>
<p>Just to be safe, we can take a quick look at a couple of the results to double check that they are indeed “merchant” texts. Text B14801 (the highest confidence score) is Edward Misselden’s <em>Free trade</em> (1622) and text A22547 (second highest confidence score) is a royal proclamation by Charles I <a href="https://quod.lib.umich.edu/e/eebo/A22547.0001.001?rgn=main;view=fulltext"><em>Concerning the trade of Ginney, Binney, in the parts of Africa</em></a> (1631) [this is a short text, if you are interested in reading it!]. Both are documents that I recognize as related, and highly so, to the topic of “merchant”–so far, so good!</p>
<section id="what-about-the-lost-documents" class="level3">
<h3 class="anchored" data-anchor-id="what-about-the-lost-documents">What about the “lost” documents:</h3>
<p>Going from 0.50 threshold to 0.70 decreased our subset of text from 36 to 23, so we “lost” 13 documents. Does this matter? Maybe or maybe not! It’s hard to know at this stage. My approach to a problem like this is the following:</p>
<ul>
<li><p>work with the smaller, “high-confidence” subset</p></li>
<li><p>look at the results of the NER task and evaluate how well the model is performing</p></li>
<li><p>manually examine a few (2-3) of the “lost” 13 documents and see if I want to include them for a second analysis</p></li>
</ul>
</section>
</section>
<section id="steps-2-and-3" class="level2">
<h2 class="anchored" data-anchor-id="steps-2-and-3">Steps 2 and 3:</h2>
<p>Now we are ready to move <u>from classification to linguistic annotation</u>: we will run spaCy on our corpus in order to perform NER and extract entities that we want to analyze. We are going to do this <em>slowly</em> so as to make our reasoning explicit.</p>
<ul>
<li><p>First, we will load one document into spaCy and inspect it</p></li>
<li><p>Then, we will extract entities from that one document</p></li>
</ul>
<p>Once we understand what the process looks like for one document, we can loop over all 23 documents and export a structured entity table (we will do JSON, but CSV is also a common choice).</p>
<p>Before using spaCy, we have to install it and download the English model. Open the terminal <u>in your project folder</u>, then:</p>
<p><code>pip install spacy</code></p>
<p><code>python -m spacy download en_core_web_sm</code></p>
<p>Here’s the <a href="https://spacy.io/usage"><strong>full installation guide</strong></a> and below is a quick reference summarizing the steps (including creating and activating venv in case you need to redo this step from Weeks-08-09):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Windows</th>
<th>macOS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Create venv</td>
<td><code>python -m venv .venv</code></td>
<td><code>python3 -m venv .venv</code></td>
</tr>
<tr class="even">
<td>Activate venv</td>
<td><code>.venv\Scripts\Activate.ps1</code></td>
<td><code>source .venv/bin/activate</code></td>
</tr>
<tr class="odd">
<td>Update pip</td>
<td><code>python -m pip install --upgrade pip setuptools wheel</code></td>
<td><code>python -m pip install --upgrade pip setuptools wheel</code></td>
</tr>
<tr class="even">
<td>Install spaCy</td>
<td><code>pip install spacy</code></td>
<td><code>pip install spacy</code></td>
</tr>
<tr class="odd">
<td>Download model</td>
<td><code>python -m spacy download en_core_web_sm</code></td>
<td><code>python -m spacy download en_core_web_sm</code></td>
</tr>
<tr class="even">
<td>Verify install</td>
<td><code>python -m spacy info</code></td>
<td><code>python -m spacy info</code></td>
</tr>
</tbody>
</table>
<p><u>Note:</u> there are a number of ways in which spaCy’s installation can go wrong (you are in the wrong directory, you did other work in between and so you need to reactivate venv, you are a Mac user and my suggestions don’t match your needs, …): youtube is your friend. There are a lot of tutorials that go through all possible scenarios for installation.</p>
<p>For future reference: you can see that there are a number of languages with trained pipelines that you can use instead of English. We will stick to English for now.</p>
<p>Now that we have the tools that we need, we can load spaCy and our high-confidence merchant corpus. Create a new file named <code>week10-step2.py</code>, we will keep adding blocks to this same file as we go along <strong>until</strong> we got to step 3. I am splitting this up to explain what we are doing along the way!</p>
<p>First, run the following code to import spaCy, the json we created, and process one file:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the English model</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"spaCy model loaded."</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>MERCHANT_PATH <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"merchant_texts_for_spacy.json"</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(MERCHANT_PATH, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    merchant_records <span class="op">=</span> json.load(f)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of merchant texts:"</span>, <span class="bu">len</span>(merchant_records))</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>sample_text <span class="op">=</span> merchant_records[<span class="dv">0</span>][<span class="st">"text"</span>]</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>doc <span class="op">=</span> nlp(sample_text) <span class="co">## All the good stuff happens here: see note below at [1]</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Processed one document."</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of tokens:"</span>, <span class="bu">len</span>(doc))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>[1] The line <code>doc = nlp (sample_text)</code> passes raw text into a processing pipeline that:</p>
<ul>
<li><p>tokenizes the text</p></li>
<li><p>splits sentences</p></li>
<li><p>assigns part-of-speech tag</p></li>
<li><p>computes syntactic dependencies</p></li>
<li><p>identifies named entities</p></li>
</ul>
<p>The result of this pipeline is a structured <a href="https://spacy.io/api/doc">spaCy Doc</a> object that contains:</p>
<ul>
<li><p>The original text</p></li>
<li><p>A sequence of tokens</p></li>
<li><p>Sentence boundaries</p></li>
<li><p>POS tags</p></li>
<li><p>Dependency relations</p></li>
<li><p>Named entities: see the <a href="https://spacy.io/usage/linguistic-features#named-entities"><strong>spaCy documentation</strong></a>. We are going to be interested in LOC: “location” and GPE: Geopolitical entities such as countries, cities, states, etc.</p></li>
<li><p>Character offsets</p></li>
</ul>
<p>Imagine you have a small, sample sentence: “John went to London in 1605.” It’s now going to be in a data structure:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Token</th>
<th>POS</th>
<th>Dependency</th>
<th>Entity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>John</td>
<td>PROPN</td>
<td>nsubj</td>
<td>PERSON</td>
</tr>
<tr class="even">
<td>went</td>
<td>VERB</td>
<td>ROOT</td>
<td>—</td>
</tr>
<tr class="odd">
<td>to</td>
<td>ADP</td>
<td>prep</td>
<td>—</td>
</tr>
<tr class="even">
<td>London</td>
<td>PROPN</td>
<td>pobj</td>
<td>GPE</td>
</tr>
<tr class="odd">
<td>in</td>
<td>ADP</td>
<td>prep</td>
<td>—</td>
</tr>
<tr class="even">
<td>1605</td>
<td>NUM</td>
<td>pobj</td>
<td>DATE</td>
</tr>
</tbody>
</table>
<p>Once you run the code, the output will look like this:</p>
<p><code>spaCy model loaded.</code></p>
<p><code>Number of merchant texts: 23</code></p>
<p><code>Processed one document.</code></p>
<p><code>Number of tokens: 6341</code></p>
<p>Now we are going to extract the named entities for that single document and inspect them:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ent <span class="kw">in</span> doc.ents:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(ent.text, <span class="st">"|"</span>, ent.label_)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>Counter([ent.label_ <span class="cf">for</span> ent <span class="kw">in</span> doc.ents]) </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What is going on in this section of code? First of all, <code>doc.ents</code> is a list of entity span objects, where each <code>ent</code> has attributes like: <code>ent.text</code> and <code>ent.label_</code>. For example:</p>
<pre><code>[Span("christian", label="NORP"),
 Span("America", label="GPE"),
 Span("two", label="Cardinal")]</code></pre>
<p>You can see this when you <code>print(ent.text, "|", ent.label_)</code>. We then extract the labels (NORP, GPE, etc) by creating a <u>new</u> list with this syntax <code>[ent.label_ for ent in doc.ents]</code>. This is called a list comprehension (<a href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions">see here</a>) and, in our case, it will produce something like this: <code>["PERSON", "PERSON", "GPE", "NORP", "Ordinal", "DATE", "DATE",...]</code>. We then use <code>Counter()</code> to create a dictionary-like object (you can review <a href="https://docs.python.org/3/tutorial/datastructures.html#dictionaries">dictionaries here</a>) that counts the frequency of each entity:</p>
<pre><code>Counter({
    "PERSON": 2,
    "DATE": 2,
    "GPE": 1
})</code></pre>
<p>This may not be very meaningful yet, but this is a helpful step in understanding the composition of texts for steps down the line in your process. For example, I will use this kind of dictionary to filter and/or route documents based on the kind of entities it contains. A document with, say, more than 5 “PERSON” entities will be flagged for further analysis when I am trying to track demographic data for migration.</p>
<p>Now we are ready to scale up to all 23 documents:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>all_entities <span class="op">=</span> []</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> record <span class="kw">in</span> merchant_records:</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    doc <span class="op">=</span> nlp(record[<span class="st">"text"</span>])</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ent <span class="kw">in</span> doc.ents:</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        all_entities.append({</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"doc_id"</span>: record[<span class="st">"doc_id"</span>],</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"entity_text"</span>: ent.text,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"entity_label"</span>: ent.label_</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total entities extracted:"</span>, <span class="bu">len</span>(all_entities))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>When you run this, you <strong>will</strong> get an error message:</p>
<blockquote class="blockquote">
<p>ValueError: [E088] Text of length 1100120 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you’re not using the parser or NER, it’s probably safe to increase the <code>nlp.max_length</code> limit. The limit is in number of characters, so you can check whether your inputs are too long by checking <code>len(text)</code>.</p>
</blockquote>
<p>The problem that we are running into is that one of our texts is very long: 1,100,120 characters. SpaCy’s default maximum length is <a href="https://spacy.io/api/language">1,000,000</a>. You <em>could</em> increase the maximum length (see the link I just gave you), but that’s probably going to be a bad idea for your machine (when you look at the error message, you can see that the “parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input”).</p>
<section id="a-note-on-chunking-strategies" class="level3">
<h3 class="anchored" data-anchor-id="a-note-on-chunking-strategies">A note on chunking strategies:</h3>
<p>When we split a long text into pieces for spaCy, we have to decide where to cut. There are <u>three reasonable approaches</u> that fit our needs and you will encounter all of them in this tutorial (that way you have options to choose from in the future). Here’s what we will do:</p>
<ul>
<li><p>Fixed-size chunks (what we’ll use in this step): Cut every 50,000 characters, no matter what. This is simple and fast. The risk is that we might slice through the middle of a word or even a multi-word entity like “the Virginia Company of London.”</p></li>
<li><p>Boundary-aware chunks (the “aside” code below that we won’t run): After reaching ~50,000 characters, look ahead for the next sentence-ending punctuation mark (. ! ? ;) and cut there. This is more linguistically principled, but Early Modern punctuation is so inconsistent that it can create as many problems as it solves for our texts.</p></li>
<li><p>Whitespace-safe chunks (what we’ll use in Step 5): After reaching ~50,000 characters, back up to the nearest space so we never split a word in half. This is a pragmatic middle ground—it doesn’t guarantee we won’t split a multi-word entity, but it does guarantee every individual token is intact, which is what spaCy’s NER model cares about most.</p></li>
</ul>
<p>For now, approach #1 is fine for getting our baseline counts. When we run the fine-tuned model in Step 5, we’ll upgrade to approach #3.</p>
<p>The solution is to break up our texts into smaller segments and process them in chunks. Remove the code that we just added and <u>instead</u> add the following:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>all_entities <span class="op">=</span> []</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>CHUNK_SIZE <span class="op">=</span> <span class="dv">50000</span>  <span class="co"># characters per chunk</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> record <span class="kw">in</span> merchant_records:</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> record[<span class="st">"text"</span>]</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split text into chunks</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(text), CHUNK_SIZE):</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        chunk <span class="op">=</span> text[i:i <span class="op">+</span> CHUNK_SIZE]</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        doc <span class="op">=</span> nlp(chunk)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> ent <span class="kw">in</span> doc.ents:</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>            all_entities.append({</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>                <span class="st">"doc_id"</span>: record[<span class="st">"doc_id"</span>],</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>                <span class="st">"entity_text"</span>: ent.text,</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>                <span class="st">"entity_label"</span>: ent.label_</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total entities extracted:"</span>, <span class="bu">len</span>(all_entities)) </span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co">#### IMPORTANT STEP: save all the raw entities</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>OUT_ENTS_RAW <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"merchant_entities_raw.json"</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(OUT_ENTS_RAW, <span class="st">"w"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    json.dump(all_entities, f, ensure_ascii<span class="op">=</span><span class="va">False</span>, indent<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Saved raw entity mentions:"</span>, OUT_ENTS_RAW.resolve())</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a><span class="co"># SAVE baseline location counts (GPE and LOC) for later comparison with trained  # (see Step 5 later in the tutorial)</span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>KEEP_LABELS <span class="op">=</span> {<span class="st">"GPE"</span>, <span class="st">"LOC"</span>}</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize_ent(text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.strip()</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r"</span><span class="dv">\s</span><span class="op">+</span><span class="vs">"</span>, <span class="st">" "</span>, text)</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text.lower()</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>counts_by_label <span class="op">=</span> {lab: Counter() <span class="cf">for</span> lab <span class="kw">in</span> KEEP_LABELS}</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> e <span class="kw">in</span> all_entities:</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>    lab <span class="op">=</span> e[<span class="st">"entity_label"</span>]</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> lab <span class="kw">in</span> KEEP_LABELS:</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>        counts_by_label[lab][normalize_ent(e[<span class="st">"entity_text"</span>])] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>OUT_BASE_COUNTS <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"merchant_locations_counts_base.json"</span></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>counts_out <span class="op">=</span> {lab: <span class="bu">dict</span>(c.most_common()) <span class="cf">for</span> lab, c <span class="kw">in</span> counts_by_label.items()}</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(OUT_BASE_COUNTS, <span class="st">"w"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>    json.dump(counts_out, f, ensure_ascii<span class="op">=</span><span class="va">False</span>, indent<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Saved baseline counts:"</span>, OUT_BASE_COUNTS.resolve())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Now try running it: this will take a while, but that’s ok! At the end, you should see:</p>
<p><code>Total entities extracted: 269702</code></p>
<p>Running spaCy is the slowest step in today’s tutorial, so we are going to do it once, save the results, and then create a new file for the rest of our work!</p>
</section>
<section id="an-aside-refining-this-process" class="level3">
<h3 class="anchored" data-anchor-id="an-aside-refining-this-process">An aside: refining this process</h3>
<p>This isn’t without risks. There is a real possibility that an entity might split across segment boundary. For example, something like “the Virginia Company of London” might end up with “Virginia Company” in one chunk and “of London” in another.</p>
<p>We can improve on this (at least a bit!) by doing the following instead:</p>
<ul>
<li><p>Cut at ~50,000 characters.</p></li>
<li><p>Look forward a limited distance (a “lookahead window”) for the first boundary mark among: <code>. ! ? ; :</code> (and optionally a newline if this makes sense for your type of text).</p></li>
<li><p>Extend the chunk to that boundary.</p></li>
</ul>
<p>We are <strong>not</strong> going to implement this code, but it would look like this:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>CHUNK_SIZE <span class="op">=</span> <span class="dv">50000</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>LOOKAHEAD  <span class="op">=</span> <span class="dv">5000</span>   <span class="co"># don't search forever; keeps chunks bounded</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>boundary_re <span class="op">=</span> re.<span class="bu">compile</span>(<span class="vs">r"</span><span class="pp">[.!?;:</span><span class="ch">\n</span><span class="pp">]</span><span class="vs">"</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> next_boundary(text: <span class="bu">str</span>, start_idx: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Return index *after* the next boundary punctuation (or newline) found</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co">    between start_idx and start_idx+LOOKAHEAD.</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co">    If none found, return start_idx (meaning: no extension).</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> boundary_re.search(text, pos<span class="op">=</span>start_idx, endpos<span class="op">=</span><span class="bu">min</span>(<span class="bu">len</span>(text), start_idx <span class="op">+</span> LOOKAHEAD))</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> m:</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> m.end()</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> start_idx  <span class="co"># no boundary found in lookahead window</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>all_entities <span class="op">=</span> []</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> record <span class="kw">in</span> merchant_records:</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> record[<span class="st">"text"</span>]</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> start <span class="op">&lt;</span> <span class="bu">len</span>(text):</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>        end <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(text), start <span class="op">+</span> CHUNK_SIZE)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> end <span class="op">&lt;</span> <span class="bu">len</span>(text):</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>            extended_end <span class="op">=</span> next_boundary(text, end)</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> extended_end <span class="op">&gt;</span> end:</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>                end <span class="op">=</span> extended_end</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>        chunk <span class="op">=</span> text[start:end]</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>        doc <span class="op">=</span> nlp(chunk)</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> ent <span class="kw">in</span> doc.ents:</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>            all_entities.append({</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>                <span class="st">"doc_id"</span>: record[<span class="st">"doc_id"</span>],</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>                <span class="st">"entity_text"</span>: ent.text,</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>                <span class="st">"entity_label"</span>: ent.label_</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>        start <span class="op">=</span> end</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total entities extracted:"</span>, <span class="bu">len</span>(all_entities))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Why am I not running this version? Well, it’s going to potentially create its own problems (especially for Early Modern texts). While this makes chunk boundaries more linguistically plausible, it still won’t guarantee you never split an entity. Entities can cross punctuation and in Early Modern English, they do so in unexpected ways: “Mr.&nbsp;John Smith” = “Mr: John Smith” = “Mr J. Smith” and many other horrors. I still want you to see this option in case you need it.</p>
</section>
<section id="back-to-our-task" class="level3">
<h3 class="anchored" data-anchor-id="back-to-our-task">Back to our task:</h3>
<p>Before we create the next file, let’s make sure that everything is in in place. At this point your project folder should contain: <code>week10_step1.py</code>, <code>classified_texts.json</code>, <code>merchant_texts_for_spacy.json</code>, <code>week10-step2.py</code>, <code>merchant_entities_raw.json</code>, <code>merchant_locations_counts_base.json</code>.</p>
<p>Create a new file named <code>week10-step3.py</code> and run it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Imports</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load entity mentions from our spaCy work</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>ENT_PATH <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"merchant_entities_raw.json"</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(ENT_PATH, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    all_entities <span class="op">=</span> json.load(f)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loaded entity mentions:"</span>, <span class="bu">len</span>(all_entities))</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Minimal normalization to remove unnecessary whitespace</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize_ent(text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.strip()</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r"</span><span class="dv">\s</span><span class="op">+</span><span class="vs">"</span>, <span class="st">" "</span>, text)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text.lower()</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Count</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>entity_text_counts <span class="op">=</span> Counter(</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    normalize_ent(e[<span class="st">"entity_text"</span>]) <span class="cf">for</span> e <span class="kw">in</span> all_entities</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Unique entity strings:"</span>, <span class="bu">len</span>(entity_text_counts))</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 20 entity strings:"</span>)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ent_text, n <span class="kw">in</span> entity_text_counts.most_common(<span class="dv">20</span>):</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>n<span class="sc">:&gt;7}</span><span class="ss">  </span><span class="sc">{</span>ent_text<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Regarding “normalization”:</strong> we want to be very restrained with this step. SpaCy gives us mentions as they appear in the text. Before counting them, I want to remove trivial formatting differences so that whitespace inconsistencies do not create artificial duplicates. For example: we want <code>"   London"</code> = <code>"London"</code>. I am also lowercasing since case use in Early Modern English is not standardized.</p>
<p>When you run this step, you should get:</p>
<p>Loaded entity mentions: 269702</p>
<p>Unique entity strings: 48301</p>
<p>Top 20 entity strings: 8193 two 7015 one 6141 three 5782 first 4260 four 3178 indian 2843 christian 2757 five 2750 english 2308 six 2243 england 1867 second 1811 seven 1663 eight 1591 ten 1527 spain 1473 twenty 1435 portugal 1325 half 1175 this day</p>
<p>Now, some of these entities are not all that interesting to us (or maybe not to me). I really don’t care for all the numbers (“one” “first” “two”…). This is obviously dependent on the research question, but I am going to filter for the kinds of entities that I am interested in:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">#### Count entity strings for desired labels ONLY</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>KEEP_LABELS <span class="op">=</span> {<span class="st">"PERSON"</span>, <span class="st">"ORG"</span>, <span class="st">"GPE"</span>, <span class="st">"LOC"</span>, <span class="st">"NORP"</span>} <span class="co">#these are the labels I want</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>filtered <span class="op">=</span> [                                        <span class="co">#filter for them</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    e <span class="cf">for</span> e <span class="kw">in</span> all_entities</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> e[<span class="st">"entity_label"</span>] <span class="kw">in</span> KEEP_LABELS</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Filtered entity mentions:"</span>, <span class="bu">len</span>(filtered))</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co">## Count by label and separate counts by entity type</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>counts_by_label <span class="op">=</span> {lab: Counter() <span class="cf">for</span> lab <span class="kw">in</span> KEEP_LABELS}</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> e <span class="kw">in</span> filtered:</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    lab <span class="op">=</span> e[<span class="st">"entity_label"</span>]</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    txt <span class="op">=</span> normalize_ent(e[<span class="st">"entity_text"</span>])</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    counts_by_label[lab][txt] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lab <span class="kw">in</span> [<span class="st">"ORG"</span>, <span class="st">"GPE"</span>, <span class="st">"PERSON"</span>, <span class="st">"LOC"</span>, <span class="st">"NORP"</span>]:</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Top 15 </span><span class="sc">{</span>lab<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ent_text, n <span class="kw">in</span> counts_by_label[lab].most_common(<span class="dv">15</span>):</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>n<span class="sc">:&gt;7}</span><span class="ss">  </span><span class="sc">{</span>ent_text<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This is what we get:</p>
<p><u>Top 15 entity strings (filtered)</u>:</p>
<blockquote class="blockquote">
<p>3178 indian 2843 christian 2243 england 1611 english 1527 spain 1435 portugal 1143 china 1036 india 1012 portugal 972 spanish 881 french 746 fez 711 dutch 676 africa 661 rome 593 italy 569 egypt 569 peru 565 france 552 john</p>
</blockquote>
<p><u>Top 15 ORG:</u></p>
<blockquote class="blockquote">
<p>371 tyrone 271 constantinople 209 senate 181 banda 153 christendom 134 inca 133 connaght 119 nanquin 103 rey 97 pegu 96 florence 93 cortes 93 helena 93 tercera 91 caesar</p>
</blockquote>
<p><u>Top 15 GPE</u>:</p>
<blockquote class="blockquote">
<p>2242 england 1527 spain 1435 portugal 1143 china 1036 india 1012 portugal 660 rome 593 italy 569 egypt 568 peru 565 france 551 jerusalem 511 mexico 499 london 472 ireland</p>
</blockquote>
<p><u>Top 15 PERSON</u>:</p>
<blockquote class="blockquote">
<p>552 john 507 thomas 392 turk 342 bantam 301 fez 295 solomon 292 chan 283 peter 279 henry 278 william 278 chinois 272 francis 267 alexander 247 charles 237 guiana</p>
</blockquote>
<p><u>Top 15 LOC</u>:</p>
<blockquote class="blockquote">
<p>675 africa 522 europe 418 asia 318 sea 231 the red sea 187 the south sea 91 the ocean sea 88 the sea coast 79 alps 72 west south-west 67 east north-east 62 the north sea 59 the mediterran sea 49 gulf 45 peninsula</p>
</blockquote>
<p><u>Top 15 NORP</u>:</p>
<blockquote class="blockquote">
<p>3178 indian 2843 christian 1611 english 972 spanish 881 french 708 dutch 541 italian 528 greek 350 irish 325 german 322 latin 287 persian 283 spaniard 274 african 252 mexican</p>
</blockquote>
<p>Well, these are mixed results, clearly. Entities like “Christendom” or “Constantinople” are <u>not</u> ORG. Similarly, entities like “Bantam” and “Guiana” are <u>not</u> PERSON.</p>
<p>Overall this isn’t surprising: I am asking spaCy to work with Early Modern texts and this is not what it was designed to do. At the same time, spaCy should be able to recognize “Guiana” as a LOC (the term “Guianas” is still used to talk about Guyana, Suriname, and French Guiana).</p>
<p>What can we do about this?</p>
<p>We are going to fine-tune spaCy to improve its ability to correctly label Early Modern text for NER.</p>
</section>
</section>
<section id="step-4-fine-tuning-spacy-for-loc-and-gpe-identification" class="level2">
<h2 class="anchored" data-anchor-id="step-4-fine-tuning-spacy-for-loc-and-gpe-identification">Step 4: Fine-tuning spaCy for LOC and GPE identification</h2>
<p>For the purposes of this tutorial, we are going to focus on two labels only. This can be extended to a full set of labels (or even create custom labels), but it will take more time to run.</p>
<p>As we did before, we want to keep the time consuming parts of the process as self-contained units, so create a new file <code>week-10-step4.py</code> for the fine-tuning step. You will need the file <code>location_train_data.json</code>, which you can find on Canvas, under “Files”, in the “Week 10” folder. With the code below, we are going to use the labelled data in the file <code>location_train_data.json</code> to improve spaCy’s ability to recognize entities correctly.</p>
<p><strong>Note I:</strong> some of the labels in the training data might look weird to you. This is because I am including historically relevant assumptions. For example, I have “Peru” labelled as a LOC rather than a GPE because it would have been considered a geographical region and not a political entity. On the other hand, while “Judea” was not a political entity in the 17th century, it would have been used as a historical political entity and therefore it would be classified as GPE.</p>
<p><strong>Note II:</strong> this code will look more complex than you are used to. I am going to comment it extensively, but it is something that I have developed over time to work with hard to deal with texts (and with a healthy dose of help from GPT and Claude to help me tinker with the configuration given my data). What you need to get out of it is a sense of the steps and understanding of the data that informs what the code is doing. Once you know what you need to do and why, the actual code will make a lot more sense!</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spacy.training.example <span class="im">import</span> Example</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spacy.scorer <span class="im">import</span> Scorer</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># CONFIGURATION</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>TRAIN_PATH    <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"location_train_data.json"</span>   </span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>OUT_MODEL_DIR <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"models"</span> <span class="op">/</span> <span class="st">"spacy_loc_tuned"</span> </span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>BASE_MODEL    <span class="op">=</span> <span class="st">"en_core_web_sm"</span>                           <span class="co"># pretrained model to start from</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>N_EPOCHS      <span class="op">=</span> <span class="dv">20</span>          <span class="co"># number of full passes over the training data</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>TRAIN_SPLIT   <span class="op">=</span> <span class="fl">0.8</span>         <span class="co"># 80 % train, 20 % dev</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>RANDOM_SEED   <span class="op">=</span> <span class="dv">42</span>          <span class="co"># for reproducibility</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>DROP_RATE     <span class="op">=</span> <span class="fl">0.3</span>         <span class="co"># dropout: randomly disables 30 % of neurons each</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>                            <span class="co"># update so the model doesn't memorise the training</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>                            <span class="co"># data too closely (helps generalisation given the                               # small training set). SEE [1] below.</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="co"># LOAD THE LABELLED DATA</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(TRAIN_PATH, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    ALL_DATA <span class="op">=</span> json.load(f)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loaded </span><span class="sc">{</span><span class="bu">len</span>(ALL_DATA)<span class="sc">}</span><span class="ss"> labelled examples from </span><span class="sc">{</span>TRAIN_PATH<span class="sc">.</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a><span class="co"># TRAIN / DEV SPLIT (shuffle the order of the documents so that the model isn't biased by order)</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="co">### SEE [2] below</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>random.seed(RANDOM_SEED)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>random.shuffle(ALL_DATA)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>split <span class="op">=</span> <span class="bu">int</span>(TRAIN_SPLIT <span class="op">*</span> <span class="bu">len</span>(ALL_DATA))</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>TRAIN <span class="op">=</span> ALL_DATA[:split]</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>DEV   <span class="op">=</span> ALL_DATA[split:]</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Train: </span><span class="sc">{</span><span class="bu">len</span>(TRAIN)<span class="sc">}</span><span class="ss">   Dev: </span><span class="sc">{</span><span class="bu">len</span>(DEV)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a><span class="co"># LOAD THE BASE MODEL AND PREPARE THE NER PIPE</span></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(BASE_MODEL) <span class="co"># SEE [3] below</span></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a><span class="co"># The NER component already exists in en_core_web_sm.  We just need to</span></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a><span class="co"># make sure it knows about the two labels we care about.  If they are</span></span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a><span class="co"># already present (GPE and LOC are standard labels) this is a "no-operation". # It's here for completeness sake.</span></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>ner <span class="op">=</span> nlp.get_pipe(<span class="st">"ner"</span>)</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>ner.add_label(<span class="st">"GPE"</span>)</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>ner.add_label(<span class="st">"LOC"</span>)</span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a><span class="co"># CREATE THE OPTIMIZER</span></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> nlp.resume_training() <span class="co">####SEE [4] below</span></span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a><span class="co">#####</span></span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a><span class="co"># TRAINING LOOP</span></span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a><span class="co">## SEE [5] below</span></span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a><span class="co">#####</span></span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>best_f1 <span class="op">=</span> <span class="fl">0.0</span>               <span class="co"># track the best dev F1 we've seen</span></span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>best_epoch <span class="op">=</span> <span class="dv">0</span>               <span class="co"># which epoch produced that best score</span></span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'Epoch'</span><span class="sc">:&gt;5}</span><span class="ss">  </span><span class="sc">{</span><span class="st">'Loss'</span><span class="sc">:&gt;10}</span><span class="ss">  </span><span class="sc">{</span><span class="st">'Precision'</span><span class="sc">:&gt;9}</span><span class="ss">  </span><span class="sc">{</span><span class="st">'Recall'</span><span class="sc">:&gt;6}</span><span class="ss">  </span><span class="sc">{</span><span class="st">'F1'</span><span class="sc">:&gt;6}</span><span class="ss">"</span>)</span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"─"</span> <span class="op">*</span> <span class="dv">48</span>)</span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(N_EPOCHS):</span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (a) Shuffle training data each epoch</span></span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a>    random.shuffle(TRAIN)</span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (b) Train on every example</span></span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> {}</span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> text, annots <span class="kw">in</span> TRAIN:</span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a>        doc <span class="op">=</span> nlp.make_doc(text)</span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a>        example <span class="op">=</span> Example.from_dict(doc, annots)</span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a>        nlp.update([example], sgd<span class="op">=</span>optimizer, losses<span class="op">=</span>losses, drop<span class="op">=</span>DROP_RATE)</span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (c) Evaluate on dev set</span></span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     We create Example objects where .predicted is the model's output</span></span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     and .reference is the gold standard, then use the built-in scorer.</span></span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     SEE weeks 08-09 on precision, recall, and F1.</span></span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a>    dev_examples <span class="op">=</span> []</span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> text, annots <span class="kw">in</span> DEV:</span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true" tabindex="-1"></a>        gold_doc <span class="op">=</span> nlp.make_doc(text)</span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true" tabindex="-1"></a>        example  <span class="op">=</span> Example.from_dict(gold_doc, annots)</span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run the full pipeline on the raw text to get predictions</span></span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true" tabindex="-1"></a>        example.predicted <span class="op">=</span> nlp(text)</span>
<span id="cb15-94"><a href="#cb15-94" aria-hidden="true" tabindex="-1"></a>        dev_examples.append(example)</span>
<span id="cb15-95"><a href="#cb15-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-96"><a href="#cb15-96" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> Scorer.score_spans(dev_examples, <span class="st">"ents"</span>)</span>
<span id="cb15-97"><a href="#cb15-97" aria-hidden="true" tabindex="-1"></a>    p  <span class="op">=</span> scores[<span class="st">"ents_p"</span>]    <span class="co"># precision: of everything the model labelled,</span></span>
<span id="cb15-98"><a href="#cb15-98" aria-hidden="true" tabindex="-1"></a>                              <span class="co">#            what fraction was correct?</span></span>
<span id="cb15-99"><a href="#cb15-99" aria-hidden="true" tabindex="-1"></a>    r  <span class="op">=</span> scores[<span class="st">"ents_r"</span>]    <span class="co"># recall:    of all true entities in the text,</span></span>
<span id="cb15-100"><a href="#cb15-100" aria-hidden="true" tabindex="-1"></a>                              <span class="co">#            what fraction did the model find?</span></span>
<span id="cb15-101"><a href="#cb15-101" aria-hidden="true" tabindex="-1"></a>    f1 <span class="op">=</span> scores[<span class="st">"ents_f"</span>]    <span class="co"># F1:        harmonic mean of P and R — the</span></span>
<span id="cb15-102"><a href="#cb15-102" aria-hidden="true" tabindex="-1"></a>                              <span class="co">#            single number that balances both</span></span>
<span id="cb15-103"><a href="#cb15-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-104"><a href="#cb15-104" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>epoch<span class="sc">:&gt;5}</span><span class="ss">  </span><span class="sc">{</span>losses[<span class="st">'ner'</span>]<span class="sc">:&gt;10.2f}</span><span class="ss">  </span><span class="sc">{</span>p<span class="sc">:&gt;9.2f}</span><span class="ss">  </span><span class="sc">{</span>r<span class="sc">:&gt;6.2f}</span><span class="ss">  </span><span class="sc">{</span>f1<span class="sc">:&gt;6.2f}</span><span class="ss">"</span>)</span>
<span id="cb15-105"><a href="#cb15-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-106"><a href="#cb15-106" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (d) Save if this is the best epoch so far (by dev F1)</span></span>
<span id="cb15-107"><a href="#cb15-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> f1 <span class="op">&gt;=</span> best_f1:</span>
<span id="cb15-108"><a href="#cb15-108" aria-hidden="true" tabindex="-1"></a>        best_f1    <span class="op">=</span> f1</span>
<span id="cb15-109"><a href="#cb15-109" aria-hidden="true" tabindex="-1"></a>        best_epoch <span class="op">=</span> epoch</span>
<span id="cb15-110"><a href="#cb15-110" aria-hidden="true" tabindex="-1"></a>        nlp.to_disk(OUT_MODEL_DIR)</span>
<span id="cb15-111"><a href="#cb15-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-112"><a href="#cb15-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-113"><a href="#cb15-113" aria-hidden="true" tabindex="-1"></a><span class="co"># DONE</span></span>
<span id="cb15-114"><a href="#cb15-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-115"><a href="#cb15-115" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"─"</span> <span class="op">*</span> <span class="dv">48</span>)</span>
<span id="cb15-116"><a href="#cb15-116" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Best dev F1: </span><span class="sc">{</span>best_f1<span class="sc">:.2f}</span><span class="ss"> (epoch </span><span class="sc">{</span>best_epoch<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb15-117"><a href="#cb15-117" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model saved to: </span><span class="sc">{</span>OUT_MODEL_DIR<span class="sc">.</span>resolve()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-118"><a href="#cb15-118" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">To use the model later:"</span>)</span>
<span id="cb15-119"><a href="#cb15-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'    nlp = spacy.load("</span><span class="sc">{</span>OUT_MODEL_DIR<span class="sc">}</span><span class="ss">")'</span>)</span>
<span id="cb15-120"><a href="#cb15-120" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'    doc = nlp("your text here")'</span>)</span>
<span id="cb15-121"><a href="#cb15-121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'    for ent in doc.ents:'</span>)</span>
<span id="cb15-122"><a href="#cb15-122" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'        print(ent.text, ent.label_)'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>More details about the code:</p>
<ol type="1">
<li><p><a href="https://spacy.io/usage/training"><u><strong>Dropout</strong></u></a> is a regularisation technique that prevents the model from becoming too dependent on any individual neuron. During each training update, a random 30% of neurons (at drop=0.3) are temporarily “switched off”, that is, their outputs are set to zero. This forces the remaining neurons to learn more robust, redundant representations rather than relying on a few specific pathways. At inference time (when you actually use the model), dropout is turned off and all neurons participate. The drop rate is a number between 0.0 (no dropout) and 1.0 (drop everything — the model learns nothing). In practice, NER models almost always use something between 0.1 and 0.5. The decision comes down to how much data you have relative to model complexity. With very small datasets like yours (31 examples), the risk of overfitting is high. Higher dropout (0.3–0.5) combats this by making memorisation harder. With large datasets (thousands of examples), overfitting is less of a concern and you can use lower dropout (0.1–0.2) so the model learns faster.</p></li>
<li><p>While I stand by this set up, I do want to point out the limitations of my training data. I have 39 total labelled paragraphs, so the validation set (dev set; 20%) is only 8 examples. Unfortunately, with only 8 dev examples containing 10 entities total, the evaluation is noisy. A single entity being found or missed swings F1 by about 10 points, which is why the dev scores bounced between 0.70 and 0.95 across epochs. This doesn’t mean the model is unstable! It just means that the measuring instrument (8 examples) is imprecise. Ideally we would have a larger labelled data set, but this is a pretty realistic case of a first “proof of concept” approach. Building human labelled data sets for highly technical language takes a lot of time.</p></li>
<li><p>When we load <code>nlp = spacy.load(BASE_MODEL)</code>, we get a whole sequence of processing components that text flows through:</p>
<ul>
<li><p>Tokenizer — splits raw text into individual tokens (words, punctuation)</p></li>
<li><p>tok2vec — converts each token into a numerical vector that captures its meaning in context</p></li>
<li><p>Tagger — assigns part-of-speech tags (noun, verb, adjective, etc.)</p></li>
<li><p>Parser — determines syntactic dependencies (which words modify which)</p></li>
<li><p>NER — identifies named entities and classifies them (person, place, organisation, etc.)</p></li>
</ul></li>
<li><p>The <a href="https://thinc.ai/docs/api-optimizers"><u><strong>optimizer</strong></u></a> is the algorithm that decides how to adjust the model’s weights after each training example. The optimizer used by spaCy is called Adam and you can use the link I just gave you to learn about the details of how it works if you are so inclined. The other key choice in this section of the code is <a href="https://spacy.io/api/language#resume_training"><code>resume_training()</code></a> [<code>instead of initialize()</code>]. By calling <code>nlp.resume_training()</code>, the optimizer is configured with a smaller learning rate appropriate for fine-tuning, where we only want small adjustments. If we had called <code>nlp.initialize()</code>, the optimizer would use a larger learning rate because it would assume that the weights are random and need big moves to get anywhere useful. To use <code>nlp.initialize()</code>, we would need a much, much larger training set (in the order of thousands)!</p></li>
<li><p>Here we are setting up our training loop. An epoch is one complete pass through the entire training set. If we have 31 training examples, one epoch means the model has seen and learned from all 31 of them once. When we set N_EPOCHS = 20, the model goes through that full set of 31 examples twenty times. Each pass gives the model another chance to refine its weights: the first time through it makes rough adjustments, and subsequent passes let it fine-tune those adjustments based on what it’s learned so far.</p>
<ul>
<li><p><code>random.shuffle(TRAIN)</code>: Before each epoch, the training examples are put in a new random order. This matters because the model updates its weights after every single example, and those updates accumulate.</p></li>
<li><p><u>Step b in the code</u>: Four things happen for each of the 31 examples: First, <code>nlp.make_doc(text)</code> creates a blank spaCy <code>Doc</code> from the raw text (it tokenizes the words at this step). Second, <code>Example.from_dict(doc, annots)</code> pairs that blank doc with the gold-standard annotations I labelled. Third, <a href="https://spacy.io/api/language#update"><code>nlp.update()</code></a> does the actual learning. Fourth, <code>drop=DROP_RATE</code> applies dropout during this update, randomly disabling 30% of neurons so the model doesn’t over-rely on any single pathway (see [1] above). After all 31 examples, the model’s weights have been adjusted 31 times. That completes one epoch of training.</p></li>
</ul></li>
</ol>
</section>
<section id="step-5-rerun-ner-with-the-fine-tuned-model" class="level2">
<h2 class="anchored" data-anchor-id="step-5-rerun-ner-with-the-fine-tuned-model">Step 5: Rerun NER with the fine-tuned model</h2>
<p>Now that we have completed our fine-tuning of spaCy, we want to actually see if we can get some better results on our 23 target texts (the chunked merchant texts). Then we are going to export a locations-only dataset (mentions + counts) that you can analyze or use for further work (such as mapping of Early Modern locations mentioned in our documents). What this will look like:</p>
<p>Input: merchant_texts_for_spacy.json — a JSON list of {“doc_id”: …, “text”: …} records</p>
<pre><code>    models/spacy_loc_tuned/
    — the fine-tuned spaCy model from train_spacy_ner.py</code></pre>
<p>Output: merchant_locations_mentions_tuned.json — every individual entity mention with doc_id, text, label, and character offsets into the full document</p>
<pre><code>    merchant_locations_counts_tuned.json
    — normalised frequency counts grouped by label (GPE / LOC)</code></pre>
<p>Let’s create a new file, <code>week-10-step5.py</code>, load the merchant text, and the tuned spaCy model. We are taking a different approach to chunking (as I warned during the week 8 tutorial, you will be sick of “chunks”): unlike our earlier naive chunking, this version backs up to the nearest whitespace so we don’t split tokens.</p>
<p>This is going to take a little while to run:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># CONFIGURATION</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>MERCHANT_PATH   <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"merchant_texts_for_spacy.json"</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>TUNED_MODEL_DIR <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"models"</span> <span class="op">/</span> <span class="st">"spacy_loc_tuned"</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>OUT_MENTIONS    <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"merchant_locations_mentions_tuned.json"</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>OUT_COUNTS      <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"merchant_locations_counts_tuned.json"</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>CHUNK_SIZE  <span class="op">=</span> <span class="dv">50_000</span>       <span class="co"># max characters per chunk sent to spaCy</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>KEEP_LABELS <span class="op">=</span> {<span class="st">"GPE"</span>, <span class="st">"LOC"</span>}</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="co"># HELPER FUNCTIONS</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize_ent(text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Normalise an entity's surface text for counting and deduplication.</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a><span class="co">    Strips whitespace, collapses internal runs of whitespace, and</span></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a><span class="co">    lowercases so that 'Virginia', ' virginia', and 'VIRGINIA' all</span></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a><span class="co">    map to the same key.</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.strip()</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r"</span><span class="dv">\s</span><span class="op">+</span><span class="vs">"</span>, <span class="st">" "</span>, text)</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text.lower()</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_chunk_end(text: <span class="bu">str</span>, start: <span class="bu">int</span>, max_size: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a><span class="co">    Find a safe chunk boundary that doesn't split a word.</span></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a><span class="co">    Starting from `start`, we look up to `max_size` characters ahead.</span></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a><span class="co">    If that landing point is mid-word, we back up to the nearest</span></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a><span class="co">    whitespace so no token is cut in half.  This prevents the NER</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a><span class="co">    model from seeing partial words at chunk edges, which could cause</span></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a><span class="co">    it to miss an entity or hallucinate one.</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> <span class="bu">min</span>(start <span class="op">+</span> max_size, <span class="bu">len</span>(text))</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If we've reached the end of the text, no adjustment needed</span></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> end <span class="op">&gt;=</span> <span class="bu">len</span>(text):</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> end</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If we landed on whitespace, we're fine</span></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> text[end].isspace():</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> end</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Otherwise back up to the nearest space within this chunk</span></span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>    safe_end <span class="op">=</span> text.rfind(<span class="st">" "</span>, start, end)</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If there's no space at all in this chunk (one giant token?),</span></span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fall back to the hard boundary rather than looping forever</span></span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> safe_end <span class="op">&lt;=</span> start:</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> end</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> safe_end</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a><span class="co"># LOAD DATA AND MODEL</span></span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(MERCHANT_PATH, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>    merchant_records <span class="op">=</span> json.load(f)</span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loaded merchant texts: </span><span class="sc">{</span><span class="bu">len</span>(merchant_records)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(TUNED_MODEL_DIR)</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loaded tuned spaCy model: </span><span class="sc">{</span>TUNED_MODEL_DIR<span class="sc">.</span>resolve()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a><span class="co">####</span></span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a><span class="co"># EXTRACT ENTITIES</span></span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a><span class="co"># This is where the work finally pays off:</span></span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a><span class="co">#    For each document we split the text into chunks (most documents</span></span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a><span class="co">#    will fit in a single chunk) and run the NER pipeline on each.</span></span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a><span class="co">#    Character offsets are recorded relative to the FULL document,</span></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a><span class="co">#    not the chunk, so they can be used later to highlight entities</span></span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a><span class="co">#    in the original text.</span></span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a><span class="co">####</span></span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a>locations <span class="op">=</span> []</span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> record <span class="kw">in</span> merchant_records:</span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a>    doc_id <span class="op">=</span> record[<span class="st">"doc_id"</span>]</span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a>    text   <span class="op">=</span> record[<span class="st">"text"</span>]</span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Walk through the text in chunks</span></span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a>    chunk_start <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> chunk_start <span class="op">&lt;</span> <span class="bu">len</span>(text):</span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a>        chunk_end <span class="op">=</span> find_chunk_end(text, chunk_start, CHUNK_SIZE)</span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a>        chunk     <span class="op">=</span> text[chunk_start:chunk_end]</span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a>        doc <span class="op">=</span> nlp(chunk)</span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> ent <span class="kw">in</span> doc.ents:</span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ent.label_ <span class="kw">in</span> KEEP_LABELS:</span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true" tabindex="-1"></a>                locations.append({</span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"doc_id"</span>:       doc_id,</span>
<span id="cb18-101"><a href="#cb18-101" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"entity_text"</span>:  ent.text,</span>
<span id="cb18-102"><a href="#cb18-102" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"entity_label"</span>: ent.label_,</span>
<span id="cb18-103"><a href="#cb18-103" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Offsets into the full document text</span></span>
<span id="cb18-104"><a href="#cb18-104" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"start_char"</span>:   chunk_start <span class="op">+</span> ent.start_char,</span>
<span id="cb18-105"><a href="#cb18-105" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"end_char"</span>:     chunk_start <span class="op">+</span> ent.end_char,</span>
<span id="cb18-106"><a href="#cb18-106" aria-hidden="true" tabindex="-1"></a>                })</span>
<span id="cb18-107"><a href="#cb18-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-108"><a href="#cb18-108" aria-hidden="true" tabindex="-1"></a>        chunk_start <span class="op">=</span> chunk_end</span>
<span id="cb18-109"><a href="#cb18-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-110"><a href="#cb18-110" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total location mentions extracted: </span><span class="sc">{</span><span class="bu">len</span>(locations)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-111"><a href="#cb18-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-112"><a href="#cb18-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-113"><a href="#cb18-113" aria-hidden="true" tabindex="-1"></a><span class="co">#SAVE MENTION-LEVEL OUTPUT</span></span>
<span id="cb18-114"><a href="#cb18-114" aria-hidden="true" tabindex="-1"></a><span class="co">#    One record per entity mention — useful for downstream analysis,</span></span>
<span id="cb18-115"><a href="#cb18-115" aria-hidden="true" tabindex="-1"></a><span class="co">#    joining back to document metadata, or reviewing model output.</span></span>
<span id="cb18-116"><a href="#cb18-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-117"><a href="#cb18-117" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(OUT_MENTIONS, <span class="st">"w"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb18-118"><a href="#cb18-118" aria-hidden="true" tabindex="-1"></a>    json.dump(locations, f, ensure_ascii<span class="op">=</span><span class="va">False</span>, indent<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-119"><a href="#cb18-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Saved mentions: </span><span class="sc">{</span>OUT_MENTIONS<span class="sc">.</span>resolve()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-120"><a href="#cb18-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-121"><a href="#cb18-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-122"><a href="#cb18-122" aria-hidden="true" tabindex="-1"></a><span class="co">#AGGREGATE COUNTS</span></span>
<span id="cb18-123"><a href="#cb18-123" aria-hidden="true" tabindex="-1"></a><span class="co">#    Normalise each entity's text and count occurrences by label.</span></span>
<span id="cb18-124"><a href="#cb18-124" aria-hidden="true" tabindex="-1"></a><span class="co">#    This gives you a quick overview of what the model found and</span></span>
<span id="cb18-125"><a href="#cb18-125" aria-hidden="true" tabindex="-1"></a><span class="co">#    which locations appear most frequently across the corpus.</span></span>
<span id="cb18-126"><a href="#cb18-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-127"><a href="#cb18-127" aria-hidden="true" tabindex="-1"></a>counts_by_label <span class="op">=</span> {label: Counter() <span class="cf">for</span> label <span class="kw">in</span> KEEP_LABELS}</span>
<span id="cb18-128"><a href="#cb18-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-129"><a href="#cb18-129" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> locations:</span>
<span id="cb18-130"><a href="#cb18-130" aria-hidden="true" tabindex="-1"></a>    norm <span class="op">=</span> normalize_ent(m[<span class="st">"entity_text"</span>])</span>
<span id="cb18-131"><a href="#cb18-131" aria-hidden="true" tabindex="-1"></a>    counts_by_label[m[<span class="st">"entity_label"</span>]][norm] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb18-132"><a href="#cb18-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-133"><a href="#cb18-133" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 20 GPE (tuned):"</span>)</span>
<span id="cb18-134"><a href="#cb18-134" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, n <span class="kw">in</span> counts_by_label[<span class="st">"GPE"</span>].most_common(<span class="dv">20</span>):</span>
<span id="cb18-135"><a href="#cb18-135" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>n<span class="sc">:&gt;5}</span><span class="ss">  </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-136"><a href="#cb18-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-137"><a href="#cb18-137" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 20 LOC (tuned):"</span>)</span>
<span id="cb18-138"><a href="#cb18-138" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, n <span class="kw">in</span> counts_by_label[<span class="st">"LOC"</span>].most_common(<span class="dv">20</span>):</span>
<span id="cb18-139"><a href="#cb18-139" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>n<span class="sc">:&gt;5}</span><span class="ss">  </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-140"><a href="#cb18-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-141"><a href="#cb18-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-142"><a href="#cb18-142" aria-hidden="true" tabindex="-1"></a><span class="co"># SAVE COUNTS</span></span>
<span id="cb18-143"><a href="#cb18-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-144"><a href="#cb18-144" aria-hidden="true" tabindex="-1"></a>counts_out <span class="op">=</span> {</span>
<span id="cb18-145"><a href="#cb18-145" aria-hidden="true" tabindex="-1"></a>    label: <span class="bu">dict</span>(counter.most_common())</span>
<span id="cb18-146"><a href="#cb18-146" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> label, counter <span class="kw">in</span> counts_by_label.items()</span>
<span id="cb18-147"><a href="#cb18-147" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-148"><a href="#cb18-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-149"><a href="#cb18-149" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(OUT_COUNTS, <span class="st">"w"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb18-150"><a href="#cb18-150" aria-hidden="true" tabindex="-1"></a>    json.dump(counts_out, f, ensure_ascii<span class="op">=</span><span class="va">False</span>, indent<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-151"><a href="#cb18-151" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Saved counts: </span><span class="sc">{</span>OUT_COUNTS<span class="sc">.</span>resolve()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>When you run this, you should get the following GPE and LOC results:</p>
<blockquote class="blockquote">
<p><strong>Top 20 GPE (tuned)</strong>: 2327 england 1531 spain 1153 china 797 fez 640 rome 581 egypt 559 france 548 jerusalem 521 london 515 italy 482 mexico 447 persia 442 germany 425 ireland 389 venice 358 cairo 353 russia 320 holland 312 bassa 273 japan</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Top 20 LOC (tuned)</strong>: 936 india 720 africa 575 peru 539 europe 516 bantam 430 asia 397 the east 365 virginia 303 barbary 250 guiana 244 america 244 goa 194 north-east 182 sea 151 armenia 143 the south sea 133 nilus 107 jordan 107 arabia 102 italy</p>
</blockquote>
<p>And to compare the “base” vs.&nbsp;“tuned” result, we can see:</p>
<p>TOTAL locations (base): 48952</p>
<p>TOTAL locations (tuned): 46093</p>
<p><strong>Top 15 base locations</strong>: 2447 portugal 2243 england 1528 spain 1227 china 1036 india 694 africa 660 rome 601 jerusalem 594 italy 569 egypt 569 peru 565 france 522 europe 512 mexico 501 london</p>
<p><strong>Top 15 tuned locations</strong>: 2327 england 1531 spain 1175 china 954 india 797 fez 721 africa 666 rome 617 italy 581 egypt 575 peru 571 jerusalem 559 france 539 europe 528 bantam 522 mexico</p>
<section id="how-can-we-understand-these-changes" class="level3">
<h3 class="anchored" data-anchor-id="how-can-we-understand-these-changes">How can we understand these changes?</h3>
<p>What we are seeing is the result of the decision boundary of the NER model changing after fine-tuning. As you can see, the tuned model is labeling fewer spans as GPE/LOC overall. That usually means one of two things: it became more conservative in its labelling (higher precision, slightly lower recall), OR it stopped labeling certain ambiguous spans as locations. Because of the dev metrics above, the tuned model likely became more precise.</p>
<p>We can see some reassuring changes: “Fez” and “Bantam” are in the Top 15 locations in the tuned model, something that we <em>would</em>&nbsp;expect from trading documents. This indicates that the tuned model became better at recognizing historically specific trading locations that the base model under-recognized. The pretrained model was trained on modern English text. When applied to Early Modern material, it over- and under-predicted in different places (it misses spelling variations, for example). Fine-tuning shifts the internal statistical weights so that entity detection better reflects the historical domain. As a result, the distribution of detected locations changes.</p>
<p>Let’s take a closer look at some of these changes to see what is going on. Create a new file, <code>week-10-tuned-counts.py</code>, and run it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>BASE_COUNTS_PATH  <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"merchant_locations_counts_base.json"</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>TUNED_COUNTS_PATH <span class="op">=</span> Path.cwd() <span class="op">/</span> <span class="st">"merchant_locations_counts_tuned.json"</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(BASE_COUNTS_PATH, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    base_counts <span class="op">=</span> json.load(f)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(TUNED_COUNTS_PATH, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    tuned_counts_raw <span class="op">=</span> json.load(f)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co"># merge tuned GPE + LOC</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>tuned_counts <span class="op">=</span> {}</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> tuned_counts_raw:</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> tuned_counts_raw[label].items():</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        tuned_counts[k] <span class="op">=</span> tuned_counts.get(k, <span class="dv">0</span>) <span class="op">+</span> v</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="co"># merge base GPE + LOC (same structure as tuned)</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>base_merged <span class="op">=</span> {}</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> base_counts:</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> base_counts[label].items():</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        base_merged[k] <span class="op">=</span> base_merged.get(k, <span class="dv">0</span>) <span class="op">+</span> v</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>all_places <span class="op">=</span> <span class="bu">set</span>(base_merged) <span class="op">|</span> <span class="bu">set</span>(tuned_counts)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> {</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    place: tuned_counts.get(place, <span class="dv">0</span>) <span class="op">-</span> base_merged.get(place, <span class="dv">0</span>)</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> place <span class="kw">in</span> all_places</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Biggest increases:"</span>)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> place, delta <span class="kw">in</span> <span class="bu">sorted</span>(diff.items(), key<span class="op">=</span><span class="kw">lambda</span> x: <span class="op">-</span>x[<span class="dv">1</span>])[:<span class="dv">15</span>]:</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>delta<span class="sc">:&gt;6}</span><span class="ss">  </span><span class="sc">{</span>place<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Biggest decreases:"</span>)</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> place, delta <span class="kw">in</span> <span class="bu">sorted</span>(diff.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>])[:<span class="dv">15</span>]:</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>delta<span class="sc">:&gt;6}</span><span class="ss">  </span><span class="sc">{</span>place<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The results should look like (I am using “bold” on the entities I will discuss below):</p>
<p><u>Biggest increases</u>:</p>
<p><strong>413 fez</strong></p>
<p>397 the east</p>
<p><strong>359 bantam</strong></p>
<p><strong>246 guiana</strong></p>
<p>245 barbary</p>
<p><strong>238 goa</strong></p>
<p>191 north-east</p>
<p>167 bassa</p>
<p>162 banda</p>
<p>133 connaght</p>
<p>117 januarie</p>
<p>113 alexandria</p>
<p>93 the east indies</p>
<p><strong>87 prete janni</strong></p>
<p>86 the east india</p>
<p><u>Biggest decreases</u>:</p>
<p><strong>-2419 portugal</strong></p>
<p>-248 the red sea</p>
<p>-199 canton</p>
<p>-165 thou</p>
<p>-136 sea</p>
<p>-120 fort</p>
<p>-120 nilus</p>
<p>-118 skiffe</p>
<p>-94 the ocean sea</p>
<p>-88 cotton</p>
<p>-87 the sea coast</p>
<p>-82 india</p>
<p>-72 west south-west</p>
<p>-67 the south sea</p>
<p>-62 panama</p>
<p>After fine-tuning spaCy on our 31 training examples, we are getting some interesting results (even though we need to remember that this is a pretty small training set). The baseline model overproduced certain contemporary or highly frequent place names (I suspect that it was confused by terms such as “the Portuguese” and labelled them as <em>Portugal</em>) and under-recognized Early Modern trade locations like <em>Fez</em>, <em>Bantam</em>, <em>Guiana</em>, and <em>Goa</em>. The fine-tuned model reduces some modern bias while increasing recognition of historically salient geographic terms and multi-word colonial phrases such as <em>the East Indies</em>. At the same time, we see small artifacts of overfitting (e.g., occasional misclassification of capitalized months like <em>Januarie</em> and the fact that “prete janni”, aka “Prete Gianni” aka “Prester John”, is considered a location rather than a mythical person!).</p>
<p>This is not too surprising as I gave you a small training set.</p>
</section>
</section>
<section id="where-to-next" class="level2">
<h2 class="anchored" data-anchor-id="where-to-next">Where to next?</h2>
<p>This week, I started using spaCy without much fuss. However, you might naturally want to know how spaCy identifies entities. Unlike Word2Vec, spaCy is based on contextual token embeddings. When spaCy processes a sentence, it first converts each token into a numerical vector (embedding) that captures not just the word’s meaning in isolation, but its meaning in context. So “Apple” in “Apple released a new phone” gets a different representation than “Apple” in “I ate an apple.” I started talking about contextual embeddings and transformer models like BERT this week. These are the foundation of spaCy: we will keep working on them for the rest of the semester!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/Astrid-Giugni\.github\.io\/IDS_570_TAD\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>